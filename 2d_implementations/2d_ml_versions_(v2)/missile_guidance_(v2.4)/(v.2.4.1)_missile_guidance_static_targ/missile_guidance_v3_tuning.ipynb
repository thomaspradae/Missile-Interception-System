{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "from gymnasium import Env\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import tensorflow as tf\n",
    "\n",
    "# # Set seed for reproducibility\n",
    "# seed = 42\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "class missile_interception(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.discrete.Discrete(3)\n",
    "        low = np.array([-1, -1, -1, -1, -1, -1, -1, -1, -2, -2, -0.04, -2.828, -70.71, 0, -14.9, 0], dtype=np.float32)\n",
    "        high = np.array([1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2.828, 2.828, 70.71, 149, 0, math.pi], dtype=np.float32)\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "        self.radius = 0.02\n",
    "\n",
    "        self.episode_count = 0\n",
    "        self.distance_t_minus_one = 0\n",
    "        self.distance_change = 0\n",
    "\n",
    "        self.out_of_bounds = 0\n",
    "        self.interceptions = 0\n",
    "        self.reached_max_steps = 0\n",
    "\n",
    "        self.defense_positions = []\n",
    "        self.attack_positions = []\n",
    "\n",
    "        self.max_steps_per_episode = 150\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.defense_positions = []\n",
    "        self.attack_positions = []\n",
    "        self.reward = 0\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.create_defense()\n",
    "        self.create_attack(self.defense)\n",
    "        self.calc_angle(self.defense, self.attack)\n",
    "        self.initial_missile_angle()\n",
    "        self.missile_distance_to_attack()\n",
    "        # self.graph(self.defense, self.attack)\n",
    "        self.get_state()\n",
    "\n",
    "        return self.state, {}\n",
    "\n",
    "    def create_defense(self):\n",
    "        x = random.uniform(-0.45, 0.45)\n",
    "        y = random.uniform(-0.45, 0.45)\n",
    "        self.defense = np.array([x, y])\n",
    "\n",
    "    def create_attack(self, defense):\n",
    "        x_side_left = random.uniform(-0.95, (defense[0] - 0.02) - 0.1)\n",
    "        x_side_right = random.uniform((defense[0] + 0.02) + 0.1, 0.95)\n",
    "        y_below = random.uniform((defense[1] - 0.02) - 0.1, -0.95)\n",
    "        y_above = random.uniform((defense[1] + 0.02) + 0.1, 0.95)\n",
    "        x_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_below_x_inclusive = np.array([x_inclusive, y_below])\n",
    "        y_above_x_inclusive = np.array([x_inclusive, y_above])\n",
    "        x_left_y_inclusive = np.array([x_side_left, y_inclusive])\n",
    "        x_right_y_inclusive = np.array([x_side_right, y_inclusive])\n",
    "\n",
    "        self.attack = random.choice([y_below_x_inclusive, y_above_x_inclusive, x_left_y_inclusive, x_right_y_inclusive])\n",
    "\n",
    "    def calc_angle(self, defense, attack):\n",
    "\n",
    "        # create an adjacent point of the form (attack_x, defense_y)\n",
    "        adjacent_point = np.array([attack[0], defense[1]])\n",
    "\n",
    "        # calculate the distance between the adjacent point and the defense, attack points\n",
    "        adj_point_defense_len = abs(defense[0] - adjacent_point[0]) \n",
    "        adj_point_attack_len = abs(attack[1] - adjacent_point[1])\n",
    "\n",
    "        # calculate the angle, using soh cah toa, where adj_point_defense_len is the adjacent side and adj_point_attack_len is the opposite side\n",
    "        self.theta = np.arctan(adj_point_attack_len / adj_point_defense_len)\n",
    "        \n",
    "        if attack[0] > defense[0]:\n",
    "            if attack[1] > defense[1]:\n",
    "                self.theta = self.theta # 1st quadrant\n",
    "            else: \n",
    "                self.theta = (2*math.pi) - self.theta # 360 - theta\n",
    "        else:\n",
    "            if attack[1] > defense[1]:\n",
    "                self.theta = math.pi - self.theta # 180 - theta\n",
    "            else:\n",
    "                self.theta = math.pi + self.theta # 180 + theta\n",
    "            \n",
    "        return self.theta\n",
    "    \n",
    "    def initial_missile_angle(self):\n",
    "        self.missile_angle = np.random.uniform(0, 2*math.pi)\n",
    "        # if self.theta < math.pi:\n",
    "        #     self.missile_angle = np.random.uniform(0, math.pi)\n",
    "        # else:\n",
    "        #     self.missile_angle = np.random.uniform(math.pi, 2*math.pi)\n",
    "\n",
    "    def calculate_distance(self, point1, point2):\n",
    "        return math.hypot(point1[0] - point2[0], point1[1] - point2[1])\n",
    "    \n",
    "    def missile_distance_to_attack(self):\n",
    "        self.distance = (self.calculate_distance(self.defense, self.attack) - (2 * self.radius))\n",
    "\n",
    "    def missile_nav_angle(self, action):\n",
    "        if action == 0:\n",
    "            self.missile_angle = self.missile_angle \n",
    "        elif action == 1:\n",
    "            self.missile_angle += 0.174532925\n",
    "        elif action == 2:\n",
    "            self.missile_angle -= 0.174532925\n",
    "\n",
    "    def missle_coord_calc(self):\n",
    "        self.defense[0] += (0.02 * math.cos(self.missile_angle)) # gotta test this\n",
    "        self.defense[1] += (0.02 * math.sin(self.missile_angle))\n",
    "\n",
    "        self.defense_positions.append(self.defense.copy())\n",
    "        self.attack_positions.append(self.attack.copy())\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        self.missile_distance_to_attack()\n",
    "\n",
    "        if self.distance < 0:\n",
    "            self.reward = 1000\n",
    "            self.done = True\n",
    "            self.interceptions += 1\n",
    "        else:\n",
    "            self.distance_change = ((self.distance - self.distance_t_minus_one) / 0.02) \n",
    "            if self.distance_change < 0:\n",
    "                self.reward = -1 * (self.distance_change * (0.01 + 1/self.distance))\n",
    "            else:\n",
    "                self.reward = -8 * (self.distance_change * 1 + 1 * (self.distance**2))\n",
    "\n",
    "            # angle penalty\n",
    "            self.angle_diff = abs(self.theta - self.missile_angle)\n",
    "            self.angle_diff = min(self.angle_diff, 2*math.pi - self.angle_diff)\n",
    "            if self.angle_diff < 0.05:\n",
    "                try:\n",
    "                    if 1/self.angle_diff < 100:\n",
    "                        reward += 1/self.angle_diff\n",
    "                    else:\n",
    "                        reward += 100\n",
    "                except ZeroDivisionError:\n",
    "                    reward += 100\n",
    "            else:\n",
    "                self.reward -= (2 * self.angle_diff)\n",
    "            \n",
    "            # time penalty\n",
    "            self.reward -= 0.2 * self.current_step\n",
    "                \n",
    "        if self.defense[0] < -1 or self.defense[0] > 1 or self.defense[1] < -1 or self.defense[1] > 1:\n",
    "            print(\"OUT OF BOUNDS\")\n",
    "            self.reward = -1000\n",
    "            self.done = True\n",
    "            self.out_of_bounds += 1\n",
    "\n",
    "    def angle_conversion(self):\n",
    "        self.sin_theta, self.sin_missile_angle = np.sin(self.theta), np.sin(self.missile_angle)\n",
    "        self.cos_theta, self.cos_missile_angle = np.cos(self.theta), np.cos(self.missile_angle)\n",
    "\n",
    "        self.delta_sin = self.sin_theta - self.sin_missile_angle\n",
    "        self.delta_cos = self.cos_theta - self.cos_missile_angle\n",
    "\n",
    "    def get_state(self):\n",
    "        self.angle_conversion()\n",
    "        self.state = np.array([\n",
    "            self.attack[0], self.attack[1], self.defense[0], self.defense[1],\n",
    "            self.sin_theta, self.cos_theta, self.sin_missile_angle, self.cos_missile_angle,\n",
    "            self.delta_sin, self.delta_cos, self.distance,\n",
    "            (self.distance - self.distance_t_minus_one),\n",
    "            ((self.distance - self.distance_t_minus_one) / 0.02),\n",
    "            self.current_step,\n",
    "            (-0.1 * self.current_step),\n",
    "            min(abs(self.theta - self.missile_angle), 2*math.pi - abs(self.theta - self.missile_angle))\n",
    "        ])\n",
    "\n",
    "    def get_state_dict(self):\n",
    "        return {\"reward\": self.reward, \"attack_x\": self.attack[0], \"attack_y\": self.attack[1], \"defense_x\": self.defense[0], \"defense_y\": self.defense[1], \"theta\": self.theta, \"missile_angle\": self.missile_angle, \"self.sin_theta\": self.sin_theta, \"self.cos_theta\": self.cos_theta, \"self.sin_missile_angle\": self.sin_missile_angle, \"self.cos_missile_angle\": self.cos_missile_angle, \"delta_sin\": self.delta_sin, \"delta_cos\": self.delta_cos, \"distance\": self.distance, \"current_step\": self.current_step}\n",
    "                 \n",
    "    def step(self, action):\n",
    "        self.distance_t_minus_one = self.distance\n",
    "        self.missile_nav_angle(action)\n",
    "        self.missle_coord_calc()\n",
    "        self.calculate_reward()\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= self.max_steps_per_episode:\n",
    "            self.done = True\n",
    "            self.reward = -1000\n",
    "            self.reached_max_steps += 1\n",
    "\n",
    "        self.get_state()\n",
    "\n",
    "        # self.graph(self.defense, self.attack)\n",
    "\n",
    "        return self.state, self.reward, self.done, False, {}\n",
    "\n",
    "    def graph(self, defense, attack):\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.xlim(-1, 1)\n",
    "        plt.ylim(-1, 1)\n",
    "\n",
    "        plt.axhline(0, color='black', linewidth=0.5)\n",
    "        plt.axvline(0, color='black', linewidth=0.5)\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot trails\n",
    "        if self.defense_positions:\n",
    "            defense_xs, defense_ys = zip(*self.defense_positions)\n",
    "            ax.plot(defense_xs, defense_ys, 'b-')  # Blue line for defense\n",
    "\n",
    "        if self.attack_positions:\n",
    "            attack_xs, attack_ys = zip(*self.attack_positions)\n",
    "            ax.plot(attack_xs, attack_ys, 'r-')  # Red line for attack\n",
    "\n",
    "        # Plot current positions\n",
    "        plt.scatter(defense[0], defense[1], color='black')\n",
    "        plt.scatter(attack[0], attack[1], color='red')\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def animate_episode(episode_data, save_name):\n",
    "    episode_past_defense_positions = episode_data[\"past_defense_positions\"]\n",
    "    past_defense_x, past_defense_y = zip(*episode_past_defense_positions)\n",
    "    episode_attack_positions = episode_data[\"attack_positions\"]\n",
    "    attack_x, attack_y = zip(*episode_attack_positions)\n",
    "    episode_defense_positions = episode_data[\"defense_positions\"]\n",
    "    defense_x, defense_y = zip(*episode_defense_positions)\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.axhline(0, color='black', linewidth=0.5)\n",
    "    plt.axvline(0, color='black', linewidth=0.5)\n",
    "    plt.grid(True)\n",
    "\n",
    "    trail, = ax.plot([], [], 'b-', label='Defense Trail')  \n",
    "    scatter1, = ax.plot([], [], 'ro')  # 'ro' means red circles\n",
    "    scatter2, = ax.plot([], [], 'bo')  # 'ro' means red circles\n",
    "\n",
    "    # Update function for animation\n",
    "    def update(frame):\n",
    "        trail.set_data(past_defense_x[:frame+1], past_defense_y[:frame+1])\n",
    "        scatter1.set_data(attack_x[frame], attack_y[frame])\n",
    "        scatter2.set_data(defense_x[frame], defense_y[frame])\n",
    "        return trail, scatter1, scatter2\n",
    "\n",
    "    # Create animation\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(past_defense_x), interval=200, blit=True)\n",
    "\n",
    "    # Option 1: Save animation\n",
    "    ani.save(save_name, writer='pillow')\n",
    "    print(f\"Animation saved as {save_name}\")\n",
    "\n",
    "    # # Option 2: Display using HTML\n",
    "    # plt.close(fig)\n",
    "    # video = ani.to_html5_video()\n",
    "    # html = display(HTML(video))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'missile_interception' object has no attribute 'angle_diff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 62\u001b[0m\n\u001b[0;32m     58\u001b[0m eval_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m  \u001b[38;5;66;03m# Evaluate and log every 10000 steps\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, total_timesteps, eval_interval):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Train for a number of timesteps\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Run an evaluation episode\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     episode_length, episode_reward, out_of_bounds, interceptions, reached_max_steps \u001b[38;5;241m=\u001b[39m run_episode(env, model)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:314\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOffPolicyAlgorithm,\n\u001b[0;32m    307\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    313\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOffPolicyAlgorithm:\n\u001b[1;32m--> 314\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must set the environment before calling learn()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:297\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise, VectorizedActionNoise)\n\u001b[0;32m    294\u001b[0m ):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise \u001b[38;5;241m=\u001b[39m VectorizedActionNoise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:423\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m     76\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m---> 77\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmaybe_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 54\u001b[0m, in \u001b[0;36mmissile_interception.reset\u001b[1;34m(self, seed)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissile_distance_to_attack()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# self.graph(self.defense, self.attack)\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, {}\n",
      "Cell \u001b[1;32mIn[8], line 176\u001b[0m, in \u001b[0;36mmissile_interception.get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mangle_conversion()\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefense[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefense[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msin_theta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcos_theta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msin_missile_angle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcos_missile_angle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_sin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_cos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_t_minus_one), ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_t_minus_one) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m0.02\u001b[39m)], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mangle_diff\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'missile_interception' object has no attribute 'angle_diff'"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# .........................................................................................................................................................\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# COPY OF THE CODE ABOVE: THIS VERSION IS FOR TESTING THE ANIMATION FUNCTION \n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "log_number = 1\n",
    "\n",
    "def run_episode(env, model):\n",
    "    # If you have only one environment wrapped, you can directly access it\n",
    "    single_env = env.envs[0]\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    animate = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    episode_data = None\n",
    "    global log_number\n",
    "\n",
    "    print(\"EPISODE NUMBER:\", log_number)\n",
    "\n",
    "    episode_data = {\n",
    "    'past_defense_positions': [], # In upgraded code, we'd only need defense_positions, this is slop, let it work for now\n",
    "    'attack_positions': [],\n",
    "    'defense_positions': [], # because you're basically storing the same data in two different places\n",
    "    }\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        episode_data['past_defense_positions'].append(deepcopy(np.array([obs[0][2], obs[0][3]])))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_data['attack_positions'].append(deepcopy(np.array([obs[0][2], obs[0][3]])))\n",
    "        episode_data['defense_positions'].append(deepcopy(np.array([obs[0][0], obs[0][1]])))\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "\n",
    "    log_number += 1\n",
    "    \n",
    "    if episode_data is not None:\n",
    "        episode_file_name = f'episode_{log_number}_v4_dqn_double.gif'\n",
    "        animate_episode(episode_data, episode_file_name)\n",
    "\n",
    "    # Now access the specific attributes from the single_env which is your actual missile_interception instance\n",
    "    return step, total_reward, single_env.out_of_bounds, single_env.interceptions, single_env.reached_max_steps\n",
    "\n",
    "# Create the environment\n",
    "env = make_vec_env(lambda: missile_interception(), n_envs=1)\n",
    "\n",
    "# Create the model\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./dqn_missile_guidance_local_v4_double\")\n",
    "\n",
    "# Create a summary writer\n",
    "summary_writer = tf.summary.create_file_writer('./dqn_missile_guidance_local_v4_double/custom_metrics')\n",
    "\n",
    "# Training loop\n",
    "total_timesteps = 2000000\n",
    "eval_interval = 100000  # Evaluate and log every 10000 steps\n",
    "\n",
    "for step in range(0, total_timesteps, eval_interval):\n",
    "    # Train for a number of timesteps\n",
    "    model.learn(total_timesteps=eval_interval, reset_num_timesteps=False)\n",
    "    \n",
    "    # Run an evaluation episode\n",
    "    episode_length, episode_reward, out_of_bounds, interceptions, reached_max_steps = run_episode(env, model)\n",
    "    \n",
    "    # Log the results\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('Evaluation/Episode Length', episode_length, step=step)\n",
    "        # Ensure episode_reward and other metrics are scalars by using .item() if they are numpy arrays or tensors\n",
    "        tf.summary.scalar('Evaluation/Episode Reward', episode_reward.item() if isinstance(episode_reward, np.ndarray) else episode_reward, step=step)\n",
    "        tf.summary.scalar('Evaluation/Out of Bounds Count', out_of_bounds.item() if isinstance(out_of_bounds, np.ndarray) else out_of_bounds, step=step)\n",
    "        tf.summary.scalar('Evaluation/Interceptions Count', interceptions.item() if isinstance(interceptions, np.ndarray) else interceptions, step=step)\n",
    "        tf.summary.scalar('Evaluation/Reached Max Steps Count', reached_max_steps.item() if isinstance(reached_max_steps, np.ndarray) else reached_max_steps, step=step)\n",
    "        summary_writer.flush()\n",
    "\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"dqn_missile_guidance_v4_double\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
