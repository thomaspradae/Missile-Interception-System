{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.4.2.6)\n",
    "Testing out v.2.4.2.5 with multiple defense missiles in order to achieve 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27866594 0.23737274]\n",
      "[-0.52058337  0.86364867]\n"
     ]
    }
   ],
   "source": [
    "class initial_conditions_missile_interception(Env):\n",
    "    def __init__(self):\n",
    "        self.radius = 0.02\n",
    "        self.create_target()\n",
    "        self.create_attack(self.target)\n",
    "    \n",
    "    def create_target(self):\n",
    "        x = random.uniform(-0.3, 0.3)\n",
    "        y = random.uniform(-0.3, 0.3)\n",
    "        self.target = np.array([x, y])\n",
    "\n",
    "    def create_attack(self, target):\n",
    "        x_side_left = random.uniform(-0.95, max(((target[0] - self.radius) - 0.5), -0.94))\n",
    "        x_side_right = random.uniform(min(((target[0] + self.radius) + 0.5), 0.94), 0.95)\n",
    "        y_below = random.uniform(max(((target[1] - self.radius) - 0.5), -0.94), -0.95)\n",
    "        y_above = random.uniform(max(((target[1] + self.radius) + 0.5), 0.94), 0.95)\n",
    "        x_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_below_x_inclusive = np.array([x_inclusive, y_below])\n",
    "        y_above_x_inclusive = np.array([x_inclusive, y_above])\n",
    "        x_left_y_inclusive = np.array([x_side_left, y_inclusive])\n",
    "        x_right_y_inclusive = np.array([x_side_right, y_inclusive])\n",
    "\n",
    "        self.attack = random.choice([y_below_x_inclusive, y_above_x_inclusive, x_left_y_inclusive, x_right_y_inclusive])\n",
    "\n",
    "    def reset(self):\n",
    "        return self.target, self.attack\n",
    "\n",
    "init = initial_conditions_missile_interception()\n",
    "init_coords = init.reset()\n",
    "print(init_coords[0])\n",
    "print(init_coords[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "from gymnasium import Env\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import gymnasium as gym\n",
    "import numpy as npm\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import tensorflow as tf\n",
    "\n",
    "# # Set seed for reproducibility\n",
    "# seed = 42\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "class missile_interception(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.discrete.Discrete(5)\n",
    "        \n",
    "        low = np.array([-1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -2, -2])\n",
    "        high = np.array([1, 1, 1, 1, 1, 1, 2*math.pi, 2*math.pi, 2*math.pi, math.pi, 2.9, 2.9, 1, 1, 1, 1, 2, 2])\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "        self.radius = 0.02\n",
    "\n",
    "        self.episode_count = 0\n",
    "        self.distance_t_minus_one = 0\n",
    "        self.distance_change = 0\n",
    "\n",
    "        self.out_of_bounds = 0\n",
    "        self.interceptions = 0\n",
    "        self.reached_max_steps = 0\n",
    "        self.enemy_impacts = 0\n",
    "\n",
    "        self.defense_positions = []\n",
    "        self.attack_positions = []\n",
    "        self.attack_starting_position = 0\n",
    "\n",
    "        self.max_steps_per_episode = 150\n",
    "        self.activate_value = 0\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        global init_coords\n",
    "        self.dict_state = {}\n",
    "        self.activate_enemy_impact = False\n",
    "        self.defense_positions = []\n",
    "        self.attack_positions = []\n",
    "        self.reward = 0\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.target = init_coords[0]\n",
    "        self.create_defense(self.target)\n",
    "        self.attack = init_coords[1]\n",
    "\n",
    "        self.calc_defense_attack_theta(self.defense, self.attack)\n",
    "        self.calc_attack_target_theta(self.attack, self.target)\n",
    "        self.initial_defense_angle()\n",
    "        self.calc_defense_attack_distance()\n",
    "        self.calc_attack_target_distance()\n",
    "        self.attack_starting_position = self.attack.copy()\n",
    "\n",
    "        self.get_state()\n",
    "        print(\"Resetting ------------------------------------\")\n",
    "        print(\"attack: \", self.attack)\n",
    "        print(\"----------------------------------------------\")\n",
    "        \n",
    "        return self.state, {}\n",
    "\n",
    "    def create_target(self):\n",
    "        x = random.uniform(-0.3, 0.3)\n",
    "        y = random.uniform(-0.3, 0.3)\n",
    "        self.target = np.array([x, y])\n",
    "\n",
    "    def create_defense(self, target):\n",
    "        x = random.uniform((target[0] - self.radius) - 0.15, (target[0] + self.radius) + 0.15)\n",
    "        y = random.uniform((target[1] - self.radius) - 0.15, (target[1] + self.radius) + 0.15)\n",
    "        self.defense = np.array([x, y])\n",
    "\n",
    "    def create_attack(self, target):\n",
    "        x_side_left = random.uniform(-0.95, max(((target[0] - self.radius) - 0.5), -0.94))\n",
    "        x_side_right = random.uniform(min(((target[0] + self.radius) + 0.5), 0.94), 0.95)\n",
    "        y_below = random.uniform(max(((target[1] - self.radius) - 0.5), -0.94), -0.95)\n",
    "        y_above = random.uniform(max(((target[1] + self.radius) + 0.5), 0.94), 0.95)\n",
    "        x_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_below_x_inclusive = np.array([x_inclusive, y_below])\n",
    "        y_above_x_inclusive = np.array([x_inclusive, y_above])\n",
    "        x_left_y_inclusive = np.array([x_side_left, y_inclusive])\n",
    "        x_right_y_inclusive = np.array([x_side_right, y_inclusive])\n",
    "\n",
    "        self.attack = random.choice([y_below_x_inclusive, y_above_x_inclusive, x_left_y_inclusive, x_right_y_inclusive])\n",
    "\n",
    "    def calc_defense_attack_theta(self, defense, attack):\n",
    "\n",
    "        # create an adjacent point of the form (attack_x, defense_y)\n",
    "        adjacent_point = np.array([attack[0], defense[1]])\n",
    "\n",
    "        # calculate the distance between the adjacent point and the defense, attack points\n",
    "        adj_point_defense_len = abs(defense[0] - adjacent_point[0]) \n",
    "        adj_point_attack_len = abs(attack[1] - adjacent_point[1])\n",
    "\n",
    "        # calculate the angle, using soh cah toa, where adj_point_defense_len is the adjacent side and adj_point_attack_len is the opposite side\n",
    "        self.defense_attack_theta = np.arctan(adj_point_attack_len / adj_point_defense_len)\n",
    "        \n",
    "        if attack[0] > defense[0]:\n",
    "            if attack[1] > defense[1]:\n",
    "                self.defense_attack_theta = self.defense_attack_theta # 1st quadrant\n",
    "            else: \n",
    "                self.defense_attack_theta = (2*math.pi) - self.defense_attack_theta # 360 - theta\n",
    "        else:\n",
    "            if attack[1] > defense[1]:\n",
    "                self.defense_attack_theta = math.pi - self.defense_attack_theta # 180 - theta\n",
    "            else:\n",
    "                self.defense_attack_theta = math.pi + self.defense_attack_theta # 180 + theta\n",
    "\n",
    "    def calc_attack_target_theta(self, attack, target):\n",
    "        # create an adjacent point of the form (target_x, attack_y)\n",
    "        adjacent_point = np.array([target[0], attack[1]])\n",
    "\n",
    "        # calculate the distance between the adjacent point and the attack, target points\n",
    "        adj_point_attack_len = abs(attack[0] - adjacent_point[0])\n",
    "        adj_point_target_len = abs(target[1] - adjacent_point[1])\n",
    "        \n",
    "        # calculate the angle, using soh cah toa, where adj_point_attack_len is the adjacent side and adj_point_target_len is the opposite side\n",
    "        self.attack_target_theta = np.arctan(adj_point_target_len / adj_point_attack_len)\n",
    "\n",
    "        if target[0] > attack[0]:\n",
    "            if target[1] > attack[1]:\n",
    "                self.attack_target_theta = self.attack_target_theta\n",
    "            else:\n",
    "                self.attack_target_theta = (2*math.pi) - self.attack_target_theta\n",
    "        else:\n",
    "            if target[1] > attack[1]:\n",
    "                self.attack_target_theta = math.pi - self.attack_target_theta\n",
    "            else:\n",
    "                self.attack_target_theta = math.pi + self.attack_target_theta        \n",
    "\n",
    "    def initial_defense_angle(self):\n",
    "        self.defense_angle = np.random.uniform((self.defense_attack_theta - 2.35619), (self.defense_attack_theta + 2.35619))\n",
    "        if self.defense_angle > 2*math.pi:\n",
    "            self.defense_angle = self.defense_angle - 2*math.pi\n",
    "        elif self.defense_angle < 0:\n",
    "            self.defense_angle = 2*math.pi + self.defense_angle\n",
    "\n",
    "    def calculate_distance(self, point1, point2):\n",
    "        return math.hypot(point1[0] - point2[0], point1[1] - point2[1])\n",
    "    \n",
    "    def calc_defense_attack_distance(self):\n",
    "        self.defense_attack_distance = (self.calculate_distance(self.defense, self.attack) - (2 * self.radius))\n",
    "\n",
    "    def calc_attack_target_distance(self):\n",
    "        self.attack_target_distance = (self.calculate_distance(self.attack, self.target) - (2 * self.radius))\n",
    "\n",
    "    def calc_defense_angle(self, action):\n",
    "        if action == 0:\n",
    "            self.defense_angle = self.defense_angle \n",
    "        elif action == 1:\n",
    "            self.defense_angle += 0.174532925\n",
    "        elif action == 2:\n",
    "            self.defense_angle += 0.523599\n",
    "        elif action == 3:\n",
    "            self.defense_angle -= 0.174532925\n",
    "        elif action == 4:\n",
    "            self.defense_angle -= 0.523599\n",
    "        \n",
    "        if self.defense_angle > 2*math.pi:\n",
    "            self.defense_angle = self.defense_angle - 2*math.pi\n",
    "        elif self.defense_angle < 0:\n",
    "            self.defense_angle = 2*math.pi + self.defense_angle\n",
    "\n",
    "    def update_coords(self):\n",
    "        self.defense[0] += (0.02 * math.cos(self.defense_angle)) # gotta test this\n",
    "        self.defense[1] += (0.02 * math.sin(self.defense_angle))\n",
    "        self.attack[0] += (0.02 * math.cos(self.attack_target_theta))\n",
    "        self.attack[1] += (0.02 * math.sin(self.attack_target_theta))\n",
    "        self.defense_positions.append(self.defense.copy())\n",
    "        self.attack_positions.append(self.attack.copy())\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        self.calc_defense_attack_distance()\n",
    "        self.calc_attack_target_distance()\n",
    "\n",
    "        if self.attack_target_distance < 0:\n",
    "            print(\"ENEMY HIT!\")\n",
    "            self.activate_enemy_impact = True\n",
    "            self.activate_value = 0\n",
    "            self.dict_state = self.get_state_dict()\n",
    "            self.reward = -10000\n",
    "            self.done = True\n",
    "            self.enemy_impacts += 1\n",
    "\n",
    "        elif self.defense_attack_distance < 0:\n",
    "            print(\"HIT!\")\n",
    "            self.activate_interception = True\n",
    "            self.reward = 10000\n",
    "            self.done = True\n",
    "            self.interceptions += 1\n",
    "        else:\n",
    "            self.angle_diff = abs(self.defense_attack_theta - self.defense_angle)\n",
    "            self.angle_diff = min(self.angle_diff, 2*math.pi - self.angle_diff)\n",
    "            self.reward = 1/self.angle_diff\n",
    "                \n",
    "        if self.defense[0] < -1 or self.defense[0] > 1 or self.defense[1] < -1 or self.defense[1] > 1:\n",
    "            print(\"OUT OF BOUNDS\")\n",
    "            self.reward = -1000\n",
    "            self.done = True\n",
    "            self.out_of_bounds += 1\n",
    "\n",
    "    def angle_conversion(self):\n",
    "        self.sin_defense_attack_theta, self.sin_defense_angle = np.sin(self.defense_attack_theta), np.sin(self.defense_angle)\n",
    "        self.cos_defense_attack_theta, self.cos_defense_angle = np.cos(self.defense_attack_theta), np.cos(self.defense_angle)\n",
    "\n",
    "        self.delta_sin = self.sin_defense_attack_theta - self.sin_defense_angle\n",
    "        self.delta_cos = self.cos_defense_attack_theta - self.cos_defense_angle\n",
    "\n",
    "    def get_state(self):\n",
    "        self.angle_conversion()\n",
    "\n",
    "        self.state = np.array([\n",
    "            self.attack[0], self.attack[1], \n",
    "            self.defense[0], self.defense[1], \n",
    "            self.target[0], self.target[1],\n",
    "            self.defense_attack_theta, self.attack_target_theta,\n",
    "            self.defense_angle,\n",
    "            min(abs(self.defense_attack_theta - self.defense_angle), 2*math.pi - abs(self.defense_attack_theta - self.defense_angle)),\n",
    "            self.defense_attack_distance,\n",
    "            self.attack_target_distance,\n",
    "            self.sin_defense_attack_theta, self.cos_defense_attack_theta, \n",
    "            self.sin_defense_angle, self.cos_defense_angle,\n",
    "            self.delta_sin, self.delta_cos\n",
    "        ])\n",
    "\n",
    "    def get_state_dict(self):\n",
    "        return {\n",
    "            \"self.activate\": self.activate_enemy_impact,\n",
    "            \"attack_x\": self.attack[0],\n",
    "            \"attack_y\": self.attack[1],\n",
    "            \"defense_x\": self.defense[0],\n",
    "            \"defense_y\": self.defense[1],\n",
    "            \"target_x\": self.target[0],\n",
    "            \"target_y\": self.target[1],\n",
    "            \"defense_attack_theta\": self.defense_attack_theta,\n",
    "            \"attack_target_theta\": self.attack_target_theta,\n",
    "            \"defense_angle\": self.defense_angle,\n",
    "            \"angle_diff\": min(abs(self.defense_attack_theta - self.defense_angle), 2*math.pi - abs(self.defense_attack_theta - self.defense_angle)),\n",
    "            \"distance_attack_missile\": self.defense_attack_distance,\n",
    "            \"distance_attack_target\": self.attack_target_distance,\n",
    "            \"sin_defense_attack_theta\": self.sin_defense_attack_theta,\n",
    "            \"cos_defense_attack_theta\": self.cos_defense_attack_theta,\n",
    "            \"sin_defense_angle\": self.sin_defense_angle,\n",
    "            \"cos_defense_angle\": self.cos_defense_angle,\n",
    "            \"delta_sin\": self.delta_sin,\n",
    "            \"delta_cos\": self.delta_cos\n",
    "        }\n",
    "                 \n",
    "    def step(self, action):\n",
    "        print(\"Inside Step -------------------------------------------\")\n",
    "        print(\"attack: \", self.attack)\n",
    "        self.distance_t_minus_one = self.defense_attack_distance\n",
    "        self.calc_defense_angle(action)\n",
    "        self.update_coords()\n",
    "        self.calc_defense_attack_theta(self.defense, self.attack)\n",
    "        self.calculate_reward()\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= self.max_steps_per_episode:\n",
    "            print(\"MAX STEPS REACHED\")\n",
    "            self.done = True\n",
    "            self.reward = -1000\n",
    "            self.reached_max_steps += 1\n",
    "\n",
    "        self.get_state()\n",
    "        # print(\"/////////////////////////////////////////\")\n",
    "        # print(\"dict state: \", dict_state)\n",
    "        # print(\"state: \", state)\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print(\"attack end state: \", self.attack)\n",
    "        print(\"-------------------------------------------------------\")\n",
    "        return self.state, self.reward, self.done, False, {'activated': self.activate_enemy_impact}\n",
    "\n",
    "    def graph(self, defense, attack, target):\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.xlim(-1, 1)\n",
    "        plt.ylim(-1, 1)\n",
    "\n",
    "        plt.axhline(0, color='black', linewidth=0.5)\n",
    "        plt.axvline(0, color='black', linewidth=0.5)\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot trails\n",
    "        if self.defense_positions:\n",
    "            defense_xs, defense_ys = zip(*self.defense_positions)\n",
    "            ax.plot(defense_xs, defense_ys, color='#858585', label='Defense Trail')  # Blue line for defense\n",
    "\n",
    "        if self.attack_positions:\n",
    "            attack_xs, attack_ys = zip(*self.attack_positions)\n",
    "            ax.plot(attack_xs, attack_ys, color='#FFA281', label='Attack Trail')  # Red line for attack\n",
    "\n",
    "        # Plot current positions\n",
    "        plt.scatter(defense[0], defense[1], color='#1C1C1C')\n",
    "        plt.scatter(attack[0], attack[1], color='#FF5A1F')\n",
    "        plt.scatter(self.target[0], self.target[1], color='#85A3FF')\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "from gymnasium import Env\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import gymnasium as gym\n",
    "import numpy as npm\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import tensorflow as tf\n",
    "\n",
    "# # Set seed for reproducibility\n",
    "# seed = 42\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "class carlos(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.discrete.Discrete(5)\n",
    "        \n",
    "        low = np.array([-1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -2, -2])\n",
    "        high = np.array([1, 1, 1, 1, 1, 1, 2*math.pi, 2*math.pi, 2*math.pi, math.pi, 2.9, 2.9, 1, 1, 1, 1, 2, 2])\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "        self.radius = 0.02\n",
    "\n",
    "        self.episode_count = 0\n",
    "        self.distance_t_minus_one = 0\n",
    "        self.distance_change = 0\n",
    "\n",
    "        self.out_of_bounds = 0\n",
    "        self.interceptions = 0\n",
    "        self.reached_max_steps = 0\n",
    "        self.enemy_impacts = 0\n",
    "\n",
    "        self.defense_positions = []\n",
    "        self.attack_positions = []\n",
    "        self.attack_starting_position = 0\n",
    "\n",
    "        self.max_steps_per_episode = 150\n",
    "        self.activate_value = 0\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        global init_coords\n",
    "        self.dict_state = {}\n",
    "        self.activate_enemy_impact = False\n",
    "        self.defense_positions = []\n",
    "        self.attack_positions = []\n",
    "        self.reward = 0\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.target = init_coords[0]\n",
    "        self.create_defense(self.target)\n",
    "        self.attack = init_coords[1]\n",
    "\n",
    "        self.calc_defense_attack_theta(self.defense, self.attack)\n",
    "        self.calc_attack_target_theta(self.attack, self.target)\n",
    "        self.initial_defense_angle()\n",
    "        self.calc_defense_attack_distance()\n",
    "        self.calc_attack_target_distance()\n",
    "        self.attack_starting_position = self.attack.copy()\n",
    "\n",
    "        self.get_state()\n",
    "        print(\"Resetting ------------------------------------\")\n",
    "        print(\"attack: \", self.attack)\n",
    "        print(\"----------------------------------------------\")\n",
    "        \n",
    "        return self.state, {}\n",
    "\n",
    "    def create_target(self):\n",
    "        x = random.uniform(-0.3, 0.3)\n",
    "        y = random.uniform(-0.3, 0.3)\n",
    "        self.target = np.array([x, y])\n",
    "\n",
    "    def create_defense(self, target):\n",
    "        x = random.uniform((target[0] - self.radius) - 0.15, (target[0] + self.radius) + 0.15)\n",
    "        y = random.uniform((target[1] - self.radius) - 0.15, (target[1] + self.radius) + 0.15)\n",
    "        self.defense = np.array([x, y])\n",
    "\n",
    "    def create_attack(self, target):\n",
    "        x_side_left = random.uniform(-0.95, max(((target[0] - self.radius) - 0.5), -0.94))\n",
    "        x_side_right = random.uniform(min(((target[0] + self.radius) + 0.5), 0.94), 0.95)\n",
    "        y_below = random.uniform(max(((target[1] - self.radius) - 0.5), -0.94), -0.95)\n",
    "        y_above = random.uniform(max(((target[1] + self.radius) + 0.5), 0.94), 0.95)\n",
    "        x_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_below_x_inclusive = np.array([x_inclusive, y_below])\n",
    "        y_above_x_inclusive = np.array([x_inclusive, y_above])\n",
    "        x_left_y_inclusive = np.array([x_side_left, y_inclusive])\n",
    "        x_right_y_inclusive = np.array([x_side_right, y_inclusive])\n",
    "\n",
    "        self.attack = random.choice([y_below_x_inclusive, y_above_x_inclusive, x_left_y_inclusive, x_right_y_inclusive])\n",
    "\n",
    "    def calc_defense_attack_theta(self, defense, attack):\n",
    "\n",
    "        # create an adjacent point of the form (attack_x, defense_y)\n",
    "        adjacent_point = np.array([attack[0], defense[1]])\n",
    "\n",
    "        # calculate the distance between the adjacent point and the defense, attack points\n",
    "        adj_point_defense_len = abs(defense[0] - adjacent_point[0]) \n",
    "        adj_point_attack_len = abs(attack[1] - adjacent_point[1])\n",
    "\n",
    "        # calculate the angle, using soh cah toa, where adj_point_defense_len is the adjacent side and adj_point_attack_len is the opposite side\n",
    "        self.defense_attack_theta = np.arctan(adj_point_attack_len / adj_point_defense_len)\n",
    "        \n",
    "        if attack[0] > defense[0]:\n",
    "            if attack[1] > defense[1]:\n",
    "                self.defense_attack_theta = self.defense_attack_theta # 1st quadrant\n",
    "            else: \n",
    "                self.defense_attack_theta = (2*math.pi) - self.defense_attack_theta # 360 - theta\n",
    "        else:\n",
    "            if attack[1] > defense[1]:\n",
    "                self.defense_attack_theta = math.pi - self.defense_attack_theta # 180 - theta\n",
    "            else:\n",
    "                self.defense_attack_theta = math.pi + self.defense_attack_theta # 180 + theta\n",
    "\n",
    "    def calc_attack_target_theta(self, attack, target):\n",
    "        # create an adjacent point of the form (target_x, attack_y)\n",
    "        adjacent_point = np.array([target[0], attack[1]])\n",
    "\n",
    "        # calculate the distance between the adjacent point and the attack, target points\n",
    "        adj_point_attack_len = abs(attack[0] - adjacent_point[0])\n",
    "        adj_point_target_len = abs(target[1] - adjacent_point[1])\n",
    "        \n",
    "        # calculate the angle, using soh cah toa, where adj_point_attack_len is the adjacent side and adj_point_target_len is the opposite side\n",
    "        self.attack_target_theta = np.arctan(adj_point_target_len / adj_point_attack_len)\n",
    "\n",
    "        if target[0] > attack[0]:\n",
    "            if target[1] > attack[1]:\n",
    "                self.attack_target_theta = self.attack_target_theta\n",
    "            else:\n",
    "                self.attack_target_theta = (2*math.pi) - self.attack_target_theta\n",
    "        else:\n",
    "            if target[1] > attack[1]:\n",
    "                self.attack_target_theta = math.pi - self.attack_target_theta\n",
    "            else:\n",
    "                self.attack_target_theta = math.pi + self.attack_target_theta        \n",
    "\n",
    "    def initial_defense_angle(self):\n",
    "        self.defense_angle = np.random.uniform((self.defense_attack_theta - 2.35619), (self.defense_attack_theta + 2.35619))\n",
    "        if self.defense_angle > 2*math.pi:\n",
    "            self.defense_angle = self.defense_angle - 2*math.pi\n",
    "        elif self.defense_angle < 0:\n",
    "            self.defense_angle = 2*math.pi + self.defense_angle\n",
    "\n",
    "    def calculate_distance(self, point1, point2):\n",
    "        return math.hypot(point1[0] - point2[0], point1[1] - point2[1])\n",
    "    \n",
    "    def calc_defense_attack_distance(self):\n",
    "        self.defense_attack_distance = (self.calculate_distance(self.defense, self.attack) - (2 * self.radius))\n",
    "\n",
    "    def calc_attack_target_distance(self):\n",
    "        self.attack_target_distance = (self.calculate_distance(self.attack, self.target) - (2 * self.radius))\n",
    "\n",
    "    def calc_defense_angle(self, action):\n",
    "        if action == 0:\n",
    "            self.defense_angle = self.defense_angle \n",
    "        elif action == 1:\n",
    "            self.defense_angle += 0.174532925\n",
    "        elif action == 2:\n",
    "            self.defense_angle += 0.523599\n",
    "        elif action == 3:\n",
    "            self.defense_angle -= 0.174532925\n",
    "        elif action == 4:\n",
    "            self.defense_angle -= 0.523599\n",
    "        \n",
    "        if self.defense_angle > 2*math.pi:\n",
    "            self.defense_angle = self.defense_angle - 2*math.pi\n",
    "        elif self.defense_angle < 0:\n",
    "            self.defense_angle = 2*math.pi + self.defense_angle\n",
    "\n",
    "    def update_coords(self):\n",
    "        self.defense[0] += (0.02 * math.cos(self.defense_angle)) # gotta test this\n",
    "        self.defense[1] += (0.02 * math.sin(self.defense_angle))\n",
    "        self.attack[0] += (0.02 * math.cos(self.attack_target_theta))\n",
    "        self.attack[1] += (0.02 * math.sin(self.attack_target_theta))\n",
    "        self.defense_positions.append(self.defense.copy())\n",
    "        self.attack_positions.append(self.attack.copy())\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        self.calc_defense_attack_distance()\n",
    "        self.calc_attack_target_distance()\n",
    "\n",
    "        if self.attack_target_distance < 0:\n",
    "            print(\"ENEMY HIT!\")\n",
    "            self.activate_enemy_impact = True\n",
    "            self.activate_value = 0\n",
    "            self.dict_state = self.get_state_dict()\n",
    "            self.reward = -10000\n",
    "            self.done = True\n",
    "            self.enemy_impacts += 1\n",
    "\n",
    "        elif self.defense_attack_distance < 0:\n",
    "            print(\"HIT!\")\n",
    "            self.activate_interception = True\n",
    "            self.reward = 10000\n",
    "            self.done = True\n",
    "            self.interceptions += 1\n",
    "        else:\n",
    "            self.angle_diff = abs(self.defense_attack_theta - self.defense_angle)\n",
    "            self.angle_diff = min(self.angle_diff, 2*math.pi - self.angle_diff)\n",
    "            self.reward = 1/self.angle_diff\n",
    "                \n",
    "        if self.defense[0] < -1 or self.defense[0] > 1 or self.defense[1] < -1 or self.defense[1] > 1:\n",
    "            print(\"OUT OF BOUNDS\")\n",
    "            self.reward = -1000\n",
    "            self.done = True\n",
    "            self.out_of_bounds += 1\n",
    "\n",
    "    def angle_conversion(self):\n",
    "        self.sin_defense_attack_theta, self.sin_defense_angle = np.sin(self.defense_attack_theta), np.sin(self.defense_angle)\n",
    "        self.cos_defense_attack_theta, self.cos_defense_angle = np.cos(self.defense_attack_theta), np.cos(self.defense_angle)\n",
    "\n",
    "        self.delta_sin = self.sin_defense_attack_theta - self.sin_defense_angle\n",
    "        self.delta_cos = self.cos_defense_attack_theta - self.cos_defense_angle\n",
    "\n",
    "    def get_state(self):\n",
    "        self.angle_conversion()\n",
    "\n",
    "        self.state = np.array([\n",
    "            self.attack[0], self.attack[1], \n",
    "            self.defense[0], self.defense[1], \n",
    "            self.target[0], self.target[1],\n",
    "            self.defense_attack_theta, self.attack_target_theta,\n",
    "            self.defense_angle,\n",
    "            min(abs(self.defense_attack_theta - self.defense_angle), 2*math.pi - abs(self.defense_attack_theta - self.defense_angle)),\n",
    "            self.defense_attack_distance,\n",
    "            self.attack_target_distance,\n",
    "            self.sin_defense_attack_theta, self.cos_defense_attack_theta, \n",
    "            self.sin_defense_angle, self.cos_defense_angle,\n",
    "            self.delta_sin, self.delta_cos\n",
    "        ])\n",
    "\n",
    "    def get_state_dict(self):\n",
    "        return {\n",
    "            \"self.activate\": self.activate_enemy_impact,\n",
    "            \"attack_x\": self.attack[0],\n",
    "            \"attack_y\": self.attack[1],\n",
    "            \"defense_x\": self.defense[0],\n",
    "            \"defense_y\": self.defense[1],\n",
    "            \"target_x\": self.target[0],\n",
    "            \"target_y\": self.target[1],\n",
    "            \"defense_attack_theta\": self.defense_attack_theta,\n",
    "            \"attack_target_theta\": self.attack_target_theta,\n",
    "            \"defense_angle\": self.defense_angle,\n",
    "            \"angle_diff\": min(abs(self.defense_attack_theta - self.defense_angle), 2*math.pi - abs(self.defense_attack_theta - self.defense_angle)),\n",
    "            \"distance_attack_missile\": self.defense_attack_distance,\n",
    "            \"distance_attack_target\": self.attack_target_distance,\n",
    "            \"sin_defense_attack_theta\": self.sin_defense_attack_theta,\n",
    "            \"cos_defense_attack_theta\": self.cos_defense_attack_theta,\n",
    "            \"sin_defense_angle\": self.sin_defense_angle,\n",
    "            \"cos_defense_angle\": self.cos_defense_angle,\n",
    "            \"delta_sin\": self.delta_sin,\n",
    "            \"delta_cos\": self.delta_cos\n",
    "        }\n",
    "                 \n",
    "    def step(self, action):\n",
    "        print(\"attack: \", self.attack)\n",
    "        self.distance_t_minus_one = self.defense_attack_distance\n",
    "        self.calc_defense_angle(action)\n",
    "        self.update_coords()\n",
    "        self.calc_defense_attack_theta(self.defense, self.attack)\n",
    "        self.calculate_reward()\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= self.max_steps_per_episode:\n",
    "            print(\"MAX STEPS REACHED\")\n",
    "            self.done = True\n",
    "            self.reward = -1000\n",
    "            self.reached_max_steps += 1\n",
    "\n",
    "        self.get_state()\n",
    "        # print(\"/////////////////////////////////////////\")\n",
    "        # print(\"dict state: \", dict_state)\n",
    "        # print(\"state: \", state)\n",
    "        print(\"attack end state: \", self.attack)\n",
    "        return self.state, self.reward, self.done, False, {'activated': self.activate_enemy_impact}\n",
    "\n",
    "    def graph(self, defense, attack, target):\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.xlim(-1, 1)\n",
    "        plt.ylim(-1, 1)\n",
    "\n",
    "        plt.axhline(0, color='black', linewidth=0.5)\n",
    "        plt.axvline(0, color='black', linewidth=0.5)\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot trails\n",
    "        if self.defense_positions:\n",
    "            defense_xs, defense_ys = zip(*self.defense_positions)\n",
    "            ax.plot(defense_xs, defense_ys, color='#858585', label='Defense Trail')  # Blue line for defense\n",
    "\n",
    "        if self.attack_positions:\n",
    "            attack_xs, attack_ys = zip(*self.attack_positions)\n",
    "            ax.plot(attack_xs, attack_ys, color='#FFA281', label='Attack Trail')  # Red line for attack\n",
    "\n",
    "        # Plot current positions\n",
    "        plt.scatter(defense[0], defense[1], color='#1C1C1C')\n",
    "        plt.scatter(attack[0], attack[1], color='#FF5A1F')\n",
    "        plt.scatter(self.target[0], self.target[1], color='#85A3FF')\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = missile_interception()\n",
    "env.reset()\n",
    "\n",
    "print(env.target)\n",
    "print(env.defense)\n",
    "print(env.attack)\n",
    "env.graph(env.defense, env.attack, env.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Custom environment import placeholder\n",
    "# from your_custom_environment import missile_interception, initial_conditions_missile_interception\n",
    "\n",
    "model_path = \"dqn_missile_guidance_v(2.4.2.5)_PROD_11\"\n",
    "model = DQN.load(model_path)\n",
    "\n",
    "# Initialize environments\n",
    "env1 = missile_interception()\n",
    "env2 = missile_interception()\n",
    "envs = [env1, env2]\n",
    "\n",
    "# Initialize episode data\n",
    "episode_data = [{\n",
    "    'past_defense_positions': [],\n",
    "    'past_attack_positions': [],\n",
    "    'attack_positions': [],\n",
    "    'defense_positions': [],\n",
    "    'target_position': [],\n",
    "    'actions': [],\n",
    "    'rewards': [],\n",
    "    'defense_angle': [],\n",
    "    'defense_attack_theta': [],\n",
    "} for _ in envs]\n",
    "\n",
    "# Initialize conditions and reset environments\n",
    "init = initial_conditions_missile_interception()\n",
    "init_coords = init.reset()\n",
    "obs_list = [env.reset()[0] for env in envs]  \n",
    "\n",
    "# Set initial target positions\n",
    "for i, obs in enumerate(obs_list):\n",
    "    episode_data[i]['target_position'] = obs[4:6]\n",
    "\n",
    "done_list = [False] * len(envs)\n",
    "total_rewards = [0] * len(envs)\n",
    "steps = [0] * len(envs)\n",
    "\n",
    "while not any(done_list):\n",
    "    print(\"Env: \", i)\n",
    "    obs = np.array([obs_list[i]]) \n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    obs_list[i] = obs\n",
    "    total_rewards[i] += reward\n",
    "    steps[i] += 1\n",
    "    done_list[i] = done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = initial_conditions_missile_interception()\n",
    "init_coords = init.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting ------------------------------------\n",
      "attack:  [-0.54880622 -0.65651074]\n",
      "----------------------------------------------\n",
      "Resetting ------------------------------------\n",
      "attack:  [-0.54880622 -0.65651074]\n",
      "----------------------------------------------\n",
      "Env 0 bef:  [-0.54880625 -0.6565108 ]\n",
      "[[-0.54880625 -0.6565108  -0.05147605 -0.09613057 -0.01159825 -0.0807585\n",
      "   3.9865303   0.8200165   4.215051    0.22852086  0.70924175  0.7474535\n",
      "  -0.7479297  -0.66377795 -0.87885576 -0.4770876   0.13092604 -0.18669035]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.54880622 -0.65651074]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.53516204 -0.6418876 ]\n",
      "-------------------------------------------------------\n",
      "[[-0.53516203 -0.6418876  -0.06392508 -0.11178374 -0.01159825 -0.0807585\n",
      "   3.9857113   0.8200165   4.0405183   0.05480688  0.66927737  0.72745353\n",
      "  -0.74738586 -0.66439027 -0.7826586  -0.62245125  0.03527271 -0.04193898]]\n",
      "Env 0:  [-0.53516203 -0.6418876 ]\n",
      "Env 0 bef:  [-0.53516203 -0.6418876 ]\n",
      "[[-0.53516203 -0.6418876  -0.06392508 -0.11178374 -0.01159825 -0.0807585\n",
      "   3.9857113   0.8200165   4.0405183   0.05480688  0.66927737  0.72745353\n",
      "  -0.74738586 -0.66439027 -0.7826586  -0.62245125  0.03527271 -0.04193898]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.53516204 -0.6418876 ]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.52151786 -0.62726446]\n",
      "-------------------------------------------------------\n",
      "[[-0.5215179  -0.62726444 -0.07637411 -0.1274369  -0.01159825 -0.0807585\n",
      "   3.9847946   0.8200165   4.0405183   0.05572363  0.62931347  0.70745355\n",
      "  -0.74677646 -0.6650751  -0.7826586  -0.62245125  0.03588211 -0.04262387]]\n",
      "Env 0:  [-0.5215179  -0.62726444]\n",
      "Env 0 bef:  [-0.5215179  -0.62726444]\n",
      "[[-0.5215179  -0.62726444 -0.07637411 -0.1274369  -0.01159825 -0.0807585\n",
      "   3.9847946   0.8200165   4.0405183   0.05572363  0.62931347  0.70745355\n",
      "  -0.74677646 -0.6650751  -0.7826586  -0.62245125  0.03588211 -0.04262387]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.52151786 -0.62726446]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.50787368 -0.61264132]\n",
      "-------------------------------------------------------\n",
      "[[-0.50787365 -0.61264133 -0.08882313 -0.14309008 -0.01159825 -0.0807585\n",
      "   3.9837613   0.8200165   4.0405183   0.05675681  0.5893502   0.6874535\n",
      "  -0.7460889  -0.66584635 -0.7826586  -0.62245125  0.03656965 -0.04339506]]\n",
      "Env 0:  [-0.50787365 -0.61264133]\n",
      "Env 0 bef:  [-0.50787365 -0.61264133]\n",
      "[[-0.50787365 -0.61264133 -0.08882313 -0.14309008 -0.01159825 -0.0807585\n",
      "   3.9837613   0.8200165   4.0405183   0.05675681  0.5893502   0.6874535\n",
      "  -0.7460889  -0.66584635 -0.7826586  -0.62245125  0.03656965 -0.04339506]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.50787368 -0.61264132]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.49422949 -0.59801817]\n",
      "-------------------------------------------------------\n",
      "[[-0.4942295  -0.59801817 -0.10127216 -0.15874326 -0.01159825 -0.0807585\n",
      "   3.982588    0.8200165   4.0405183   0.05793009  0.54938775  0.6674535\n",
      "  -0.74530715 -0.6667212  -0.7826586  -0.62245125  0.03735138 -0.04426998]]\n",
      "Env 0:  [-0.4942295  -0.59801817]\n",
      "Env 0 bef:  [-0.4942295  -0.59801817]\n",
      "[[-0.4942295  -0.59801817 -0.10127216 -0.15874326 -0.01159825 -0.0807585\n",
      "   3.982588    0.8200165   4.0405183   0.05793009  0.54938775  0.6674535\n",
      "  -0.74530715 -0.6667212  -0.7826586  -0.62245125  0.03735138 -0.04426998]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.49422949 -0.59801817]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.48058531 -0.58339503]\n",
      "-------------------------------------------------------\n",
      "[[-0.4805853  -0.583395   -0.11372118 -0.17439643 -0.01159825 -0.0807585\n",
      "   3.981244    0.8200165   4.0405183   0.05927405  0.5094262   0.64745355\n",
      "  -0.74441046 -0.6677223  -0.7826586  -0.62245125  0.03824811 -0.04527104]]\n",
      "Env 0:  [-0.4805853 -0.583395 ]\n",
      "Env 0 bef:  [-0.4805853 -0.583395 ]\n",
      "[[-0.4805853  -0.583395   -0.11372118 -0.17439643 -0.01159825 -0.0807585\n",
      "   3.981244    0.8200165   4.0405183   0.05927405  0.5094262   0.64745355\n",
      "  -0.74441046 -0.6677223  -0.7826586  -0.62245125  0.03824811 -0.04527104]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.48058531 -0.58339503]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.46694113 -0.56877189]\n",
      "-------------------------------------------------------\n",
      "[[-0.46694112 -0.5687719  -0.1261702  -0.1900496  -0.01159825 -0.0807585\n",
      "   3.9796894   0.8200165   4.0405183   0.06082884  0.4694658   0.6274535\n",
      "  -0.74337137 -0.6688789  -0.7826586  -0.62245125  0.03928718 -0.04642763]]\n",
      "Env 0:  [-0.46694112 -0.5687719 ]\n",
      "Env 0 bef:  [-0.46694112 -0.5687719 ]\n",
      "[[-0.46694112 -0.5687719  -0.1261702  -0.1900496  -0.01159825 -0.0807585\n",
      "   3.9796894   0.8200165   4.0405183   0.06082884  0.4694658   0.6274535\n",
      "  -0.74337137 -0.6688789  -0.7826586  -0.62245125  0.03928718 -0.04642763]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.46694113 -0.56877189]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.45329695 -0.55414875]\n",
      "-------------------------------------------------------\n",
      "[[-0.45329696 -0.55414873 -0.13861923 -0.20570277 -0.01159825 -0.0807585\n",
      "   3.97787     0.8200165   4.0405183   0.06264829  0.42950684  0.6074535\n",
      "  -0.74215317 -0.6702303  -0.7826586  -0.62245125  0.04050539 -0.04777905]]\n",
      "Env 0:  [-0.45329696 -0.55414873]\n",
      "Env 0 bef:  [-0.45329696 -0.55414873]\n",
      "[[-0.45329696 -0.55414873 -0.13861923 -0.20570277 -0.01159825 -0.0807585\n",
      "   3.97787     0.8200165   4.0405183   0.06264829  0.42950684  0.6074535\n",
      "  -0.74215317 -0.6702303  -0.7826586  -0.62245125  0.04050539 -0.04777905]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.45329695 -0.55414875]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.43965276 -0.53952561]\n",
      "-------------------------------------------------------\n",
      "[[-0.43965277 -0.5395256  -0.15106826 -0.22135594 -0.01159825 -0.0807585\n",
      "   3.975712    0.8200165   4.0405183   0.06480625  0.3895497   0.58745354\n",
      "  -0.74070513 -0.6718303  -0.7826586  -0.62245125  0.04195344 -0.04937902]]\n",
      "Env 0:  [-0.43965277 -0.5395256 ]\n",
      "Env 0 bef:  [-0.43965277 -0.5395256 ]\n",
      "[[-0.43965277 -0.5395256  -0.15106826 -0.22135594 -0.01159825 -0.0807585\n",
      "   3.975712    0.8200165   4.0405183   0.06480625  0.3895497   0.58745354\n",
      "  -0.74070513 -0.6718303  -0.7826586  -0.62245125  0.04195344 -0.04937902]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.43965276 -0.53952561]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.42600858 -0.52490246]\n",
      "-------------------------------------------------------\n",
      "[[-0.42600858 -0.52490246 -0.16351728 -0.23700911 -0.01159825 -0.0807585\n",
      "   3.9731114   0.8200165   4.0405183   0.06740682  0.349595    0.5674535\n",
      "  -0.73895544 -0.6737543  -0.7826586  -0.62245125  0.0437031  -0.05130301]]\n",
      "Env 0:  [-0.42600858 -0.52490246]\n",
      "Env 0 bef:  [-0.42600858 -0.52490246]\n",
      "[[-0.42600858 -0.52490246 -0.16351728 -0.23700911 -0.01159825 -0.0807585\n",
      "   3.9731114   0.8200165   4.0405183   0.06740682  0.349595    0.5674535\n",
      "  -0.73895544 -0.6737543  -0.7826586  -0.62245125  0.0437031  -0.05130301]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.42600858 -0.52490246]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.4123644  -0.51027932]\n",
      "-------------------------------------------------------\n",
      "[[-0.4123644  -0.5102793  -0.17596631 -0.25266227 -0.01159825 -0.0807585\n",
      "   3.9699163   0.8200165   4.0405183   0.07060173  0.30964354  0.5474535\n",
      "  -0.7367991  -0.67611176 -0.7826586  -0.62245125  0.04585945 -0.05366046]]\n",
      "Env 0:  [-0.4123644 -0.5102793]\n",
      "Env 0 bef:  [-0.4123644 -0.5102793]\n",
      "[[-0.4123644  -0.5102793  -0.17596631 -0.25266227 -0.01159825 -0.0807585\n",
      "   3.9699163   0.8200165   4.0405183   0.07060173  0.30964354  0.5474535\n",
      "  -0.7367991  -0.67611176 -0.7826586  -0.62245125  0.04585945 -0.05366046]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.4123644  -0.51027932]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.39872022 -0.49565618]\n",
      "-------------------------------------------------------\n",
      "[[-0.3987202  -0.4956562  -0.18841533 -0.26831546 -0.01159825 -0.0807585\n",
      "   3.9658973   0.8200165   4.0405183   0.0746209   0.26969653  0.52745354\n",
      "  -0.7340758  -0.67906755 -0.7826586  -0.62245125  0.0485828  -0.05661631]]\n",
      "Env 0:  [-0.3987202 -0.4956562]\n",
      "Env 0 bef:  [-0.3987202 -0.4956562]\n",
      "[[-0.3987202  -0.4956562  -0.18841533 -0.26831546 -0.01159825 -0.0807585\n",
      "   3.9658973   0.8200165   4.0405183   0.0746209   0.26969653  0.52745354\n",
      "  -0.7340758  -0.67906755 -0.7826586  -0.62245125  0.0485828  -0.05661631]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.39872022 -0.49565618]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.38507603 -0.48103304]\n",
      "-------------------------------------------------------\n",
      "[[-0.38507605 -0.48103303 -0.20086436 -0.28396863 -0.01159825 -0.0807585\n",
      "   3.9606879   0.8200165   4.0405183   0.07983033  0.22975604  0.5074535\n",
      "  -0.73052824 -0.6828825  -0.7826586  -0.62245125  0.0521303  -0.0604312 ]]\n",
      "Env 0:  [-0.38507605 -0.48103303]\n",
      "Env 0 bef:  [-0.38507605 -0.48103303]\n",
      "[[-0.38507605 -0.48103303 -0.20086436 -0.28396863 -0.01159825 -0.0807585\n",
      "   3.9606879   0.8200165   4.0405183   0.07983033  0.22975604  0.5074535\n",
      "  -0.73052824 -0.6828825  -0.7826586  -0.62245125  0.0521303  -0.0604312 ]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.38507603 -0.48103304]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.37143185 -0.4664099 ]\n",
      "-------------------------------------------------------\n",
      "[[-0.37143186 -0.4664099  -0.21947213 -0.29130015 -0.01159825 -0.0807585\n",
      "   3.997653    0.8200165   3.5169191   0.4807339   0.19185165  0.48745352\n",
      "  -0.75526637 -0.655418   -0.36657634 -0.930388   -0.38869002  0.27497   ]]\n",
      "Env 0:  [-0.37143186 -0.4664099 ]\n",
      "Env 0 bef:  [-0.37143186 -0.4664099 ]\n",
      "[[-0.37143186 -0.4664099  -0.21947213 -0.29130015 -0.01159825 -0.0807585\n",
      "   3.997653    0.8200165   3.5169191   0.4807339   0.19185165  0.48745352\n",
      "  -0.75526637 -0.655418   -0.36657634 -0.930388   -0.38869002  0.27497   ]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.37143185 -0.4664099 ]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.35778767 -0.45178675]\n",
      "-------------------------------------------------------\n",
      "[[-0.35778767 -0.45178676 -0.23192115 -0.3069533  -0.01159825 -0.0807585\n",
      "   3.9969428   0.8200165   4.0405183   0.04357553  0.15188305  0.4674535\n",
      "  -0.75480056 -0.65595436 -0.7826586  -0.62245125  0.02785801 -0.0335031 ]]\n",
      "Env 0:  [-0.35778767 -0.45178676]\n",
      "Env 0 bef:  [-0.35778767 -0.45178676]\n",
      "[[-0.35778767 -0.45178676 -0.23192115 -0.3069533  -0.01159825 -0.0807585\n",
      "   3.9969428   0.8200165   4.0405183   0.04357553  0.15188305  0.4674535\n",
      "  -0.75480056 -0.65595436 -0.7826586  -0.62245125  0.02785801 -0.0335031 ]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.35778767 -0.45178675]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.34414349 -0.43716361]\n",
      "-------------------------------------------------------\n",
      "[[-0.34414348 -0.43716362 -0.24437016 -0.3226065  -0.01159825 -0.0807585\n",
      "   3.9958584   0.8200165   4.0405183   0.04465977  0.1119146   0.44745353\n",
      "  -0.7540889  -0.6567724  -0.7826586  -0.62245125  0.02856967 -0.0343211 ]]\n",
      "Env 0:  [-0.34414348 -0.43716362]\n",
      "Env 0 bef:  [-0.34414348 -0.43716362]\n",
      "[[-0.34414348 -0.43716362 -0.24437016 -0.3226065  -0.01159825 -0.0807585\n",
      "   3.9958584   0.8200165   4.0405183   0.04465977  0.1119146   0.44745353\n",
      "  -0.7540889  -0.6567724  -0.7826586  -0.62245125  0.02856967 -0.0343211 ]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.34414349 -0.43716361]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.3304993  -0.42254047]\n",
      "-------------------------------------------------------\n",
      "[[-0.3304993  -0.42254046 -0.2568192  -0.33825967 -0.01159825 -0.0807585\n",
      "   3.994       0.8200165   4.0405183   0.04651823  0.07194647  0.42745352\n",
      "  -0.752867   -0.65817267 -0.7826586  -0.62245125  0.02979155 -0.03572141]]\n",
      "Env 0:  [-0.3304993  -0.42254046]\n",
      "Env 0 bef:  [-0.3304993  -0.42254046]\n",
      "[[-0.3304993  -0.42254046 -0.2568192  -0.33825967 -0.01159825 -0.0807585\n",
      "   3.994       0.8200165   4.0405183   0.04651823  0.07194647  0.42745352\n",
      "  -0.752867   -0.65817267 -0.7826586  -0.62245125  0.02979155 -0.03572141]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.3304993  -0.42254047]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.31685512 -0.40791733]\n",
      "-------------------------------------------------------\n",
      "[[-0.31685513 -0.40791732 -0.2692682  -0.35391283 -0.01159825 -0.0807585\n",
      "   3.9900775   0.8200165   4.0405183   0.05044058  0.03197915  0.40745354\n",
      "  -0.75027966 -0.6611206  -0.7826586  -0.62245125  0.03237892 -0.03866935]]\n",
      "Env 0:  [-0.31685513 -0.40791732]\n",
      "Env 0 bef:  [-0.31685513 -0.40791732]\n",
      "[[-0.31685513 -0.40791732 -0.2692682  -0.35391283 -0.01159825 -0.0807585\n",
      "   3.9900775   0.8200165   4.0405183   0.05044058  0.03197915  0.40745354\n",
      "  -0.75027966 -0.6611206  -0.7826586  -0.62245125  0.03237892 -0.03866935]]\n",
      "Inside Step -------------------------------------------\n",
      "attack:  [-0.31685512 -0.40791733]\n",
      "HIT!\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "attack end state:  [-0.30321094 -0.39329419]\n",
      "-------------------------------------------------------\n",
      "Resetting ------------------------------------\n",
      "attack:  [-0.30321094 -0.39329419]\n",
      "----------------------------------------------\n",
      "[[-0.30321094 -0.3932942  -0.12724896  0.06402777 -0.01159825 -0.0807585\n",
      "   4.3450837   0.8200165   0.2787509   2.2168524   0.45000613  0.38745353\n",
      "  -0.93329847 -0.3591016   0.27515498  0.9613999  -1.2084534  -1.3205014 ]]\n",
      "Env 0:  [-0.30321094 -0.3932942 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Custom environment import placeholder\n",
    "# from your_custom_environment import missile_interception, initial_conditions_missile_interception\n",
    "\n",
    "model_path = \"dqn_missile_guidance_v(2.4.2.5)_PROD_11\"\n",
    "model = DQN.load(model_path)\n",
    "\n",
    "env0 = make_vec_env(lambda: missile_interception(), n_envs=1)\n",
    "env1 = make_vec_env(lambda: carlos(), n_envs=1)\n",
    "envs = [env0, env1]\n",
    "\n",
    "# Initialize episode data\n",
    "episode_data = [{\n",
    "    'past_defense_positions': [],\n",
    "    'past_attack_positions': [],\n",
    "    'attack_positions': [],\n",
    "    'defense_positions': [],\n",
    "    'target_position': [],\n",
    "    'actions': [],\n",
    "    'rewards': [],\n",
    "    'defense_angle': [],\n",
    "    'defense_attack_theta': [],\n",
    "} for _ in envs]\n",
    "\n",
    "# Initialize conditions and reset environments\n",
    "\n",
    "obs0 = env0.reset()\n",
    "obs1 = env1.reset()\n",
    "\n",
    "done_list = [False] * len(envs)\n",
    "total_rewards = [0] * len(envs)\n",
    "steps = [0] * len(envs)\n",
    "\n",
    "while not any(done_list):\n",
    "    action0, _ = model.predict(obs0, deterministic=True)\n",
    "    # action1, _ = model.predict(obs1, deterministic=True)\n",
    "\n",
    "    print(\"Env 0 bef: \", obs0[0][0:2])\n",
    "    print(obs0)\n",
    "    obs0, reward0, done0, info0 = env0.step(action0)\n",
    "    print(obs0)\n",
    "    print(\"Env 0: \", obs0[0][0:2])\n",
    "    # print(\"Env 1 after obs0: \", obs1[0][0:2], \"\\n\")\n",
    "    \n",
    "    # print(\"Env 1 bef: \", obs1[0][0:2])\n",
    "    # print(obs1)\n",
    "    # obs1, reward1, done1, info1 = env1.step(action1)\n",
    "    # print(obs1)\n",
    "    # print(\"Env 1: \", obs1[0][0:2], \"\\n\")\n",
    "\n",
    "    done_list[0] = done0\n",
    "    # done_list[1] = done1\n",
    "    total_rewards[0] += reward0\n",
    "    # total_rewards[1] += reward1\n",
    "\n",
    "####\n",
    "####\n",
    "# solution is in adding prints to the main funct to see what the fuck is going on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting ------------------------------------\n",
      "attack:  [-0.30321094 -0.39329419]\n",
      "----------------------------------------------\n",
      "Resetting ------------------------------------\n",
      "attack:  [-0.30321094 -0.39329419]\n",
      "----------------------------------------------\n",
      "Env 1 after obs0:  [-0.30321094 -0.3932942 ] \n",
      "\n",
      "Env 1 bef:  [-0.30321094 -0.3932942 ]\n",
      "[[-0.30321094 -0.3932942   0.02701201 -0.13780816 -0.01159825 -0.0807585\n",
      "   3.8000758   0.8200165   4.5253587   0.72528285  0.37751684  0.38745353\n",
      "  -0.6119179  -0.7909213  -0.98256075 -0.18594176  0.37064287 -0.6049795 ]]\n",
      "attack:  [-0.30321094 -0.39329419]\n",
      "attack end state:  [-0.28956676 -0.37867104]\n",
      "[[-0.28956676 -0.37867105  0.01396579 -0.1529672  -0.01159825 -0.0807585\n",
      "   3.780979    0.8200165   4.0017595   0.22078083  0.33825156  0.36745352\n",
      "  -0.59670305 -0.80246216 -0.75795156 -0.65231085  0.16124852 -0.1501513 ]]\n",
      "Env 1:  [-0.28956676 -0.37867105] \n",
      "\n",
      "Env 1 after obs0:  [-0.28956676 -0.37867105] \n",
      "\n",
      "Env 1 bef:  [-0.28956676 -0.37867105]\n",
      "[[-0.28956676 -0.37867105  0.01396579 -0.1529672  -0.01159825 -0.0807585\n",
      "   3.780979    0.8200165   4.0017595   0.22078083  0.33825156  0.36745352\n",
      "  -0.59670305 -0.80246216 -0.75795156 -0.65231085  0.16124852 -0.1501513 ]]\n",
      "attack:  [-0.28956676 -0.37867104]\n",
      "attack end state:  [-0.27592257 -0.3640479 ]\n",
      "[[-2.7592257e-01 -3.6404791e-01 -1.5145593e-03 -1.6563047e-01\n",
      "  -1.1598245e-02 -8.0758505e-02  3.7676375e+00  8.2001650e-01\n",
      "   3.8272269e+00  5.9589248e-02  2.9862845e-01  3.4745353e-01\n",
      "  -5.8594435e-01 -8.1035131e-01 -6.3316399e-01 -7.7401769e-01\n",
      "   4.7219649e-02 -3.6333624e-02]]\n",
      "Env 1:  [-0.27592257 -0.3640479 ] \n",
      "\n",
      "Env 1 after obs0:  [-0.27592257 -0.3640479 ] \n",
      "\n",
      "Env 1 bef:  [-0.27592257 -0.3640479 ]\n",
      "[[-2.7592257e-01 -3.6404791e-01 -1.5145593e-03 -1.6563047e-01\n",
      "  -1.1598245e-02 -8.0758505e-02  3.7676375e+00  8.2001650e-01\n",
      "   3.8272269e+00  5.9589248e-02  2.9862845e-01  3.4745353e-01\n",
      "  -5.8594435e-01 -8.1035131e-01 -6.3316399e-01 -7.7401769e-01\n",
      "   4.7219649e-02 -3.6333624e-02]]\n",
      "attack:  [-0.27592257 -0.3640479 ]\n",
      "attack end state:  [-0.26227839 -0.34942476]\n",
      "[[-0.26227838 -0.34942475 -0.01699491 -0.17829375 -0.01159825 -0.0807585\n",
      "   3.7507644   0.8200165   3.8272269   0.07646247  0.2590816   0.32745352\n",
      "  -0.5721883  -0.82012224 -0.633164   -0.7740177   0.06097565 -0.04610457]]\n",
      "Env 1:  [-0.26227838 -0.34942475] \n",
      "\n",
      "Env 1 after obs0:  [-0.26227838 -0.34942475] \n",
      "\n",
      "Env 1 bef:  [-0.26227838 -0.34942475]\n",
      "[[-0.26227838 -0.34942475 -0.01699491 -0.17829375 -0.01159825 -0.0807585\n",
      "   3.7507644   0.8200165   3.8272269   0.07646247  0.2590816   0.32745352\n",
      "  -0.5721883  -0.82012224 -0.633164   -0.7740177   0.06097565 -0.04610457]]\n",
      "attack:  [-0.26227839 -0.34942476]\n",
      "attack end state:  [-0.24863421 -0.33480162]\n",
      "[[-0.2486342  -0.3348016  -0.03673293 -0.1815203  -0.01159825 -0.0807585\n",
      "   3.7678263   0.8200165   3.3036277   0.46419853  0.2215288   0.3074535\n",
      "  -0.5860973  -0.8102407  -0.16132703 -0.986901   -0.42477027  0.17666031]]\n",
      "Env 1:  [-0.2486342 -0.3348016] \n",
      "\n",
      "Env 1 after obs0:  [-0.2486342 -0.3348016] \n",
      "\n",
      "Env 1 bef:  [-0.2486342 -0.3348016]\n",
      "[[-0.2486342  -0.3348016  -0.03673293 -0.1815203  -0.01159825 -0.0807585\n",
      "   3.7678263   0.8200165   3.3036277   0.46419853  0.2215288   0.3074535\n",
      "  -0.5860973  -0.8102407  -0.16132703 -0.986901   -0.42477027  0.17666031]]\n",
      "attack:  [-0.24863421 -0.33480162]\n",
      "attack end state:  [-0.23499003 -0.32017848]\n",
      "[[-0.23499003 -0.32017848 -0.05221329 -0.19418357 -0.01159825 -0.0807585\n",
      "   3.7451267   0.8200165   3.8272269   0.08209998  0.18199562  0.28745353\n",
      "  -0.56755584 -0.82333493 -0.633164   -0.7740177   0.06560817 -0.04931724]]\n",
      "Env 1:  [-0.23499003 -0.32017848] \n",
      "\n",
      "Env 1 after obs0:  [-0.23499003 -0.32017848] \n",
      "\n",
      "Env 1 bef:  [-0.23499003 -0.32017848]\n",
      "[[-0.23499003 -0.32017848 -0.05221329 -0.19418357 -0.01159825 -0.0807585\n",
      "   3.7451267   0.8200165   3.8272269   0.08209998  0.18199562  0.28745353\n",
      "  -0.56755584 -0.82333493 -0.633164   -0.7740177   0.06560817 -0.04931724]]\n",
      "attack:  [-0.23499003 -0.32017848]\n",
      "attack end state:  [-0.22134584 -0.30555533]\n",
      "[[-0.22134584 -0.30555534 -0.06965742 -0.20396633 -0.01159825 -0.0807585\n",
      "   3.731707    0.8200165   3.652694    0.07901326  0.14256425  0.26745352\n",
      "  -0.55645615 -0.830877   -0.48913804 -0.8722064  -0.06731811  0.04132941]]\n",
      "Env 1:  [-0.22134584 -0.30555534] \n",
      "\n",
      "Env 1 after obs0:  [-0.22134584 -0.30555534] \n",
      "\n",
      "Env 1 bef:  [-0.22134584 -0.30555534]\n",
      "[[-0.22134584 -0.30555534 -0.06965742 -0.20396633 -0.01159825 -0.0807585\n",
      "   3.731707    0.8200165   3.652694    0.07901326  0.14256425  0.26745352\n",
      "  -0.55645615 -0.830877   -0.48913804 -0.8722064  -0.06731811  0.04132941]]\n",
      "attack:  [-0.22134584 -0.30555533]\n",
      "attack end state:  [-0.20770166 -0.29093219]\n",
      "[[-0.20770167 -0.29093218 -0.08513777 -0.21662961 -0.01159825 -0.0807585\n",
      "   3.6865845   0.8200165   3.8272269   0.14064237  0.10332753  0.24745353\n",
      "  -0.51841104 -0.85513157 -0.633164   -0.7740177   0.11475293 -0.08111387]]\n",
      "Env 1:  [-0.20770167 -0.29093218] \n",
      "\n",
      "Env 1 after obs0:  [-0.20770167 -0.29093218] \n",
      "\n",
      "Env 1 bef:  [-0.20770167 -0.29093218]\n",
      "[[-0.20770167 -0.29093218 -0.08513777 -0.21662961 -0.01159825 -0.0807585\n",
      "   3.6865845   0.8200165   3.8272269   0.14064237  0.10332753  0.24745353\n",
      "  -0.51841104 -0.85513157 -0.633164   -0.7740177   0.11475293 -0.08111387]]\n",
      "attack:  [-0.20770166 -0.29093219]\n",
      "attack end state:  [-0.19405748 -0.27630905]\n",
      "[[-0.19405748 -0.27630904 -0.1025819  -0.22641237 -0.01159825 -0.0807585\n",
      "   3.6409469   0.8200165   3.652694    0.01174692  0.06419913  0.22745353\n",
      "  -0.4788588  -0.87789196 -0.48913804 -0.8722064   0.01027925 -0.00568556]]\n",
      "Env 1:  [-0.19405748 -0.27630904] \n",
      "\n",
      "Env 1 after obs0:  [-0.19405748 -0.27630904] \n",
      "\n",
      "Env 1 bef:  [-0.19405748 -0.27630904]\n",
      "[[-0.19405748 -0.27630904 -0.1025819  -0.22641237 -0.01159825 -0.0807585\n",
      "   3.6409469   0.8200165   3.652694    0.01174692  0.06419913  0.22745353\n",
      "  -0.4788588  -0.87789196 -0.48913804 -0.8722064   0.01027925 -0.00568556]]\n",
      "attack:  [-0.19405748 -0.27630905]\n",
      "attack end state:  [-0.1804133  -0.26168591]\n",
      "[[-0.18041329 -0.2616859  -0.12002602 -0.23619513 -0.01159825 -0.0807585\n",
      "   3.5410228   0.8200165   3.652694    0.11167116  0.02554695  0.20745352\n",
      "  -0.38889334 -0.92128277 -0.48913804 -0.8722064   0.10024471 -0.0490764 ]]\n",
      "Env 1:  [-0.18041329 -0.2616859 ] \n",
      "\n",
      "Env 1 after obs0:  [-0.18041329 -0.2616859 ] \n",
      "\n",
      "Env 1 bef:  [-0.18041329 -0.2616859 ]\n",
      "[[-0.18041329 -0.2616859  -0.12002602 -0.23619513 -0.01159825 -0.0807585\n",
      "   3.5410228   0.8200165   3.652694    0.11167116  0.02554695  0.20745352\n",
      "  -0.38889334 -0.92128277 -0.48913804 -0.8722064   0.10024471 -0.0490764 ]]\n",
      "attack:  [-0.1804133  -0.26168591]\n",
      "HIT!\n",
      "attack end state:  [-0.16676911 -0.24706276]\n",
      "Resetting ------------------------------------\n",
      "attack:  [-0.16676911 -0.24706276]\n",
      "----------------------------------------------\n",
      "[[-0.16676912 -0.24706276  0.1287095  -0.23320092 -0.01159825 -0.0807585\n",
      "   3.1884716   0.8200165   5.5125184   2.3240469   0.25580359  0.18745352\n",
      "  -0.04686163 -0.99890137 -0.6966139   0.7174462   0.64975226 -1.7163476 ]]\n",
      "Env 1:  [-0.16676912 -0.24706276] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Custom environment import placeholder\n",
    "# from your_custom_environment import missile_interception, initial_conditions_missile_interception\n",
    "\n",
    "model_path = \"dqn_missile_guidance_v(2.4.2.5)_PROD_11\"\n",
    "model = DQN.load(model_path)\n",
    "\n",
    "env0 = make_vec_env(lambda: missile_interception(), n_envs=1)\n",
    "env1 = make_vec_env(lambda: carlos(), n_envs=1)\n",
    "envs = [env0, env1]\n",
    "\n",
    "# Initialize episode data\n",
    "episode_data = [{\n",
    "    'past_defense_positions': [],\n",
    "    'past_attack_positions': [],\n",
    "    'attack_positions': [],\n",
    "    'defense_positions': [],\n",
    "    'target_position': [],\n",
    "    'actions': [],\n",
    "    'rewards': [],\n",
    "    'defense_angle': [],\n",
    "    'defense_attack_theta': [],\n",
    "} for _ in envs]\n",
    "\n",
    "# Initialize conditions and reset environments\n",
    "\n",
    "obs0 = env0.reset()\n",
    "obs1 = env1.reset()\n",
    "\n",
    "done_list = [False] * len(envs)\n",
    "total_rewards = [0] * len(envs)\n",
    "steps = [0] * len(envs)\n",
    "\n",
    "while not any(done_list):\n",
    "    # action0, _ = model.predict(obs0, deterministic=True)\n",
    "    action1, _ = model.predict(obs1, deterministic=True)\n",
    "\n",
    "    # print(\"Env 0 bef: \", obs0[0][0:2])\n",
    "    # print(obs0)\n",
    "    # obs0, reward0, done0, info0 = env0.step(action0)\n",
    "    # print(obs0)\n",
    "    # print(\"Env 0: \", obs0[0][0:2])\n",
    "\n",
    "    print(\"Env 1 after obs0: \", obs1[0][0:2], \"\\n\")\n",
    "    \n",
    "    print(\"Env 1 bef: \", obs1[0][0:2])\n",
    "    print(obs1)\n",
    "    obs1, reward1, done1, info1 = env1.step(action1)\n",
    "    print(obs1)\n",
    "    print(\"Env 1: \", obs1[0][0:2], \"\\n\")\n",
    "\n",
    "    # done_list[0] = done0\n",
    "    done_list[1] = done1\n",
    "    # total_rewards[0] += reward0\n",
    "    total_rewards[1] += reward1\n",
    "\n",
    "####\n",
    "####\n",
    "# solution is in adding prints to the main funct to see what the fuck is going on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missile_interception' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create environments\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m env1 \u001b[38;5;241m=\u001b[39m \u001b[43mmake_vec_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissile_interception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m env2 \u001b[38;5;241m=\u001b[39m make_vec_env(\u001b[38;5;28;01mlambda\u001b[39;00m: missile_interception(), n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m envs \u001b[38;5;241m=\u001b[39m [env1, env2]\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\env_util.py:125\u001b[0m, in \u001b[0;36mmake_vec_env\u001b[1;34m(env_id, n_envs, seed, start_index, monitor_dir, wrapper_class, env_kwargs, vec_env_cls, vec_env_kwargs, monitor_kwargs, wrapper_kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vec_env_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# Default: use a DummyVecEnv\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     vec_env_cls \u001b[38;5;241m=\u001b[39m DummyVecEnv\n\u001b[1;32m--> 125\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m \u001b[43mvec_env_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvec_env_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Prepare the seeds for the first reset\u001b[39;00m\n\u001b[0;32m    127\u001b[0m vec_env\u001b[38;5;241m.\u001b[39mseed(seed)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:30\u001b[0m, in \u001b[0;36mDummyVecEnv.__init__\u001b[1;34m(self, env_fns)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m_patch_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv_fns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [_patch_env(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\env_util.py:98\u001b[0m, in \u001b[0;36mmake_vec_env.<locals>.make_env.<locals>._init\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m         env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(env_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_kwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43menv_id\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# Patch to support gym 0.21/0.26 and gymnasium\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     env \u001b[38;5;241m=\u001b[39m _patch_env(env)\n",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create environments\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m env1 \u001b[38;5;241m=\u001b[39m make_vec_env(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mmissile_interception\u001b[49m(), n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m env2 \u001b[38;5;241m=\u001b[39m make_vec_env(\u001b[38;5;28;01mlambda\u001b[39;00m: missile_interception(), n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m envs \u001b[38;5;241m=\u001b[39m [env1, env2]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'missile_interception' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Path to the saved model\n",
    "model_path = \"dqn_missile_guidance_v(2.4.2.5)_PROD_11\"\n",
    "model = DQN.load(model_path)\n",
    "\n",
    "# Create environments\n",
    "env1 = make_vec_env(lambda: missile_interception(), n_envs=1)\n",
    "env2 = make_vec_env(lambda: missile_interception(), n_envs=1)\n",
    "envs = [env1, env2]\n",
    "\n",
    "# Initialize episode data\n",
    "episode_data = [{\n",
    "    'past_defense_positions': [],\n",
    "    'past_attack_positions': [],\n",
    "    'attack_positions': [],\n",
    "    'defense_positions': [],\n",
    "    'target_position': [],\n",
    "    'actions': [],\n",
    "    'rewards': [],\n",
    "    'defense_angle': [],\n",
    "    'defense_attack_theta': [],\n",
    "} for _ in envs]\n",
    "\n",
    "# Reset environments\n",
    "init = initial_conditions_missile_interception()\n",
    "init_coords = init.reset()\n",
    "print(init_coords[0])\n",
    "print(init_coords[1])\n",
    "obs_list = [env.reset() for env in envs]\n",
    "\n",
    "for i, obs in enumerate(obs_list):\n",
    "    episode_data[i]['target_position'] = obs[0][4:6]\n",
    "\n",
    "# Run environments step-by-step\n",
    "done_list = [False] * len(envs)\n",
    "total_rewards = [0] * len(envs)\n",
    "steps = [0] * len(envs)\n",
    "\n",
    "while not any(done_list):\n",
    "    for i, env in enumerate(envs):  \n",
    "        # print(obs_list)\n",
    "        # print(\"i\", i)\n",
    "        # print(obs_list[i])\n",
    "        # print(f\"Environment {i} ob: {obs_list[i]}\")\n",
    "        # print(f\"Environment {i} done: {done_list[i]}\")\n",
    "        if not done_list[i]:\n",
    "            print(f\"Environment {i} done: {done_list[i]}\")\n",
    "            print(\"----------------------------------------------------------\")\n",
    "            print(\"Before\")\n",
    "            print(f\"Environment {i}\")\n",
    "            print(f\"Environment {0} steaaaaaaaaaaaaaaaap: {obs_list[0][0][0:2]}\")\n",
    "            print(f\"Environment {1} steaaaaaaaaaaaaaaaap: {obs_list[1][0][0:2]}\")\n",
    "            print(\"----------------------------------------------------------\")\n",
    "            action, _ = model.predict(obs_list[i], deterministic=True)\n",
    "            episode_data[i]['actions'].append(deepcopy(action))\n",
    "            # print(\"\\n Defense position\")\n",
    "            # print(np.array([obs_list[i][0][2], obs_list[i][0][3]]))\n",
    "            # print(\"\\n\")\n",
    "            episode_data[i]['past_defense_positions'].append(deepcopy(np.array([obs_list[i][0][2], obs_list[i][0][3]])))\n",
    "            episode_data[i]['past_attack_positions'].append(deepcopy(np.array([obs_list[i][0][0], obs_list[i][0][1]])))\n",
    "            # print(\"\\n Attack position\")\n",
    "            # print(np.array([obs_list[i][0][0], obs_list[i][0][1]]))\n",
    "            # print(\"\\n\")\n",
    "            print(f\"Environment {i} action: {obs_list[i][0][0:2]}\")\n",
    "            print(envs)\n",
    "            print(env)\n",
    "            carlos, reward, done, info = env.step(action)\n",
    "            print(\"Carlos: \", carlos)   \n",
    "            print(f\"Environment {i} action: {obs_list[i][0][0:2]}\")\n",
    "            print(\"----------------------------------------------------------\")\n",
    "            print(\"After\")\n",
    "            print(f\"Environment {i}\")\n",
    "            print(f\"Environment {0} steaaaaaaaaaaaaaaaap: {obs_list[0][0][0:2]}\")\n",
    "            print(f\"Environment {1} steaaaaaaaaaaaaaaaap: {obs_list[1][0][0:2]}\")\n",
    "            print(\"----------------------------------------------------------\")\n",
    "\n",
    "            episode_data[i]['rewards'].append(deepcopy(reward))\n",
    "            episode_data[i]['defense_angle'].append(deepcopy(obs[0][8]))\n",
    "            episode_data[i]['defense_attack_theta'].append(deepcopy(obs[0][6]))\n",
    "            episode_data[i]['attack_positions'].append(deepcopy(np.array([obs[0][2], obs[0][3]])))\n",
    "            episode_data[i]['defense_positions'].append(deepcopy(np.array([obs[0][0], obs[0][1]])))\n",
    "            print(\"----------------------------------------------------------------------\")\n",
    "            print(f\"Environment {i} step: {steps[i]}\")\n",
    "            # print(f\"Environment {i} action: {action}\")\n",
    "            # print(f\"Environment {i} reward: {reward}\")\n",
    "            print(f\"Environment {i} done: {done}\")\n",
    "            # print(f\"Environment {i} defense position: {obs[0][2], obs[0][3]}\")\n",
    "            print(f\"Environment {i} attack position: {obs_list[i][0][0], obs_list[i][0][1]}\")\n",
    "            # print(f\"Environment {i} target position: {obs[0][4], obs[0][5]}\")\n",
    "            # print(f\"Environment {i} defense angle: {obs[0][8]}\")\n",
    "            # print(f\"Environment {i} defense attack theta: {obs[0][6]}\")\n",
    "            print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "            total_rewards[i] += reward\n",
    "            steps[i] += 1\n",
    "            # obs_list[i] = ob\n",
    "            done_list[i] = done\n",
    "\n",
    "# Print results\n",
    "for i, total_reward in enumerate(total_rewards):\n",
    "    print(f\"Environment {i} finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# so we're gonna need two environments \n",
    "\n",
    "# Function to run an episode\n",
    "def run_episode(env, model):\n",
    "    single_env = env.envs[0]\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    episode_data = {\n",
    "        'past_defense_positions': [],\n",
    "        'past_attack_positions': [],\n",
    "        'attack_positions': [],\n",
    "        'defense_positions': [],\n",
    "        'target_position': [],\n",
    "        'actions': [],\n",
    "        'rewards': [],\n",
    "        'defense_angle': [],\n",
    "        'defense_attack_theta': [],\n",
    "    }\n",
    "    \n",
    "    episode_data['target_position'] = obs[0][4:6]\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        episode_data['actions'].append(deepcopy(action))\n",
    "        episode_data['past_defense_positions'].append(deepcopy(np.array([obs[0][2], obs[0][3]])))\n",
    "        episode_data['past_attack_positions'].append(deepcopy(np.array([obs[0][0], obs[0][1]])))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_data['rewards'].append(deepcopy(reward))\n",
    "        episode_data['defense_angle'].append(deepcopy(obs[0][8]))\n",
    "        episode_data['defense_attack_theta'].append(deepcopy(obs[0][6]))\n",
    "        episode_data['attack_positions'].append(deepcopy(np.array([obs[0][2], obs[0][3]])))\n",
    "        episode_data['defense_positions'].append(deepcopy(np.array([obs[0][0], obs[0][1]])))\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "\n",
    "    print(f\"Episode finished in {step} steps with reward {total_reward}. Out of bounds: {single_env.out_of_bounds} ({((single_env.out_of_bounds / (single_env.out_of_bounds + single_env.interceptions + single_env.reached_max_steps + single_env.enemy_impacts)) * 100)}), Interceptions: {single_env.interceptions} ({((single_env.interceptions / (single_env.out_of_bounds + single_env.interceptions + single_env.reached_max_steps + single_env.enemy_impacts)) * 100)}), Reached max steps: {single_env.reached_max_steps} ({((single_env.reached_max_steps / (single_env.out_of_bounds + single_env.interceptions + single_env.reached_max_steps + single_env.enemy_impacts)) * 100)}), Enemy impacts: {single_env.enemy_impacts} ({((single_env.enemy_impacts / (single_env.out_of_bounds + single_env.interceptions + single_env.reached_max_steps + single_env.enemy_impacts)) * 100)})\")\n",
    "\n",
    "    return step, total_reward, single_env.out_of_bounds, single_env.interceptions, single_env.reached_max_steps, single_env.enemy_impacts, info, episode_data\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model_path, n_episodes=100):\n",
    "    # Create the environment\n",
    "    env = make_vec_env(lambda: missile_interception(), n_envs=1)\n",
    "    env2 = make_vec_env(lambda: missile_interception(), n_envs=1)\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = DQN.load(model_path)\n",
    "    \n",
    "    target_hit_episodes = []\n",
    "    \n",
    "    # Run multiple evaluation episodes\n",
    "    for i in range(n_episodes):\n",
    "        print(f\"Running episode {i + 1}\")\n",
    "        episode_length, episode_reward, out_of_bounds, interceptions, reached_max_steps, enemy_impacts, info, episode_data = run_episode(env, model)\n",
    "\n",
    "        print(info[0]['activated'])\n",
    "        print(\"........................................................................................\")\n",
    "\n",
    "        if info[0]['activated'] == True:\n",
    "            target_hit_episodes.append(episode_data)\n",
    "    \n",
    "    return target_hit_episodes\n",
    "\n",
    "# Function to graph the episode data\n",
    "def graph_episode(defense_positions, attack_positions, target_position):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "\n",
    "    plt.axhline(0, color='black', linewidth=0.5)\n",
    "    plt.axvline(0, color='black', linewidth=0.5)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot trails\n",
    "    if defense_positions:\n",
    "        defense_xs, defense_ys = zip(*defense_positions)\n",
    "        ax.plot(defense_xs, defense_ys, color='#858585', label='Defense Trail')  # Blue line for defense\n",
    "\n",
    "    if attack_positions:\n",
    "        attack_xs, attack_ys = zip(*attack_positions)\n",
    "        ax.plot(attack_xs, attack_ys, color='#FFA281', label='Attack Trail')  # Red line for attack\n",
    "\n",
    "    # Plot current positions\n",
    "    plt.scatter(defense_positions[-1][0], defense_positions[-1][1], color='#1C1C1C', label='Defense Position')\n",
    "    plt.scatter(attack_positions[-1][0], attack_positions[-1][1], color='#FF5A1F', label='Attack Position')\n",
    "    plt.scatter(target_position[0], target_position[1], color='#85A3FF', label='Target Position')\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Path to the saved model\n",
    "model_path = \"dqn_missile_guidance_v(2.4.2.5)_PROD_11\"\n",
    "\n",
    "# Evaluate the model and get episodes with target hits\n",
    "target_hit_episodes = evaluate_model(model_path, n_episodes=50000)\n",
    "\n",
    "# Graph the episodes with target hits\n",
    "i = 0\n",
    "for episode_data in target_hit_episodes:\n",
    "    episode_name = f'Episode {i} collision.gif '\n",
    "    animate_episode(episode_data, episode_name)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
