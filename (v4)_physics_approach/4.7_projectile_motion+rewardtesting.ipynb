{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "287e25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "class missile_interception_3d(gym.Env):\n",
    "    def __init__(self):\n",
    "        # 1. Define Action Space (The Joystick: Left/Right, Up/Down)\n",
    "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2, ), dtype=np.float32)\n",
    "        \n",
    "        # 2. Define Observation Space (20D ego-frame version)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(20,), dtype=np.float32)\n",
    "\n",
    "        self.np_random = np.random.RandomState()\n",
    "        \n",
    "        # 3. Time Settings\n",
    "        self.dt_act = 0.1             \n",
    "        self.n_substeps = 10          \n",
    "        self.dt_sim = self.dt_act / self.n_substeps \n",
    "        self.t_max = 650.0            \n",
    "\n",
    "        # 4. Physical Limits\n",
    "        self.a_max = 350.0   # Max G-force (m/s^2) ~35G\n",
    "        self.da_max = 2500.0 # Jerk Limit (m/s^3)\n",
    "        self.tau = 0.05      # Airframe Lag\n",
    "        self.g = 9.81        \n",
    "        self.collision_radius = 150.0  \n",
    "        self.max_distance = 4_000_000.0 \n",
    "\n",
    "        self.p_easy = 1.0                   \n",
    "        self.range_min = 70_000.0           \n",
    "        self.range_easy_max = 200_000.0     \n",
    "        self.range_hard_max = 1_000_000.0   \n",
    "\n",
    "        self.targetbox_x_min = -15000\n",
    "        self.targetbox_x_max = 15000\n",
    "        self.targetbox_y_min = -15000\n",
    "        self.targetbox_y_max = 15000\n",
    "\n",
    "        # ----------------------------\n",
    "        # Potential-based shaping (ZEM_perp)\n",
    "        # IMPORTANT: gamma_shape MUST match PPO's gamma\n",
    "        # ----------------------------\n",
    "        self.gamma_shape = 0.99      # set to your PPO gamma\n",
    "        self.w_zem = 1.0             # shaping weight\n",
    "        self.zem_scale = 50_000.0    # meters; tunes magnitude of phi\n",
    "        self.tgo_max = 15.0          # seconds; clamp lookahead when closing\n",
    "        self.tgo_fixed = 3.0         # seconds; lookahead when NOT closing\n",
    "        self.vc_min = 1.0            # m/s; treat <= this as \"not closing\"\n",
    "\n",
    "    def generate_enemy_missile(self):\n",
    "        if self.np_random.rand() < self.p_easy:\n",
    "            self.range_max_used = self.range_easy_max\n",
    "        else:\n",
    "            self.range_max_used = self.range_hard_max\n",
    "\n",
    "        range_min = self.range_min\n",
    "        self.attack_target_x = self.np_random.uniform(self.targetbox_x_min, self.targetbox_x_max)\n",
    "        self.attack_target_y = self.np_random.uniform(self.targetbox_y_min, self.targetbox_y_max)\n",
    "        self.enemy_launch_angle = self.np_random.uniform(0, 2 * np.pi)\n",
    "        self.enemy_theta = self.np_random.uniform(0.523599, 1.0472) \n",
    "\n",
    "        self.range_max_used = max(self.range_max_used, range_min + 1.0)\n",
    "        lower_limit = np.sqrt((range_min * self.g) / np.sin(2 * self.enemy_theta))\n",
    "        upper_limit = np.sqrt((self.range_max_used * self.g) / np.sin(2 * self.enemy_theta))\n",
    "        self.enemy_initial_velocity = self.np_random.uniform(lower_limit, upper_limit)\n",
    "\n",
    "        ground_range = (\n",
    "            self.enemy_initial_velocity * np.cos(self.enemy_theta)\n",
    "            * (2 * self.enemy_initial_velocity * np.sin(self.enemy_theta) / self.g)\n",
    "        )\n",
    "\n",
    "        self.enemy_launch_x = self.attack_target_x + ground_range * np.cos(self.enemy_launch_angle)\n",
    "        self.enemy_launch_y = self.attack_target_y + ground_range * np.sin(self.enemy_launch_angle)\n",
    "        self.enemy_z = 0\n",
    "        self.enemy_x = self.enemy_launch_x\n",
    "        self.enemy_y = self.enemy_launch_y\n",
    "        self.enemy_pos = np.array([self.enemy_x, self.enemy_y, self.enemy_z], dtype=np.float32)\n",
    "        self.enemy_azimuth = (self.enemy_launch_angle + np.pi) % (2 * np.pi)\n",
    "\n",
    "    def generate_defense_missile(self):\n",
    "        self.defense_launch_x = self.np_random.uniform(self.targetbox_x_min, self.targetbox_x_max)\n",
    "        self.defense_launch_y = self.np_random.uniform(self.targetbox_y_min, self.targetbox_y_max)\n",
    "\n",
    "        dx = self.enemy_launch_x - self.defense_launch_x\n",
    "        dy = self.enemy_launch_y - self.defense_launch_y\n",
    "        az_nominal = np.arctan2(dy, dx)\n",
    "\n",
    "        # --- Misalignment (domain randomization of initial heading) ---\n",
    "        # Mixture: most episodes small error, some episodes large az error\n",
    "        p_misaligned = 0.35  # 35% \"hard\" starts\n",
    "        if self.np_random.rand() < p_misaligned:\n",
    "            # Hard: big azimuth error → strong RIGHT required\n",
    "            az_noise = self.np_random.uniform(-np.deg2rad(60.0), np.deg2rad(60.0))\n",
    "        else:\n",
    "            # Easy: small azimuth error → gentle correction\n",
    "            az_noise = self.np_random.uniform(-np.deg2rad(10.0), np.deg2rad(10.0))\n",
    "\n",
    "        self.defense_azimuth = az_nominal + az_noise\n",
    "\n",
    "        # Elevation noise: avoid always same vertical plane\n",
    "        theta_nominal = 0.785398  # ~45 deg\n",
    "        theta_noise_deg = 10.0\n",
    "        theta_noise = self.np_random.uniform(\n",
    "            -np.deg2rad(theta_noise_deg),\n",
    "            +np.deg2rad(theta_noise_deg),\n",
    "        )\n",
    "        self.defense_theta = float(np.clip(theta_nominal + theta_noise,\n",
    "                                           np.deg2rad(10.0),\n",
    "                                           np.deg2rad(80.0)))\n",
    "        # ---------------------------------------------------------------\n",
    "\n",
    "        base_velocity = 3000.0\n",
    "        if hasattr(self, 'range_max_used'):\n",
    "            velocity_scale = min(self.range_max_used / self.range_easy_max, 1.5)\n",
    "            self.defense_initial_velocity = base_velocity * velocity_scale\n",
    "        else:\n",
    "            self.defense_initial_velocity = base_velocity\n",
    "\n",
    "        self.defense_x = self.defense_launch_x\n",
    "        self.defense_y = self.defense_launch_y\n",
    "        self.defense_z = 0.0\n",
    "        self.defense_pos = np.array([self.defense_x, self.defense_y, self.defense_z], dtype=np.float32)\n",
    "\n",
    "        self.defense_ax = 0.0\n",
    "        self.defense_ay = 0.0\n",
    "        self.defense_az = 0.0\n",
    "    \n",
    "    def _smoothstep(self, x: float) -> float:\n",
    "        \"\"\"Smooth ramp 0->1 with zero slope at ends, clamps outside [0,1]\"\"\"\n",
    "        x = float(np.clip(x, 0.0, 1.0))\n",
    "        return x * x * (3.0 - 2.0 * x)\n",
    "    \n",
    "    def calculate_pronav(self):\n",
    "        eps = 1e-9\n",
    "\n",
    "        # Relative geometry (use float64 for stability)\n",
    "        r = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
    "        v = self.defense_vel.astype(np.float64)\n",
    "        vrel = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
    "\n",
    "        R = float(np.linalg.norm(r)) + eps\n",
    "        V = float(np.linalg.norm(v)) + eps\n",
    "\n",
    "        rhat = r / R\n",
    "        vhat = v / V\n",
    "\n",
    "        # Heading error alpha = angle between velocity direction and LOS direction\n",
    "        cosang = float(np.clip(np.dot(vhat, rhat), -1.0, 1.0))\n",
    "        alpha = float(np.arccos(cosang))  # radians\n",
    "\n",
    "        # LOS angular rate omega (world frame)\n",
    "        omega = np.cross(r, vrel) / (float(np.dot(r, r)) + eps)\n",
    "        omega_mag = float(np.linalg.norm(omega))\n",
    "\n",
    "        # Closing speed (positive => closing)\n",
    "        vc = -float(np.dot(r, vrel)) / R\n",
    "\n",
    "        # --- PN term ---\n",
    "        N = 3.0\n",
    "        a_pn = N * vc * np.cross(omega, rhat)  # lateral accel in world frame\n",
    "\n",
    "        # --- Acquisition term (turn-to-LOS) ---\n",
    "        # Perpendicular component of LOS relative to forward direction\n",
    "        rhat_perp = rhat - float(np.dot(rhat, vhat)) * vhat\n",
    "        nperp = float(np.linalg.norm(rhat_perp))\n",
    "\n",
    "        if nperp < 1e-8:\n",
    "            a_acq = np.zeros(3, dtype=np.float64)\n",
    "        else:\n",
    "            rhat_perp /= nperp  # unit sideways \"turn toward LOS\" direction\n",
    "\n",
    "            # Curvature-based magnitude: ~k * V^2 / R, saturate later via a_max\n",
    "            k_acq = 5.0  # try 3.0–8.0\n",
    "            a_acq = k_acq * (V * V / R) * rhat_perp\n",
    "\n",
    "        # --- Blend weight w: 0 => pure PN, 1 => pure acquisition ---\n",
    "\n",
    "        # Alpha-based weight (dominant)\n",
    "        alpha_on   = np.deg2rad(20.0)   # start blending earlier\n",
    "        alpha_full = np.deg2rad(55.0)\n",
    "\n",
    "        x_alpha = (alpha - alpha_on) / (alpha_full - alpha_on + eps)\n",
    "        w_alpha = self._smoothstep(x_alpha)\n",
    "\n",
    "        # Omega-based modifier (only boosts acquisition when PN is sleepy)\n",
    "        omega_full = 0.00\n",
    "        omega_on   = 0.05   # <-- key: less brittle than 0.02\n",
    "\n",
    "        x_omega = (omega_on - omega_mag) / (omega_on - omega_full + eps)\n",
    "        w_omega = self._smoothstep(x_omega)\n",
    "\n",
    "        # Robust combine: alpha dominates; omega can't fully shut it off\n",
    "        w = w_alpha * (0.25 + 0.75 * w_omega)\n",
    "\n",
    "        # Optional: if not closing, force strong acquisition\n",
    "        if vc <= 0.0:\n",
    "            w = max(w, 0.9)\n",
    "\n",
    "        a_ideal = (1.0 - w) * a_pn + w * a_acq\n",
    "\n",
    "        # Project into your lateral control basis (right/up) and normalize by a_max\n",
    "        # Note: Environment now handles gravity compensation internally,\n",
    "        # so ProNav outputs desired NET lateral accel (same semantics as PPO)\n",
    "        forward, right, up = self._compute_lateral_basis(self.defense_vel)\n",
    "        a_right = float(np.dot(a_ideal, right))\n",
    "        a_up    = float(np.dot(a_ideal, up))\n",
    "\n",
    "        action = np.array([a_right / self.a_max, a_up / self.a_max], dtype=np.float32)\n",
    "        return np.clip(action, -1.0, 1.0)\n",
    "    \n",
    "    def _rate_limit_norm(self, a_cmd, a_prev, da_max, dt):\n",
    "        delta = a_cmd - a_prev\n",
    "        max_delta = da_max * dt\n",
    "        dnorm = float(np.linalg.norm(delta))\n",
    "        if dnorm <= max_delta or dnorm < 1e-9:\n",
    "            return a_cmd\n",
    "        return a_prev + delta * (max_delta / dnorm)\n",
    "    \n",
    "    def _segment_sphere_intersect(self, r0, r1, r_hit):\n",
    "        dr = r1 - r0\n",
    "        dr_norm_sq = float(np.dot(dr, dr))\n",
    "        if dr_norm_sq < 1e-12:\n",
    "            return float(np.dot(r0, r0)) <= r_hit * r_hit\n",
    "        s_star = -float(np.dot(r0, dr)) / dr_norm_sq\n",
    "        s_star = max(0.0, min(1.0, s_star))\n",
    "        r_closest = r0 + s_star * dr\n",
    "        return float(np.dot(r_closest, r_closest)) <= r_hit * r_hit\n",
    "    \n",
    "    def _phi_zem_perp(self):\n",
    "        \"\"\"\n",
    "        Potential for potential-based shaping:\n",
    "          Phi(s) = - ||ZEM_perp|| / zem_scale\n",
    "\n",
    "        Returns:\n",
    "          phi, zem_perp_norm, Vc, tgo\n",
    "        \"\"\"\n",
    "        eps = 1e-9\n",
    "\n",
    "        r = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
    "        vrel = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
    "\n",
    "        R = float(np.linalg.norm(r)) + eps\n",
    "        rhat = r / R\n",
    "\n",
    "        # Closing speed (positive = closing)\n",
    "        Vc = -float(np.dot(rhat, vrel))\n",
    "\n",
    "        if Vc > self.vc_min:\n",
    "            tgo = R / max(Vc, eps)\n",
    "            tgo = float(np.clip(tgo, 0.0, self.tgo_max))\n",
    "        else:\n",
    "            tgo = float(self.tgo_fixed)\n",
    "\n",
    "        zem = r + vrel * tgo\n",
    "        zem_perp = zem - float(np.dot(zem, rhat)) * rhat\n",
    "        zem_perp_norm = float(np.linalg.norm(zem_perp))\n",
    "\n",
    "        phi = -zem_perp_norm / (float(self.zem_scale) + eps)\n",
    "        return float(phi), zem_perp_norm, float(Vc), tgo\n",
    "    \n",
    "    \n",
    "    def _get_obs(self):\n",
    "        eps = 1e-9\n",
    "\n",
    "        # World-frame relative state\n",
    "        r_world = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
    "        vrel_world = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
    "\n",
    "        # Local basis from defense velocity (world frame unit vectors)\n",
    "        forward, right, up = self._compute_lateral_basis(self.defense_vel)\n",
    "\n",
    "        # ===============================\n",
    "        # 1) Ego-frame (body-frame) r and vrel\n",
    "        # ===============================\n",
    "        r_body = np.array([\n",
    "            float(np.dot(r_world, forward)),\n",
    "            float(np.dot(r_world, right)),\n",
    "            float(np.dot(r_world, up)),\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        vrel_body = np.array([\n",
    "            float(np.dot(vrel_world, forward)),\n",
    "            float(np.dot(vrel_world, right)),\n",
    "            float(np.dot(vrel_world, up)),\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        # Normalize r_body / vrel_body (keep your original scaling)\n",
    "        pos_scale = float(self.range_hard_max)   # 1_000_000\n",
    "        vel_scale = 4000.0\n",
    "\n",
    "        r_body_n = (r_body / (pos_scale + eps)).astype(np.float32)\n",
    "        vrel_body_n = (vrel_body / (vel_scale + eps)).astype(np.float32)\n",
    "\n",
    "        # ===============================\n",
    "        # 2) Actuator state in the same action frame\n",
    "        # ===============================\n",
    "        a_lat = np.array([\n",
    "            float(np.dot(self.a_actual, right)) / (self.a_max + eps),\n",
    "            float(np.dot(self.a_actual, up)) / (self.a_max + eps),\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        # NEW: hidden actuator state that affects transitions\n",
    "        a_cmd_prev_lat = np.array([\n",
    "            float(np.dot(self.a_cmd_prev, right)) / (self.a_max + eps),\n",
    "            float(np.dot(self.a_cmd_prev, up)) / (self.a_max + eps),\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        # ===============================\n",
    "        # 3) Scalar helpers (kept)\n",
    "        # ===============================\n",
    "        dist = float(np.linalg.norm(r_world)) + 1e-6\n",
    "        v_close = -float(np.dot(r_world, vrel_world)) / dist  # positive when closing\n",
    "\n",
    "        dist_n = np.float32(np.clip(dist / 1_000_000.0, 0.0, 4.0))\n",
    "        vclose_n = np.float32(np.clip(v_close / 3000.0, -2.0, 2.0))\n",
    "        dist_vclose_feat = np.array([dist_n, vclose_n], dtype=np.float32)\n",
    "\n",
    "        # Defense own vertical state (keep for ground constraint)\n",
    "        def_z_n = np.float32(np.clip(self.defense_pos[2] / 100_000.0, -1.0, 2.0))\n",
    "        def_vz_n = np.float32(np.clip(self.defense_vel[2] / 3000.0, -2.0, 2.0))\n",
    "        def_state_feat = np.array([def_z_n, def_vz_n], dtype=np.float32)\n",
    "\n",
    "        # ===============================\n",
    "        # 4) Keep your geometry features (consistent with ego-frame)\n",
    "        # ===============================\n",
    "        dist_body = float(np.linalg.norm(r_body)) + 1e-6\n",
    "\n",
    "        # LOS lateral projections in body frame\n",
    "        los_right = float(r_body[1] / dist_body)\n",
    "        los_up    = float(r_body[2] / dist_body)\n",
    "\n",
    "        # LOS rate omega in body frame: omega = (r x vrel)/||r||^2\n",
    "        dist2_body = float(np.dot(r_body, r_body)) + eps\n",
    "        omega_body = np.cross(r_body, vrel_body) / dist2_body\n",
    "\n",
    "        omega_right = float(omega_body[1])\n",
    "        omega_up    = float(omega_body[2])\n",
    "\n",
    "        omega_scale = 2.0\n",
    "        omega_right_n = float(np.clip(omega_right / omega_scale, -2.0, 2.0))\n",
    "        omega_up_n    = float(np.clip(omega_up / omega_scale, -2.0, 2.0))\n",
    "\n",
    "        geom_feat = np.array([los_right, los_up, omega_right_n, omega_up_n], dtype=np.float32)\n",
    "\n",
    "        # ===============================\n",
    "        # 5) NEW: kinematics garnish\n",
    "        # ===============================\n",
    "        V_def = float(np.linalg.norm(self.defense_vel))\n",
    "        V_def_n = np.float32(np.clip(V_def / 3000.0, 0.0, 3.0))  # scale: 3000 m/s baseline\n",
    "        forward_z = np.float32(float(forward[2]))               # dot(forward, world_up) since world_up=[0,0,1]\n",
    "\n",
    "        kin_feat = np.array([V_def_n, forward_z], dtype=np.float32)\n",
    "\n",
    "        # Final obs (20D)\n",
    "        obs = np.concatenate(\n",
    "            [r_body_n, vrel_body_n, a_lat, a_cmd_prev_lat, dist_vclose_feat, def_state_feat, geom_feat, kin_feat],\n",
    "            axis=0\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # Optional sanity check while iterating\n",
    "        # assert obs.shape == (20,), obs.shape\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _compute_lateral_basis(self, velocity):\n",
    "        \"\"\"\n",
    "        Horizon-stable basis:\n",
    "          forward = along velocity\n",
    "          right   = world_up x forward  (horizontal right)\n",
    "          up      = forward x right     (completes orthonormal frame)\n",
    "        This keeps 'up' as close to world-up as possible and avoids weird twisting.\n",
    "        \"\"\"\n",
    "        speed = float(np.linalg.norm(velocity))\n",
    "        if speed < 1.0:\n",
    "            forward = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
    "        else:\n",
    "            forward = (velocity / speed).astype(np.float32)\n",
    "\n",
    "        world_up = np.array([0.0, 0.0, 1.0], dtype=np.float32)\n",
    "\n",
    "        # right = world_up x forward\n",
    "        right_raw = np.cross(world_up, forward)\n",
    "        rnorm = float(np.linalg.norm(right_raw))\n",
    "\n",
    "        # If forward is near world_up, right_raw ~ 0. Pick a consistent fallback.\n",
    "        if rnorm < 1e-6:\n",
    "            # Choose a fixed \"north\" axis in world XY and build right from that\n",
    "            # This prevents random spinning when vertical.\n",
    "            north = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
    "            right_raw = np.cross(north, forward)\n",
    "            rnorm = float(np.linalg.norm(right_raw))\n",
    "            if rnorm < 1e-6:\n",
    "                north = np.array([0.0, 1.0, 0.0], dtype=np.float32)\n",
    "                right_raw = np.cross(north, forward)\n",
    "                rnorm = float(np.linalg.norm(right_raw))\n",
    "\n",
    "        right = (right_raw / (rnorm + 1e-9)).astype(np.float32)\n",
    "\n",
    "        # up = forward x right (not right x forward)\n",
    "        up_raw = np.cross(forward, right)\n",
    "        up = (up_raw / (float(np.linalg.norm(up_raw)) + 1e-9)).astype(np.float32)\n",
    "\n",
    "        return forward, right, up\n",
    "\n",
    "    def step(self, action):\n",
    "        if getattr(self, \"done\", False):\n",
    "            return self._get_obs(), 0.0, True, False, {\"event\": \"called_step_after_done\", \"dist\": self.relative_distances[-1]}\n",
    "        \n",
    "        action = np.clip(action, -1.0, 1.0).astype(np.float32)\n",
    "        mag = float(np.linalg.norm(action))\n",
    "        if mag > 1.0:\n",
    "            action = action / mag\n",
    "            mag = 1.0\n",
    "        \n",
    "        # Update episode trackers\n",
    "        self.ep_max_action_mag = max(self.ep_max_action_mag, float(mag))\n",
    "        self.ep_max_accel = max(self.ep_max_accel, float(np.linalg.norm(self.a_actual)))\n",
    "\n",
    "        dist_before = float(np.linalg.norm(self.enemy_pos - self.defense_pos))\n",
    "        \n",
    "        # --- Shaping: compute phi BEFORE transition ---\n",
    "        phi_before, zem_perp_before, Vc_before, tgo_before = self._phi_zem_perp()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        event = \"running\"\n",
    "        \n",
    "        for _ in range(self.n_substeps):\n",
    "            dt = self.dt_sim\n",
    "            enemy_pos_old = self.enemy_pos.copy()\n",
    "            defense_pos_old = self.defense_pos.copy()\n",
    "            \n",
    "            forward, right, up = self._compute_lateral_basis(self.defense_vel)\n",
    "            \n",
    "            # Agent command = desired NET lateral accel (world frame)\n",
    "            a_net_lat_cmd = (action[0] * self.a_max * right) + (action[1] * self.a_max * up)\n",
    "            \n",
    "            # Gravity (world frame)\n",
    "            g_vec = np.array([0.0, 0.0, -self.g], dtype=np.float32)\n",
    "            \n",
    "            # Lateral component of gravity in the right/up plane\n",
    "            g_lat = (np.dot(g_vec, right) * right) + (np.dot(g_vec, up) * up)\n",
    "            \n",
    "            # Fins must cancel lateral gravity to achieve commanded NET lateral accel\n",
    "            a_fins_cmd = a_net_lat_cmd - g_lat\n",
    "            \n",
    "            # Apply rate limit + lag to fins acceleration\n",
    "            self.a_cmd_prev = self._rate_limit_norm(a_fins_cmd, self.a_cmd_prev, self.da_max, dt)\n",
    "            self.a_actual += (self.a_cmd_prev - self.a_actual) * (dt / self.tau)\n",
    "            \n",
    "            # Integrate translational dynamics\n",
    "            self.defense_vel += (self.a_actual + g_vec) * dt\n",
    "            self.defense_pos += self.defense_vel * dt\n",
    "            self.defense_x, self.defense_y, self.defense_z = self.defense_pos\n",
    "            \n",
    "            # Enemy missile: pure ballistic (gravity only)\n",
    "            self.enemy_vel += g_vec * dt\n",
    "            self.enemy_pos += self.enemy_vel * dt\n",
    "            self.enemy_x, self.enemy_y, self.enemy_z = self.enemy_pos\n",
    "            self.t += dt\n",
    "            \n",
    "            r0 = enemy_pos_old - defense_pos_old\n",
    "            r1 = self.enemy_pos - self.defense_pos\n",
    "            if self._segment_sphere_intersect(r0, r1, self.collision_radius):\n",
    "                self.success = True\n",
    "                terminated = True\n",
    "                self.done = True\n",
    "                event = \"hit\"\n",
    "                self.time_to_hit = float(self.t)\n",
    "                self.terminal_event = \"hit\"\n",
    "                break\n",
    "            \n",
    "            dist = float(np.linalg.norm(self.enemy_pos - self.defense_pos))\n",
    "            if dist > self.max_distance:\n",
    "                truncated = True\n",
    "                self.done = True\n",
    "                event = \"diverged\"\n",
    "                self.terminal_event = \"diverged\"\n",
    "                break\n",
    "            if self.defense_pos[2] < 0:\n",
    "                terminated = True\n",
    "                self.done = True\n",
    "                event = \"defense_ground\"\n",
    "                self.terminal_event = \"defense_ground\"\n",
    "                break\n",
    "            if self.enemy_pos[2] < 0:\n",
    "                terminated = True\n",
    "                self.done = True\n",
    "                event = \"enemy_ground\"\n",
    "                self.terminal_event = \"enemy_ground\"\n",
    "                break\n",
    "            if self.t >= self.t_max:\n",
    "                truncated = True\n",
    "                self.done = True\n",
    "                event = \"timeout\"\n",
    "                self.terminal_event = \"timeout\"\n",
    "                break\n",
    "\n",
    "        self.enemy_path.append(self.enemy_pos.copy())\n",
    "        self.defense_path.append(self.defense_pos.copy())\n",
    "        self.relative_distances.append(float(np.linalg.norm(self.enemy_pos - self.defense_pos)))\n",
    "        self.times.append(self.t)\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "        dist_after = float(np.linalg.norm(self.enemy_pos - self.defense_pos))\n",
    "        self.min_dist = min(getattr(self, \"min_dist\", float(\"inf\")), dist_after)\n",
    "        self.ep_min_dist = min(self.ep_min_dist, float(dist_after))\n",
    "        \n",
    "        # Reward calculation (same as before)\n",
    "        r_progress = (dist_before - dist_after) / 100.0\n",
    "        v_scale = 1500.0\n",
    "        r = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
    "        vrel = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
    "        d = float(np.linalg.norm(r)) + 1e-9\n",
    "        rhat = r / d\n",
    "        d_dot = float(np.dot(rhat, vrel))\n",
    "        r_close = np.tanh((-d_dot) / v_scale)\n",
    "        \n",
    "        # --- Shaping: compute phi AFTER transition ---\n",
    "        if terminated or truncated:\n",
    "            # standard trick for episodic shaping: set terminal potential to 0\n",
    "            phi_after = 0.0\n",
    "            zem_perp_after = None\n",
    "            Vc_after = None\n",
    "            tgo_after = None\n",
    "        else:\n",
    "            phi_after, zem_perp_after, Vc_after, tgo_after = self._phi_zem_perp()\n",
    "\n",
    "        r_zem = self.w_zem * (self.gamma_shape * phi_after - phi_before)\n",
    "        \n",
    "        # Accumulate shaping rewards for debugging\n",
    "        self.sum_r_progress += float(r_progress)\n",
    "        self.sum_r_close += float(r_close)\n",
    "        self.sum_r_zem += float(r_zem)\n",
    "        \n",
    "        # Reward breakdown (named components)\n",
    "        step_penalty = -0.001\n",
    "        terminal_bonus = 0.0\n",
    "        terminal_penalty = 0.0\n",
    "        \n",
    "        if self.success:\n",
    "            terminal_bonus = 10000.0\n",
    "        elif terminated or truncated:\n",
    "            if event == \"defense_ground\":\n",
    "                terminal_penalty += 5000.0\n",
    "            terminal_penalty += min(2000.0, self.ep_min_dist / 50.0)\n",
    "        \n",
    "        reward = (1.0 * r_progress) + (0.1 * r_close) + step_penalty + terminal_bonus - terminal_penalty + r_zem\n",
    "        \n",
    "        info = {\n",
    "            \"event\": event,\n",
    "            \"t\": float(self.t),\n",
    "            \n",
    "            # reward pieces (training-related)\n",
    "            \"reward_terms\": {\n",
    "                \"r_progress\": float(r_progress),\n",
    "                \"r_close\": float(r_close),\n",
    "                \"step_penalty\": float(step_penalty),\n",
    "                \"terminal_bonus\": float(terminal_bonus),\n",
    "                \"terminal_penalty\": float(-terminal_penalty),  # negative contribution\n",
    "                \"r_zem\": float(r_zem),\n",
    "            },\n",
    "            \"reward\": float(reward),\n",
    "            \n",
    "            # ZEM shaping debug info\n",
    "            \"zem_debug\": {\n",
    "                \"phi_before\": float(phi_before),\n",
    "                \"phi_after\": float(phi_after),\n",
    "                \"zem_perp_before\": float(zem_perp_before),\n",
    "                \"zem_perp_after\": None if zem_perp_after is None else float(zem_perp_after),\n",
    "                \"Vc_before\": float(Vc_before),\n",
    "                \"Vc_after\": None if Vc_after is None else float(Vc_after),\n",
    "                \"tgo_before\": float(tgo_before),\n",
    "                \"tgo_after\": None if tgo_after is None else float(tgo_after),\n",
    "            },\n",
    "            \n",
    "            # eval snapshots (NOT the full episode metrics yet)\n",
    "            \"eval_step\": {\n",
    "                \"dist\": float(dist_after),\n",
    "                \"action_mag\": float(mag),\n",
    "                \"accel_norm\": float(np.linalg.norm(self.a_actual)),\n",
    "            },\n",
    "        }\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    def _snapshot(self):\n",
    "        \"\"\"Copy everything that affects future transitions + trackers used by reward.\"\"\"\n",
    "        return {\n",
    "            \"t\": float(self.t),\n",
    "            \"enemy_pos\": self.enemy_pos.copy(),\n",
    "            \"enemy_vel\": self.enemy_vel.copy(),\n",
    "            \"defense_pos\": self.defense_pos.copy(),\n",
    "            \"defense_vel\": self.defense_vel.copy(),\n",
    "            \"a_actual\": self.a_actual.copy(),\n",
    "            \"a_cmd_prev\": self.a_cmd_prev.copy(),\n",
    "            \"done\": bool(getattr(self, \"done\", False)),\n",
    "            \"success\": bool(getattr(self, \"success\", False)),\n",
    "\n",
    "            # reward-critical trackers\n",
    "            \"min_dist\": float(getattr(self, \"min_dist\", float(\"inf\"))),\n",
    "            \"ep_min_dist\": float(getattr(self, \"ep_min_dist\", float(\"inf\"))),\n",
    "\n",
    "            # episode trackers (not strictly needed for the local test but cheap)\n",
    "            \"ep_max_action_mag\": float(getattr(self, \"ep_max_action_mag\", 0.0)),\n",
    "            \"ep_max_accel\": float(getattr(self, \"ep_max_accel\", 0.0)),\n",
    "            \"time_to_hit\": getattr(self, \"time_to_hit\", None),\n",
    "            \"terminal_event\": getattr(self, \"terminal_event\", \"running\"),\n",
    "\n",
    "            # log lengths (so restore can truncate)\n",
    "            \"enemy_path_len\": len(getattr(self, \"enemy_path\", [])),\n",
    "            \"defense_path_len\": len(getattr(self, \"defense_path\", [])),\n",
    "            \"relative_distances_len\": len(getattr(self, \"relative_distances\", [])),\n",
    "            \"times_len\": len(getattr(self, \"times\", [])),\n",
    "        }\n",
    "\n",
    "    def _restore(self, snap):\n",
    "        \"\"\"Restore environment state from snapshot.\"\"\"\n",
    "        self.t = float(snap[\"t\"])\n",
    "        self.enemy_pos = snap[\"enemy_pos\"].copy()\n",
    "        self.enemy_vel = snap[\"enemy_vel\"].copy()\n",
    "        self.defense_pos = snap[\"defense_pos\"].copy()\n",
    "        self.defense_vel = snap[\"defense_vel\"].copy()\n",
    "        self.a_actual = snap[\"a_actual\"].copy()\n",
    "        self.a_cmd_prev = snap[\"a_cmd_prev\"].copy()\n",
    "        self.done = bool(snap[\"done\"])\n",
    "        self.success = bool(snap[\"success\"])\n",
    "\n",
    "        self.min_dist = float(snap[\"min_dist\"])\n",
    "        self.ep_min_dist = float(snap[\"ep_min_dist\"])\n",
    "\n",
    "        self.ep_max_action_mag = float(snap[\"ep_max_action_mag\"])\n",
    "        self.ep_max_accel = float(snap[\"ep_max_accel\"])\n",
    "        self.time_to_hit = snap[\"time_to_hit\"]\n",
    "        self.terminal_event = snap[\"terminal_event\"]\n",
    "\n",
    "        # truncate logs (avoid memory blow + keep things consistent)\n",
    "        if hasattr(self, \"enemy_path\"):\n",
    "            self.enemy_path = self.enemy_path[: snap[\"enemy_path_len\"]]\n",
    "        if hasattr(self, \"defense_path\"):\n",
    "            self.defense_path = self.defense_path[: snap[\"defense_path_len\"]]\n",
    "        if hasattr(self, \"relative_distances\"):\n",
    "            self.relative_distances = self.relative_distances[: snap[\"relative_distances_len\"]]\n",
    "        if hasattr(self, \"times\"):\n",
    "            self.times = self.times[: snap[\"times_len\"]]\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None: self.np_random = np.random.RandomState(seed)\n",
    "        self.done = False\n",
    "        self.success = False\n",
    "        self.t = 0.0\n",
    "        self.generate_enemy_missile()\n",
    "        self.generate_defense_missile()\n",
    "        \n",
    "        self.defense_vel = np.array([\n",
    "            self.defense_initial_velocity * np.cos(self.defense_azimuth) * np.cos(self.defense_theta),\n",
    "            self.defense_initial_velocity * np.sin(self.defense_azimuth) * np.cos(self.defense_theta),\n",
    "            self.defense_initial_velocity * np.sin(self.defense_theta)\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        self.enemy_vel = np.array([\n",
    "            self.enemy_initial_velocity * np.cos(self.enemy_azimuth) * np.cos(self.enemy_theta),\n",
    "            self.enemy_initial_velocity * np.sin(self.enemy_azimuth) * np.cos(self.enemy_theta),\n",
    "            self.enemy_initial_velocity * np.sin(self.enemy_theta)\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        self.a_actual = np.zeros(3, dtype=np.float32)\n",
    "        self.a_cmd_prev = np.zeros(3, dtype=np.float32)\n",
    "        self.defense_pos = np.array([self.defense_x, self.defense_y, self.defense_z], dtype=np.float32)\n",
    "        self.enemy_pos = np.array([self.enemy_x, self.enemy_y, self.enemy_z], dtype=np.float32)\n",
    "        self.enemy_path = [self.enemy_pos.copy()]\n",
    "        self.defense_path = [self.defense_pos.copy()]\n",
    "        self.relative_distances = [float(np.linalg.norm(self.enemy_pos - self.defense_pos))]\n",
    "        self.times = [self.t]\n",
    "        self.min_dist = float(self.relative_distances[-1])\n",
    "        self.sum_r_progress = 0.0\n",
    "        self.sum_r_close = 0.0\n",
    "        self.sum_r_zem = 0.0\n",
    "        \n",
    "        # --- episode eval trackers (NOT used in reward) ---\n",
    "        self.ep_min_dist = float(\"inf\")\n",
    "        self.ep_max_action_mag = 0.0\n",
    "        self.ep_max_accel = 0.0          # optional: actual accel norm\n",
    "        self.time_to_hit = None\n",
    "        self.terminal_event = \"running\"\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# EVALUATION FUNCTION (separate from training reward)\n",
    "# ==========================================\n",
    "def evaluate_policy(env, policy_fn, n_episodes=100, seed0=0):\n",
    "    \"\"\"\n",
    "    Evaluate a policy and return episode-level metrics.\n",
    "    This is separate from training reward - these metrics are what you actually care about.\n",
    "    \n",
    "    Args:\n",
    "        env: missile_interception_3d environment instance\n",
    "        policy_fn: Function that takes obs and returns action\n",
    "        n_episodes: Number of episodes to evaluate\n",
    "        seed0: Starting seed (episodes use seed0, seed0+1, ..., seed0+n_episodes-1)\n",
    "    \n",
    "    Returns:\n",
    "        summary: Dict with aggregated metrics (hit_rate, min_dist stats, etc.)\n",
    "        metrics: Dict with raw episode data\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"hits\": 0,\n",
    "        \"ground_defense\": 0,\n",
    "        \"ground_enemy\": 0,\n",
    "        \"diverged\": 0,\n",
    "        \"timeout\": 0,\n",
    "        \"min_dist_list\": [],\n",
    "        \"time_to_hit_list\": [],\n",
    "        \"max_g_list\": [],\n",
    "    }\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        obs, _ = env.reset(seed=seed0 + i)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy_fn(obs)  # your PPO policy OR env.calculate_pronav() baseline\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "        event = info[\"event\"]\n",
    "        metrics[\"min_dist_list\"].append(env.ep_min_dist)\n",
    "        metrics[\"max_g_list\"].append(env.ep_max_action_mag)\n",
    "\n",
    "        if event == \"hit\":\n",
    "            metrics[\"hits\"] += 1\n",
    "            if env.time_to_hit is not None:\n",
    "                metrics[\"time_to_hit_list\"].append(env.time_to_hit)\n",
    "        elif event == \"defense_ground\":\n",
    "            metrics[\"ground_defense\"] += 1\n",
    "        elif event == \"enemy_ground\":\n",
    "            metrics[\"ground_enemy\"] += 1\n",
    "        elif event == \"diverged\":\n",
    "            metrics[\"diverged\"] += 1\n",
    "        elif event == \"timeout\":\n",
    "            metrics[\"timeout\"] += 1\n",
    "\n",
    "    hit_rate = metrics[\"hits\"] / n_episodes\n",
    "\n",
    "    summary = {\n",
    "        \"hit_rate\": hit_rate,\n",
    "        \"min_dist_mean\": float(np.mean(metrics[\"min_dist_list\"])),\n",
    "        \"min_dist_p50\": float(np.median(metrics[\"min_dist_list\"])),\n",
    "        \"min_dist_p10\": float(np.percentile(metrics[\"min_dist_list\"], 10)),\n",
    "        \"time_to_hit_mean\": float(np.mean(metrics[\"time_to_hit_list\"])) if metrics[\"time_to_hit_list\"] else None,\n",
    "        \"max_g_mean\": float(np.mean(metrics[\"max_g_list\"])),\n",
    "        \"violations\": {\n",
    "            \"defense_ground\": metrics[\"ground_defense\"],\n",
    "            \"enemy_ground\": metrics[\"ground_enemy\"],\n",
    "            \"diverged\": metrics[\"diverged\"],\n",
    "            \"timeout\": metrics[\"timeout\"],\n",
    "        }\n",
    "    }\n",
    "    return summary, metrics\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# TEST PRONAV BASELINE (No Animation)\n",
    "# ==========================================\n",
    "\n",
    "# def run_baseline():\n",
    "#     env = missile_interception_3d()\n",
    "#     outcomes = []\n",
    "#     min_distances = []\n",
    "#     action_loads = [] # Track if we are saturating (maxing out fins)\n",
    "\n",
    "#     N_EPISODES = 50\n",
    "#     print(f\"Running {N_EPISODES} episodes of Augmented ProNav...\")\n",
    "\n",
    "#     for i in range(N_EPISODES):\n",
    "#         obs, _ = env.reset(seed=i)\n",
    "#         done = False\n",
    "#         ep_actions = []\n",
    "\n",
    "#         while not done:\n",
    "#             # 1. Ask ProNav for the move\n",
    "#             action = env.calculate_pronav()\n",
    "            \n",
    "#             # 2. Track how hard it's pushing (0.0 to 1.0)\n",
    "#             mag = np.linalg.norm(action)\n",
    "#             ep_actions.append(mag)\n",
    "\n",
    "#             obs, reward, terminated, truncated, info = env.step(action)\n",
    "#             done = terminated or truncated\n",
    "        \n",
    "#         outcomes.append(info['event'])\n",
    "#         min_distances.append(info['min_dist'])\n",
    "#         avg_load = np.mean(ep_actions)\n",
    "#         action_loads.append(avg_load)\n",
    "\n",
    "#         print(f\"Ep {i+1:02d} | Res: {info['event']:<14} | Min Dist: {info['min_dist']:.1f} m | Avg G-Load: {avg_load*100:.1f}%\")\n",
    "\n",
    "#     # Final Stats\n",
    "#     hits = outcomes.count(\"hit\")\n",
    "#     print(\"\\n--- SUMMARY ---\")\n",
    "#     print(f\"Hit Rate: {hits}/{N_EPISODES} ({hits/N_EPISODES*100:.1f}%)\")\n",
    "#     print(f\"Average Miss Distance (Non-hits): {np.mean([d for d, e in zip(min_distances, outcomes) if e != 'hit']):.2f} m\")\n",
    "#     print(f\"Average G-Loading: {np.mean(action_loads)*100:.1f}% (If >90%, missile is physically too weak)\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1158f029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating policy zoo...\n",
      "\n",
      "--- Running policy: teacher (50 episodes) ---\n",
      "  ep   10/50 | seed    9 | event=hit            | ret= 11025.86 | min_d=    127.3 | HR=100.0%\n",
      "  ep   20/50 | seed   19 | event=hit            | ret= 10702.13 | min_d=    134.1 | HR=100.0%\n",
      "  ep   30/50 | seed   29 | event=hit            | ret= 11591.47 | min_d=    143.0 | HR=100.0%\n",
      "  ep   40/50 | seed   39 | event=hit            | ret= 11484.13 | min_d=    135.1 | HR=100.0%\n",
      "  ep   50/50 | seed   49 | event=hit            | ret= 11186.65 | min_d=    115.7 | HR=100.0%\n",
      "--- Done: teacher | hit_rate=100.0% | return_mean=11301.01 ---\n",
      "\n",
      "--- Running policy: do_nothing (50 episodes) ---\n",
      "  ep   10/50 | seed    9 | event=enemy_ground   | ret= -2955.92 | min_d=  46595.9 | HR=  0.0%\n",
      "  ep   20/50 | seed   19 | event=enemy_ground   | ret= -3108.64 | min_d=  29574.8 | HR=  0.0%\n",
      "  ep   30/50 | seed   29 | event=enemy_ground   | ret= -3570.39 | min_d=  60892.3 | HR=  0.0%\n",
      "  ep   40/50 | seed   39 | event=enemy_ground   | ret= -4123.06 | min_d=  77450.6 | HR=  0.0%\n",
      "  ep   50/50 | seed   49 | event=enemy_ground   | ret= -4451.02 | min_d=  56812.4 | HR=  0.0%\n",
      "--- Done: do_nothing | hit_rate=0.0% | return_mean=-3912.93 ---\n",
      "\n",
      "--- Running policy: random (50 episodes) ---\n",
      "  ep   10/50 | seed    9 | event=enemy_ground   | ret= -3110.44 | min_d=  54293.1 | HR=  0.0%\n",
      "  ep   20/50 | seed   19 | event=enemy_ground   | ret= -3121.33 | min_d=  27088.4 | HR=  0.0%\n",
      "  ep   30/50 | seed   29 | event=enemy_ground   | ret= -3526.17 | min_d=  44304.3 | HR=  0.0%\n",
      "  ep   40/50 | seed   39 | event=enemy_ground   | ret= -4465.26 | min_d=  87170.9 | HR=  0.0%\n",
      "  ep   50/50 | seed   49 | event=enemy_ground   | ret= -4555.58 | min_d=  58308.2 | HR=  0.0%\n",
      "--- Done: random | hit_rate=0.0% | return_mean=-3977.65 ---\n",
      "\n",
      "--- Running policy: anti_teacher (50 episodes) ---\n",
      "  ep   10/50 | seed    9 | event=enemy_ground   | ret= -3767.25 | min_d=  85542.6 | HR=  0.0%\n",
      "  ep   20/50 | seed   19 | event=enemy_ground   | ret= -3363.15 | min_d=  54956.4 | HR=  0.0%\n",
      "  ep   30/50 | seed   29 | event=enemy_ground   | ret= -4746.91 | min_d= 134979.6 | HR=  0.0%\n",
      "  ep   40/50 | seed   39 | event=enemy_ground   | ret= -5250.95 | min_d= 132253.0 | HR=  0.0%\n",
      "  ep   50/50 | seed   49 | event=enemy_ground   | ret= -5645.65 | min_d= 104513.6 | HR=  0.0%\n",
      "--- Done: anti_teacher | hit_rate=0.0% | return_mean=-4768.34 ---\n",
      "\n",
      "--- Running policy: student (50 episodes) ---\n",
      "  ep   10/50 | seed    9 | event=enemy_ground   | ret= -2955.92 | min_d=  46595.9 | HR=  0.0%\n",
      "  ep   20/50 | seed   19 | event=enemy_ground   | ret= -3108.64 | min_d=  29574.8 | HR=  0.0%\n",
      "  ep   30/50 | seed   29 | event=enemy_ground   | ret= -3570.39 | min_d=  60892.3 | HR=  0.0%\n",
      "  ep   40/50 | seed   39 | event=enemy_ground   | ret= -4123.06 | min_d=  77450.6 | HR=  0.0%\n",
      "  ep   50/50 | seed   49 | event=enemy_ground   | ret= -4451.02 | min_d=  56812.4 | HR=  0.0%\n",
      "--- Done: student | hit_rate=0.0% | return_mean=-3912.93 ---\n",
      "\n",
      "=== POLICY ZOO REPORT (sorted by mean return) ===\n",
      "\n",
      "[teacher]\n",
      "  return_mean ± std : 11301.011 ± 401.116\n",
      "  hit_rate          : 100.0%\n",
      "  min_dist_mean     : 132.4 m | p50: 133.0 m\n",
      "  time_to_hit_mean  : 35.0484000000006\n",
      "  max_g_mean        : 0.778\n",
      "  violations        : {'defense_ground': 0, 'enemy_ground': 0, 'diverged': 0, 'timeout': 0}\n",
      "  reward_components_sum: {'r_progress': 63301.09775291443, 'r_close': 17237.0465872514, 'step_penalty': -17.545000000000012, 'terminal_bonus': 500000.0, 'terminal_penalty': 0.0, 'r_zem': 43.3107727604012}\n",
      "\n",
      "[do_nothing]\n",
      "  return_mean ± std : -3912.935 ± 624.893\n",
      "  hit_rate          : 0.0%\n",
      "  min_dist_mean     : 60252.1 m | p50: 61212.7 m\n",
      "  time_to_hit_mean  : None\n",
      "  max_g_mean        : 0.000\n",
      "  violations        : {'defense_ground': 0, 'enemy_ground': 50, 'diverged': 0, 'timeout': 0}\n",
      "  reward_components_sum: {'r_progress': -131454.25390625, 'r_close': -43797.77690097108, 'step_penalty': -79.95399999999671, 'terminal_bonus': 0.0, 'terminal_penalty': -59908.564101562515, 'r_zem': 175.81508557050753}\n",
      "\n",
      "[student]\n",
      "  return_mean ± std : -3912.935 ± 624.893\n",
      "  hit_rate          : 0.0%\n",
      "  min_dist_mean     : 60252.1 m | p50: 61212.7 m\n",
      "  time_to_hit_mean  : None\n",
      "  max_g_mean        : 0.000\n",
      "  violations        : {'defense_ground': 0, 'enemy_ground': 50, 'diverged': 0, 'timeout': 0}\n",
      "  reward_components_sum: {'r_progress': -131454.25390625, 'r_close': -43797.77690097108, 'step_penalty': -79.95399999999671, 'terminal_bonus': 0.0, 'terminal_penalty': -59908.564101562515, 'r_zem': 175.81508557050753}\n",
      "\n",
      "[random]\n",
      "  return_mean ± std : -3977.650 ± 668.004\n",
      "  hit_rate          : 0.0%\n",
      "  min_dist_mean     : 59126.8 m | p50: 58997.7 m\n",
      "  time_to_hit_mean  : None\n",
      "  max_g_mean        : 1.000\n",
      "  violations        : {'defense_ground': 0, 'enemy_ground': 50, 'diverged': 0, 'timeout': 0}\n",
      "  reward_components_sum: {'r_progress': -135744.20234375, 'r_close': -44112.07601123858, 'step_penalty': -79.95399999999671, 'terminal_bonus': 0.0, 'terminal_penalty': -58827.692148437505, 'r_zem': 180.56175049028423}\n",
      "\n",
      "[anti_teacher]\n",
      "  return_mean ± std : -4768.335 ± 922.204\n",
      "  hit_rate          : 0.0%\n",
      "  min_dist_mean     : 111927.3 m | p50: 115103.7 m\n",
      "  time_to_hit_mean  : None\n",
      "  max_g_mean        : 0.892\n",
      "  violations        : {'defense_ground': 0, 'enemy_ground': 50, 'diverged': 0, 'timeout': 0}\n",
      "  reward_components_sum: {'r_progress': -142248.36828124998, 'r_close': -61173.32218125375, 'step_penalty': -79.95399999999671, 'terminal_bonus': 0.0, 'terminal_penalty': -90046.89859375, 'r_zem': 75.79723646835536}\n",
      "\n",
      "No ordering red flags detected (still inspect component sums).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------\n",
    "# Policies\n",
    "# -------------------------\n",
    "\n",
    "def policy_teacher(env, obs):\n",
    "    return env.calculate_pronav()\n",
    "\n",
    "def policy_do_nothing(env, obs):\n",
    "    return np.zeros(2, dtype=np.float32)\n",
    "\n",
    "def policy_random(env, obs):\n",
    "    # uniform random in [-1,1], then renorm to unit ball like env does\n",
    "    a = env.np_random.uniform(-1.0, 1.0, size=(2,)).astype(np.float32)\n",
    "    mag = float(np.linalg.norm(a))\n",
    "    if mag > 1.0:\n",
    "        a = a / mag\n",
    "    return a\n",
    "\n",
    "def policy_anti_teacher(env, obs):\n",
    "    return -env.calculate_pronav()\n",
    "\n",
    "class StudentPolicy:\n",
    "    \"\"\"\n",
    "    Plug in your current student here.\n",
    "    Supported:\n",
    "      - SB3 model with .predict(obs, deterministic=True)\n",
    "      - PyTorch model returning action tensor\n",
    "      - or you can replace __call__ with your own logic\n",
    "    \"\"\"\n",
    "    def __init__(self, model=None, deterministic=True):\n",
    "        self.model = model\n",
    "        self.deterministic = deterministic\n",
    "\n",
    "    def __call__(self, env, obs):\n",
    "        if self.model is None:\n",
    "            # fallback: do nothing if you haven't loaded a student yet\n",
    "            return np.zeros(2, dtype=np.float32)\n",
    "\n",
    "        # Stable-Baselines3 style\n",
    "        if hasattr(self.model, \"predict\"):\n",
    "            action, _ = self.model.predict(obs, deterministic=self.deterministic)\n",
    "            return np.asarray(action, dtype=np.float32)\n",
    "\n",
    "        # Torch style (very generic)\n",
    "        try:\n",
    "            import torch\n",
    "            with torch.no_grad():\n",
    "                x = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "                a = self.model(x).squeeze(0).cpu().numpy().astype(np.float32)\n",
    "            return a\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Student model interface not recognized: {e}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Rollout / logging\n",
    "# -------------------------\n",
    "\n",
    "def run_episode(env, policy_fn, seed=None, max_steps=20000):\n",
    "    \"\"\"\n",
    "    Runs one episode, returns:\n",
    "      - total_return\n",
    "      - terminal event\n",
    "      - eval metrics (min_dist, time_to_hit, max_g, violations)\n",
    "      - reward component sums\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    total_return = 0.0\n",
    "    # sum reward components over the episode\n",
    "    comp_sums = defaultdict(float)\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        action = policy_fn(env, obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        steps += 1\n",
    "\n",
    "        total_return += float(reward)\n",
    "\n",
    "        # reward breakdown is in info[\"reward_terms\"]\n",
    "        rt = info.get(\"reward_terms\", {})\n",
    "        for k, v in rt.items():\n",
    "            comp_sums[k] += float(v)\n",
    "\n",
    "    event = info.get(\"event\", \"unknown\")\n",
    "\n",
    "    # Episode-level eval metrics come from env trackers\n",
    "    ep_eval = {\n",
    "        \"hit\": (event == \"hit\"),\n",
    "        \"min_dist\": float(getattr(env, \"ep_min_dist\", np.nan)),\n",
    "        \"time_to_hit\": float(env.time_to_hit) if getattr(env, \"time_to_hit\", None) is not None else None,\n",
    "        \"max_g\": float(getattr(env, \"ep_max_action_mag\", np.nan)),\n",
    "        \"ground_defense\": int(event == \"defense_ground\"),\n",
    "        \"ground_enemy\": int(event == \"enemy_ground\"),\n",
    "        \"diverged\": int(event == \"diverged\"),\n",
    "        \"timeout\": int(event == \"timeout\"),\n",
    "    }\n",
    "\n",
    "    return total_return, event, ep_eval, dict(comp_sums)\n",
    "\n",
    "\n",
    "def evaluate_policy_zoo(env, policies, n_episodes=50, seed0=0, print_every=10):\n",
    "    \"\"\"\n",
    "    policies: dict name -> policy_fn(env, obs) -> action\n",
    "    returns: dict name -> aggregated stats\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for name, pol in policies.items():\n",
    "        print(f\"\\n--- Running policy: {name} ({n_episodes} episodes) ---\")\n",
    "\n",
    "        totals = []\n",
    "        hits = 0\n",
    "        min_dists = []\n",
    "        times_to_hit = []\n",
    "        max_gs = []\n",
    "        violations = defaultdict(int)\n",
    "        comp_totals = defaultdict(float)\n",
    "\n",
    "        for i in range(n_episodes):\n",
    "            ep_seed = seed0 + i\n",
    "            total_return, event, ep_eval, comp_sums = run_episode(env, pol, seed=ep_seed)\n",
    "\n",
    "            totals.append(total_return)\n",
    "            hits += int(ep_eval[\"hit\"])\n",
    "            min_dists.append(ep_eval[\"min_dist\"])\n",
    "            max_gs.append(ep_eval[\"max_g\"])\n",
    "\n",
    "            if ep_eval[\"time_to_hit\"] is not None:\n",
    "                times_to_hit.append(ep_eval[\"time_to_hit\"])\n",
    "\n",
    "            violations[\"defense_ground\"] += ep_eval[\"ground_defense\"]\n",
    "            violations[\"enemy_ground\"] += ep_eval[\"ground_enemy\"]\n",
    "            violations[\"diverged\"] += ep_eval[\"diverged\"]\n",
    "            violations[\"timeout\"] += ep_eval[\"timeout\"]\n",
    "\n",
    "            for k, v in comp_sums.items():\n",
    "                comp_totals[k] += float(v)\n",
    "\n",
    "            # ---- progress print ----\n",
    "            if (i + 1) % print_every == 0 or (i + 1) == n_episodes:\n",
    "                hit_rate_so_far = hits / (i + 1)\n",
    "                print(\n",
    "                    f\"  ep {i+1:4d}/{n_episodes} | seed {ep_seed:4d} | \"\n",
    "                    f\"event={event:<14} | \"\n",
    "                    f\"ret={total_return:9.2f} | \"\n",
    "                    f\"min_d={ep_eval['min_dist']:9.1f} | \"\n",
    "                    f\"HR={hit_rate_so_far*100:5.1f}%\"\n",
    "                )\n",
    "\n",
    "        results[name] = {\n",
    "            \"hit_rate\": hits / n_episodes,\n",
    "            \"return_mean\": float(np.mean(totals)),\n",
    "            \"return_std\": float(np.std(totals)),\n",
    "            \"min_dist_mean\": float(np.mean(min_dists)),\n",
    "            \"min_dist_p50\": float(np.median(min_dists)),\n",
    "            \"time_to_hit_mean\": float(np.mean(times_to_hit)) if times_to_hit else None,\n",
    "            \"max_g_mean\": float(np.mean(max_gs)),\n",
    "            \"violations\": dict(violations),\n",
    "            \"reward_components_sum\": dict(comp_totals),\n",
    "        }\n",
    "\n",
    "        print(f\"--- Done: {name} | hit_rate={results[name]['hit_rate']*100:.1f}% | return_mean={results[name]['return_mean']:.2f} ---\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Pretty print + sanity ordering checks\n",
    "# -------------------------\n",
    "\n",
    "def print_policy_zoo_report(results):\n",
    "    # rank by return_mean\n",
    "    names_sorted = sorted(results.keys(), key=lambda n: results[n][\"return_mean\"], reverse=True)\n",
    "\n",
    "    print(\"\\n=== POLICY ZOO REPORT (sorted by mean return) ===\")\n",
    "    for n in names_sorted:\n",
    "        r = results[n]\n",
    "        print(f\"\\n[{n}]\")\n",
    "        print(f\"  return_mean ± std : {r['return_mean']:.3f} ± {r['return_std']:.3f}\")\n",
    "        print(f\"  hit_rate          : {r['hit_rate']*100:.1f}%\")\n",
    "        print(f\"  min_dist_mean     : {r['min_dist_mean']:.1f} m | p50: {r['min_dist_p50']:.1f} m\")\n",
    "        print(f\"  time_to_hit_mean  : {r['time_to_hit_mean'] if r['time_to_hit_mean'] is not None else 'None'}\")\n",
    "        print(f\"  max_g_mean        : {r['max_g_mean']:.3f}\")\n",
    "        print(f\"  violations        : {r['violations']}\")\n",
    "        print(f\"  reward_components_sum: {r['reward_components_sum']}\")\n",
    "\n",
    "    # sanity checks (soft assertions)\n",
    "    def get(name, key):\n",
    "        return results[name][key] if name in results else None\n",
    "\n",
    "    teacher = \"teacher\"\n",
    "    anti = \"anti_teacher\"\n",
    "    dn = \"do_nothing\"\n",
    "    rnd = \"random\"\n",
    "\n",
    "    warnings = []\n",
    "    if teacher in results and anti in results:\n",
    "        if results[teacher][\"return_mean\"] <= results[anti][\"return_mean\"]:\n",
    "            warnings.append(\"Teacher does NOT beat Anti-teacher on return_mean -> reward likely wrong.\")\n",
    "        if results[teacher][\"hit_rate\"] <= results[anti][\"hit_rate\"]:\n",
    "            warnings.append(\"Teacher does NOT beat Anti-teacher on hit_rate -> guidance or reward suspicious.\")\n",
    "\n",
    "    if teacher in results and dn in results:\n",
    "        if results[teacher][\"return_mean\"] <= results[dn][\"return_mean\"]:\n",
    "            warnings.append(\"Teacher does NOT beat Do-nothing on return_mean -> reward shaping might be inverted/weak.\")\n",
    "\n",
    "    if teacher in results and rnd in results:\n",
    "        if results[teacher][\"return_mean\"] <= results[rnd][\"return_mean\"]:\n",
    "            warnings.append(\"Teacher does NOT beat Random on return_mean -> reward is likely broken.\")\n",
    "\n",
    "    # Also: bad policies getting high returns is often due to shaping terms dominating terminal outcome\n",
    "    # Flag if anti-teacher has surprisingly high return but terrible hit_rate\n",
    "    if anti in results:\n",
    "        if results[anti][\"hit_rate\"] < 0.05 and results[anti][\"return_mean\"] > 0.0:\n",
    "            warnings.append(\"Anti-teacher has near-zero hit_rate but positive return -> reward hacking opportunity.\")\n",
    "\n",
    "    if warnings:\n",
    "        print(\"\\n=== WARNINGS ===\")\n",
    "        for w in warnings:\n",
    "            print(\" - \" + w)\n",
    "    else:\n",
    "        print(\"\\nNo ordering red flags detected (still inspect component sums).\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Local action-reward coupling test (Step 2 v3)\n",
    "# -------------------------\n",
    "\n",
    "def normalize_action(a: np.ndarray) -> np.ndarray:\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    a = np.clip(a, -1.0, 1.0)\n",
    "    mag = float(np.linalg.norm(a))\n",
    "    if mag > 1.0:\n",
    "        a = a / mag\n",
    "    return a\n",
    "\n",
    "def rand_unit_2(rng: np.random.RandomState) -> np.ndarray:\n",
    "    v = rng.randn(2).astype(np.float32)\n",
    "    n = float(np.linalg.norm(v))\n",
    "    if n < 1e-9:\n",
    "        return np.array([1.0, 0.0], dtype=np.float32)\n",
    "    return v / n\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Policies (feedback)\n",
    "# policy signature: policy(env, obs, rng) -> action\n",
    "# -------------------------\n",
    "\n",
    "def pol_teacher(env, obs, rng=None):\n",
    "    return env.calculate_pronav()\n",
    "\n",
    "def pol_anti_teacher(env, obs, rng=None):\n",
    "    return -env.calculate_pronav()\n",
    "\n",
    "def pol_do_nothing(env, obs, rng=None):\n",
    "    return np.zeros(2, dtype=np.float32)\n",
    "\n",
    "def pol_random(env, obs, rng):\n",
    "    a = rng.uniform(-1.0, 1.0, size=(2,)).astype(np.float32)\n",
    "    return normalize_action(a)\n",
    "\n",
    "def make_pol_teacher_noisy(eps: float):\n",
    "    # noise injected each step: a_t = norm(a*_t + eps * u_t)\n",
    "    def _pol(env, obs, rng):\n",
    "        a_star = env.calculate_pronav()\n",
    "        u = rand_unit_2(rng)\n",
    "        return normalize_action(a_star + eps * u)\n",
    "    return _pol\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Rollout utilities\n",
    "# -------------------------\n",
    "\n",
    "def rollout_k_steps_from_snapshot(env, snap, policy, K: int, rng: np.random.RandomState):\n",
    "    \"\"\"\n",
    "    Restores snap, then rolls K steps using policy feedback.\n",
    "    Returns:\n",
    "      total_return, comp_sums, event_last, dist_last\n",
    "    \"\"\"\n",
    "    env._restore(snap)\n",
    "    obs = env._get_obs()\n",
    "\n",
    "    total_r = 0.0\n",
    "    comps = defaultdict(float)\n",
    "    info_last = {}\n",
    "\n",
    "    for _ in range(K):\n",
    "        a = policy(env, obs, rng)\n",
    "        obs, r, terminated, truncated, info = env.step(a)\n",
    "        total_r += float(r)\n",
    "\n",
    "        rt = info.get(\"reward_terms\", {})\n",
    "        for k, v in rt.items():\n",
    "            comps[k] += float(v)\n",
    "\n",
    "        info_last = info\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    event = info_last.get(\"event\", \"unknown\")\n",
    "    dist = info_last.get(\"eval_step\", {}).get(\"dist\", None)\n",
    "    return total_r, dict(comps), event, dist\n",
    "\n",
    "\n",
    "def get_snapshot_from_trajectory(env, seed: int, snap_step: int, traj_policy, rng: np.random.RandomState, max_steps=20000):\n",
    "    \"\"\"\n",
    "    Runs traj_policy until snap_step, returns (ok, snap, obs_at_snap).\n",
    "    traj_policy is used only to generate the trajectory you sample the snapshot from.\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "\n",
    "    for t in range(min(snap_step, max_steps)):\n",
    "        a = traj_policy(env, obs, rng)\n",
    "        obs, r, terminated, truncated, info = env.step(a)\n",
    "        if terminated or truncated:\n",
    "            return False, None, None, info.get(\"event\", \"terminated_early\")\n",
    "\n",
    "    snap = env._snapshot()\n",
    "    return True, snap, obs, \"ok\"\n",
    "\n",
    "\n",
    "def find_valid_snapshot(env, base_seed: int, snap_step: int, snapshot_source: str, max_tries=20):\n",
    "    \"\"\"\n",
    "    snapshot_source: \"teacher\" or \"random\"\n",
    "    Tries multiple seeds until the trajectory survives to snap_step.\n",
    "    \"\"\"\n",
    "    for k in range(max_tries):\n",
    "        seed = base_seed + k\n",
    "        traj_rng = np.random.RandomState(10_000 + seed)\n",
    "\n",
    "        if snapshot_source == \"teacher\":\n",
    "            traj_policy = pol_teacher\n",
    "        elif snapshot_source == \"random\":\n",
    "            traj_policy = lambda e, o, rr: pol_random(e, o, rr)\n",
    "        else:\n",
    "            raise ValueError(\"snapshot_source must be 'teacher' or 'random'\")\n",
    "\n",
    "        ok, snap, obs, reason = get_snapshot_from_trajectory(\n",
    "            env, seed=seed, snap_step=snap_step, traj_policy=traj_policy, rng=traj_rng\n",
    "        )\n",
    "        if ok:\n",
    "            return True, seed, snap\n",
    "    return False, None, None\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main local coupling test (v3)\n",
    "# -------------------------\n",
    "\n",
    "def local_action_reward_sanity_v3(\n",
    "    env,\n",
    "    base_seed=0,\n",
    "    snap_step=50,\n",
    "    K=20,\n",
    "    snapshot_source=\"random\",         # \"teacher\" or \"random\"\n",
    "    eps_list=(0.05, 0.15, 0.30),\n",
    "    n_noisy=64,\n",
    "    n_random=64,\n",
    "    print_top_bottom=8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compares short-horizon K-step returns starting from the exact same snapshot:\n",
    "      - teacher feedback baseline\n",
    "      - anti-teacher feedback\n",
    "      - do-nothing\n",
    "      - random feedback\n",
    "      - teacher+noise feedback (per-step noise) for multiple eps magnitudes\n",
    "\n",
    "    Reports advantage distribution vs teacher: A = R - R_teacher.\n",
    "    \"\"\"\n",
    "\n",
    "    ok, seed_used, snap = find_valid_snapshot(env, base_seed, snap_step, snapshot_source=snapshot_source)\n",
    "    if not ok:\n",
    "        print(f\"[FAIL] Could not find valid snapshot: base_seed={base_seed}, snap_step={snap_step}, source={snapshot_source}\")\n",
    "        return {\"ok\": False}\n",
    "\n",
    "    # baseline teacher feedback\n",
    "    rng_base = np.random.RandomState(12345)\n",
    "    R_teacher, C_teacher, evT, distT = rollout_k_steps_from_snapshot(env, snap, pol_teacher, K, rng_base)\n",
    "\n",
    "    # evaluate some fixed baselines\n",
    "    baselines = {\n",
    "        \"teacher\": pol_teacher,\n",
    "        \"anti_teacher\": pol_anti_teacher,\n",
    "        \"do_nothing\": pol_do_nothing,\n",
    "    }\n",
    "\n",
    "    records = []  # (name, R, A, comps, event)\n",
    "    for name, pol in baselines.items():\n",
    "        rng = np.random.RandomState(20000 + hash(name) % 10_000)\n",
    "        R, comps, ev, dist = rollout_k_steps_from_snapshot(env, snap, pol, K, rng)\n",
    "        records.append((name, R, R - R_teacher, comps, ev))\n",
    "\n",
    "    # random policies\n",
    "    for j in range(n_random):\n",
    "        rng = np.random.RandomState(30000 + j)\n",
    "        R, comps, ev, dist = rollout_k_steps_from_snapshot(env, snap, lambda e,o,rr: pol_random(e,o,rr), K, rng)\n",
    "        records.append((f\"random_{j}\", R, R - R_teacher, comps, ev))\n",
    "\n",
    "    # teacher + per-step noise at different eps\n",
    "    for eps in eps_list:\n",
    "        pol_noisy = make_pol_teacher_noisy(eps)\n",
    "        for j in range(n_noisy):\n",
    "            rng = np.random.RandomState(40000 + int(1000*eps) + j)\n",
    "            R, comps, ev, dist = rollout_k_steps_from_snapshot(env, snap, pol_noisy, K, rng)\n",
    "            records.append((f\"teacher_noise_eps{eps:.2f}_{j}\", R, R - R_teacher, comps, ev))\n",
    "\n",
    "    # summarize by groups\n",
    "    def group_of(name: str):\n",
    "        if name == \"teacher\": return \"teacher\"\n",
    "        if name == \"anti_teacher\": return \"anti_teacher\"\n",
    "        if name == \"do_nothing\": return \"do_nothing\"\n",
    "        if name.startswith(\"random_\"): return \"random\"\n",
    "        if name.startswith(\"teacher_noise_\"): return \"teacher+noise\"\n",
    "        return \"other\"\n",
    "\n",
    "    groups = defaultdict(list)\n",
    "    for name, R, A, comps, ev in records:\n",
    "        groups[group_of(name)].append((name, R, A, comps, ev))\n",
    "\n",
    "    def summarize_adv(items):\n",
    "        adv = np.array([x[2] for x in items], dtype=np.float64)\n",
    "        return {\n",
    "            \"n\": int(len(adv)),\n",
    "            \"mean\": float(np.mean(adv)),\n",
    "            \"p50\": float(np.median(adv)),\n",
    "            \"p90\": float(np.percentile(adv, 90)),\n",
    "            \"max\": float(np.max(adv)),\n",
    "            \"min\": float(np.min(adv)),\n",
    "        }\n",
    "\n",
    "    # print report\n",
    "    print(\"\\n\" + \"=\"*78)\n",
    "    print(\"=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\")\n",
    "    print(f\"snapshot_source={snapshot_source} | seed_used={seed_used} | snap_step={snap_step} | K={K}\")\n",
    "    print(f\"R_teacher = {R_teacher:.6f} | event_last={evT}\")\n",
    "    print(\"-\"*78)\n",
    "\n",
    "    for gname in [\"teacher\", \"teacher+noise\", \"random\", \"do_nothing\", \"anti_teacher\"]:\n",
    "        if gname not in groups: \n",
    "            continue\n",
    "        s = summarize_adv(groups[gname])\n",
    "        print(f\"{gname:12s}  A=R-R_teacher: mean={s['mean']:+.6f}  p50={s['p50']:+.6f}  p90={s['p90']:+.6f}  max={s['max']:+.6f}  min={s['min']:+.6f}  n={s['n']}\")\n",
    "\n",
    "    # top/bottom by advantage (who beats teacher most / loses most)\n",
    "    records_sorted = sorted(records, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(\"\\nTop candidates by advantage (beating teacher):\")\n",
    "    for name, R, A, comps, ev in records_sorted[:print_top_bottom]:\n",
    "        # component deltas (against teacher)\n",
    "        d_prog = comps.get(\"r_progress\", 0.0) - C_teacher.get(\"r_progress\", 0.0)\n",
    "        d_close = comps.get(\"r_close\", 0.0) - C_teacher.get(\"r_close\", 0.0)\n",
    "        print(f\"  {name:28s}  A={A:+.6f}  R={R:.6f}  Δprog={d_prog:+.4f}  Δclose={d_close:+.4f}  ev={ev}\")\n",
    "\n",
    "    print(\"\\nBottom candidates by advantage (worst):\")\n",
    "    for name, R, A, comps, ev in records_sorted[-print_top_bottom:]:\n",
    "        d_prog = comps.get(\"r_progress\", 0.0) - C_teacher.get(\"r_progress\", 0.0)\n",
    "        d_close = comps.get(\"r_close\", 0.0) - C_teacher.get(\"r_close\", 0.0)\n",
    "        print(f\"  {name:28s}  A={A:+.6f}  R={R:.6f}  Δprog={d_prog:+.4f}  Δclose={d_close:+.4f}  ev={ev}\")\n",
    "\n",
    "    # verdict logic:\n",
    "    # - \"good\": random p90 advantage is negative (most random are worse than teacher)\n",
    "    # - \"warning\": random p90 advantage positive (many random beat teacher)\n",
    "    verdict = \"PASS\"\n",
    "    if \"random\" in groups:\n",
    "        sR = summarize_adv(groups[\"random\"])\n",
    "        if sR[\"p90\"] > 0.0:\n",
    "            verdict = \"WARNING: random p90 advantage > 0 (many random beat teacher locally)\"\n",
    "    if \"teacher+noise\" in groups:\n",
    "        sN = summarize_adv(groups[\"teacher+noise\"])\n",
    "        # if mild noise often beats teacher, basin is very flat\n",
    "        if sN[\"p90\"] > 0.0 and verdict == \"PASS\":\n",
    "            verdict = \"WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\"\n",
    "\n",
    "    print(\"\\nVERDICT:\", verdict)\n",
    "    print(\"=\"*78)\n",
    "\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"snapshot_source\": snapshot_source,\n",
    "        \"seed_used\": seed_used,\n",
    "        \"snap_step\": snap_step,\n",
    "        \"K\": K,\n",
    "        \"R_teacher\": R_teacher,\n",
    "        \"C_teacher\": C_teacher,\n",
    "        \"groups_summary\": {k: summarize_adv(v) for k, v in groups.items()},\n",
    "        \"records\": records,\n",
    "        \"verdict\": verdict,\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Coupling curve: advantage vs epsilon (win_rate + moments)\n",
    "# -------------------------\n",
    "\n",
    "def rollout_feedback_teacher(env, snap, K):\n",
    "    env._restore(snap)\n",
    "    total_r = 0.0\n",
    "    comps = defaultdict(float)\n",
    "    ev_last = \"running\"\n",
    "\n",
    "    obs = env._get_obs()\n",
    "    for _ in range(K):\n",
    "        a = env.calculate_pronav()\n",
    "        obs, r, terminated, truncated, info = env.step(a)\n",
    "        total_r += float(r)\n",
    "        ev_last = info.get(\"event\", ev_last)\n",
    "\n",
    "        rt = info.get(\"reward_terms\", {})\n",
    "        for k, v in rt.items():\n",
    "            comps[k] += float(v)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return total_r, dict(comps), ev_last\n",
    "\n",
    "\n",
    "def rollout_feedback_teacher_plus_noise(env, snap, K, eps, rng: np.random.RandomState):\n",
    "    env._restore(snap)\n",
    "    total_r = 0.0\n",
    "    comps = defaultdict(float)\n",
    "    ev_last = \"running\"\n",
    "\n",
    "    obs = env._get_obs()\n",
    "    for _ in range(K):\n",
    "        a_star = env.calculate_pronav()\n",
    "\n",
    "        noise = rng.normal(size=a_star.shape).astype(np.float32)\n",
    "        noise /= (float(np.linalg.norm(noise)) + 1e-8)\n",
    "        a = a_star + float(eps) * noise\n",
    "        a = normalize_action(a)\n",
    "\n",
    "        obs, r, terminated, truncated, info = env.step(a)\n",
    "        total_r += float(r)\n",
    "        ev_last = info.get(\"event\", ev_last)\n",
    "\n",
    "        rt = info.get(\"reward_terms\", {})\n",
    "        for k, v in rt.items():\n",
    "            comps[k] += float(v)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return total_r, dict(comps), ev_last\n",
    "\n",
    "\n",
    "def win_stats(A):\n",
    "    A = np.asarray(A, dtype=np.float64)\n",
    "    if A.size == 0:\n",
    "        return {\n",
    "            \"n\": 0,\n",
    "            \"win_rate\": 0.0,\n",
    "            \"mean\": 0.0,\n",
    "            \"p50\": 0.0,\n",
    "            \"p90\": 0.0,\n",
    "            \"max\": 0.0,\n",
    "            \"min\": 0.0,\n",
    "            \"mean_pos\": 0.0,\n",
    "            \"mean_neg\": 0.0,\n",
    "        }\n",
    "\n",
    "    win = A > 0\n",
    "    return {\n",
    "        \"n\": int(A.size),\n",
    "        \"win_rate\": float(np.mean(win)),\n",
    "        \"mean\": float(np.mean(A)),\n",
    "        \"p50\": float(np.percentile(A, 50)),\n",
    "        \"p90\": float(np.percentile(A, 90)),\n",
    "        \"max\": float(np.max(A)),\n",
    "        \"min\": float(np.min(A)),\n",
    "        \"mean_pos\": float(np.mean(A[win])) if np.any(win) else 0.0,\n",
    "        \"mean_neg\": float(np.mean(A[~win])) if np.any(~win) else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "def local_coupling_curve(\n",
    "    env,\n",
    "    snapshot_source=\"random\",\n",
    "    base_seed=0,\n",
    "    snap_step=50,\n",
    "    K=50,\n",
    "    eps_list=(0.05, 0.10, 0.15, 0.30),\n",
    "    n_trials=256,\n",
    "    max_tries=20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a snapshot (from random or teacher trajectory), then measure advantage\n",
    "    distribution A = R_noisy - R_teacher for teacher+noise feedback policies.\n",
    "\n",
    "    Uses a consistent restore+truncate snapshot to compare apples-to-apples.\n",
    "    \"\"\"\n",
    "    ok, seed_used, snap = find_valid_snapshot(env, base_seed, snap_step, snapshot_source=snapshot_source, max_tries=max_tries)\n",
    "    if not ok:\n",
    "        print(f\"[FAIL] Could not find valid snapshot: base_seed={base_seed}, snap_step={snap_step}, source={snapshot_source}\")\n",
    "        return {\"ok\": False}\n",
    "\n",
    "    # teacher baseline (feedback)\n",
    "    R_teacher, comps_teacher, ev_teacher = rollout_feedback_teacher(env, snap, K)\n",
    "\n",
    "    A_by_eps = {}\n",
    "    for eps in eps_list:\n",
    "        A = []\n",
    "        for j in range(n_trials):\n",
    "            rng = np.random.RandomState(12345 + 100000 * int(100 * eps) + j)\n",
    "            R_noisy, comps_noisy, ev_noisy = rollout_feedback_teacher_plus_noise(env, snap, K, eps, rng)\n",
    "            A.append(R_noisy - R_teacher)\n",
    "        A_by_eps[float(eps)] = A\n",
    "\n",
    "    print(\"\\n=== LOCAL COUPLING CURVE ===\")\n",
    "    print(f\"source={snapshot_source} seed_used={seed_used} snap_step={snap_step} K={K}\")\n",
    "    print(f\"R_teacher={R_teacher:.6f} event_last={ev_teacher}\")\n",
    "\n",
    "    means = []\n",
    "    for eps in eps_list:\n",
    "        s = win_stats(A_by_eps[float(eps)])\n",
    "        means.append(s[\"mean\"])\n",
    "        print(\n",
    "            f\"eps={eps:0.2f} | win_rate={s['win_rate']:.3f} | mean={s['mean']:+.4f} | \"\n",
    "            f\"p50={s['p50']:+.4f} | p90={s['p90']:+.4f} | max={s['max']:+.4f} | min={s['min']:+.4f}\"\n",
    "        )\n",
    "\n",
    "    means = np.asarray(means, dtype=np.float64)\n",
    "    monotone_more_negative = bool(np.all(np.diff(means) <= 1e-6))\n",
    "    print(f\"monotone_more_negative_with_eps: {monotone_more_negative}\")\n",
    "\n",
    "    return {\n",
    "        \"ok\": True,\n",
    "        \"snapshot_source\": snapshot_source,\n",
    "        \"seed_used\": seed_used,\n",
    "        \"snap_step\": snap_step,\n",
    "        \"K\": K,\n",
    "        \"R_teacher\": float(R_teacher),\n",
    "        \"A_by_eps\": A_by_eps,\n",
    "    }\n",
    "\n",
    "\n",
    "def local_one_step_coupling_curve(\n",
    "    env,\n",
    "    snapshot_source=\"teacher\",   # \"teacher\" or \"random\"\n",
    "    base_seed=0,\n",
    "    snap_step=50,\n",
    "    n_trials=256,\n",
    "    eps_list=(0.05, 0.10, 0.15, 0.30),\n",
    "):\n",
    "    rng = np.random.RandomState(base_seed)\n",
    "\n",
    "    # Collect deltas: reward(noisy) - reward(base)\n",
    "    deltas = {eps: [] for eps in eps_list}\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        obs, _ = env.reset(seed=base_seed + i)\n",
    "\n",
    "        # Roll forward to snap_step using teacher or random\n",
    "        for _ in range(snap_step):\n",
    "            if snapshot_source == \"teacher\":\n",
    "                a = env.calculate_pronav()\n",
    "            else:\n",
    "                a = env.action_space.sample()\n",
    "            obs, r, term, trunc, info = env.step(a)\n",
    "            if term or trunc:\n",
    "                break\n",
    "\n",
    "        if env.done:\n",
    "            continue  # skip early-terminated episodes\n",
    "\n",
    "        snap = env._snapshot()\n",
    "\n",
    "        # Base action at snapshot\n",
    "        if snapshot_source == \"teacher\":\n",
    "            a_base = env.calculate_pronav()\n",
    "        else:\n",
    "            a_base = env.action_space.sample()\n",
    "\n",
    "        # Base one-step reward\n",
    "        env._restore(snap)\n",
    "        _, r_base, term, trunc, info_base = env.step(a_base)\n",
    "        if term or trunc:\n",
    "            continue\n",
    "\n",
    "        # Perturbed actions\n",
    "        for eps in eps_list:\n",
    "            noise = rng.normal(loc=0.0, scale=eps, size=(2,)).astype(np.float32)\n",
    "            a_noisy = np.clip(a_base + noise, -1.0, 1.0).astype(np.float32)\n",
    "\n",
    "            env._restore(snap)\n",
    "            _, r_noisy, term, trunc, info_noisy = env.step(a_noisy)\n",
    "            if term or trunc:\n",
    "                continue\n",
    "\n",
    "            deltas[eps].append(float(r_noisy - r_base))\n",
    "\n",
    "    # Print summary like your other curves\n",
    "    print(\"\\n=== ONE-STEP LOCAL COUPLING CURVE ===\")\n",
    "    print(f\"source={snapshot_source} snap_step={snap_step} n_trials={n_trials}\")\n",
    "    prev_mean = None\n",
    "    monotone = True\n",
    "\n",
    "    for eps in eps_list:\n",
    "        arr = np.array(deltas[eps], dtype=np.float64)\n",
    "        if len(arr) == 0:\n",
    "            print(f\"eps={eps:.2f} | no data\")\n",
    "            monotone = False\n",
    "            continue\n",
    "        mean = float(arr.mean())\n",
    "        p50 = float(np.median(arr))\n",
    "        p90 = float(np.percentile(arr, 90))\n",
    "        mx = float(arr.max())\n",
    "        mn = float(arr.min())\n",
    "        win_rate = float((arr < 0.0).mean())  # fraction noise makes reward worse\n",
    "\n",
    "        print(\n",
    "            f\"eps={eps:.2f} | win_rate={win_rate:.3f} | mean={mean:+.4f} | \"\n",
    "            f\"p50={p50:+.4f} | p90={p90:+.4f} | max={mx:+.4f} | min={mn:+.4f}\"\n",
    "        )\n",
    "\n",
    "        if prev_mean is not None and mean > prev_mean + 1e-6:\n",
    "            monotone = False\n",
    "        prev_mean = mean\n",
    "\n",
    "    print(f\"monotone_more_negative_with_eps: {monotone}\")\n",
    "    return deltas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def local_one_step_coupling_curve_teacher(\n",
    "    env,\n",
    "    base_seed=0,\n",
    "    snap_step=50,\n",
    "    n_trials=256,\n",
    "    eps_list=(0.05, 0.10, 0.15, 0.30),\n",
    "):\n",
    "    rng = np.random.RandomState(123)\n",
    "\n",
    "    deltas = {eps: [] for eps in eps_list}\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        obs, _ = env.reset(seed=base_seed + i)\n",
    "\n",
    "        # roll to snap_step using TEACHER (important)\n",
    "        for _ in range(snap_step):\n",
    "            a = env.calculate_pronav()\n",
    "            obs, r, term, trunc, info = env.step(a)\n",
    "            if term or trunc:\n",
    "                break\n",
    "        if env.done:\n",
    "            continue\n",
    "\n",
    "        snap = env._snapshot()\n",
    "\n",
    "        # base teacher action at snapshot\n",
    "        env._restore(snap)\n",
    "        a_base = normalize_action(env.calculate_pronav())\n",
    "\n",
    "        # base one-step reward\n",
    "        env._restore(snap)\n",
    "        _, r_base, term, trunc, _ = env.step(a_base)\n",
    "        if term or trunc:\n",
    "            continue\n",
    "\n",
    "        for eps in eps_list:\n",
    "            u = rand_unit_2(rng)              # unit direction\n",
    "            a_noisy = normalize_action(a_base + float(eps) * u)\n",
    "\n",
    "            env._restore(snap)\n",
    "            _, r_noisy, term, trunc, _ = env.step(a_noisy)\n",
    "            if term or trunc:\n",
    "                continue\n",
    "\n",
    "            deltas[eps].append(float(r_noisy - r_base))\n",
    "\n",
    "    print(\"\\n=== ONE-STEP COUPLING (teacher-centered) ===\")\n",
    "    prev_mean = None\n",
    "    monotone = True\n",
    "    for eps in eps_list:\n",
    "        arr = np.array(deltas[eps], dtype=np.float64)\n",
    "        if arr.size == 0:\n",
    "            print(f\"eps={eps:.2f} | no data\")\n",
    "            monotone = False\n",
    "            continue\n",
    "        mean = float(arr.mean())\n",
    "        p50 = float(np.median(arr))\n",
    "        p90 = float(np.percentile(arr, 90))\n",
    "        win_rate = float((arr < 0.0).mean())  # noise makes reward worse\n",
    "        print(f\"eps={eps:.2f} | win_rate={win_rate:.3f} | mean={mean:+.6f} | p50={p50:+.6f} | p90={p90:+.6f}\")\n",
    "\n",
    "        if prev_mean is not None and mean > prev_mean + 1e-9:\n",
    "            monotone = False\n",
    "        prev_mean = mean\n",
    "\n",
    "    print(f\"monotone_more_negative_with_eps: {monotone}\")\n",
    "    return deltas\n",
    "# -------------------------\n",
    "# Main entry: run the zoo\n",
    "# -------------------------\n",
    "\n",
    "def run_policy_zoo(student_model=None, n_episodes=50, seed0=0, print_every=10):\n",
    "    env = missile_interception_3d()\n",
    "\n",
    "    student = StudentPolicy(model=student_model, deterministic=True)\n",
    "\n",
    "    policies = {\n",
    "        \"teacher\": policy_teacher,\n",
    "        \"do_nothing\": policy_do_nothing,\n",
    "        \"random\": policy_random,\n",
    "        \"anti_teacher\": policy_anti_teacher,\n",
    "        \"student\": student.__call__,\n",
    "    }\n",
    "\n",
    "    print(\"Evaluating policy zoo...\")\n",
    "    results = evaluate_policy_zoo(env, policies, n_episodes=n_episodes, seed0=seed0, print_every=print_every)\n",
    "    print_policy_zoo_report(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_policy_zoo(student_model=None, n_episodes=50, seed0=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "local_action_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing local coupling (feedback policies) ...\n",
      "======================================================================\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=random | seed_used=0 | snap_step=20 | K=20\n",
      "R_teacher = 62.875733 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=-0.028105  p50=-0.017502  p90=+0.132971  max=+0.337522  min=-0.453564  n=192\n",
      "random        A=R-R_teacher: mean=-3.556433  p50=-3.677414  p90=-2.220628  max=-1.541327  min=-5.706475  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-3.733072  p50=-3.733072  p90=-3.733072  max=-3.733072  min=-3.733072  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-7.764379  p50=-7.764379  p90=-7.764379  max=-7.764379  min=-7.764379  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  teacher_noise_eps0.30_16      A=+0.337522  R=63.213255  Δprog=+0.3273  Δclose=+0.0143  ev=running\n",
      "  teacher_noise_eps0.30_49      A=+0.333820  R=63.209553  Δprog=+0.3223  Δclose=+0.0139  ev=running\n",
      "  teacher_noise_eps0.30_7       A=+0.333054  R=63.208787  Δprog=+0.3216  Δclose=+0.0134  ev=running\n",
      "  teacher_noise_eps0.30_63      A=+0.329928  R=63.205661  Δprog=+0.3252  Δclose=+0.0140  ev=running\n",
      "  teacher_noise_eps0.30_17      A=+0.265154  R=63.140887  Δprog=+0.2640  Δclose=+0.0111  ev=running\n",
      "  teacher_noise_eps0.30_50      A=+0.261601  R=63.137334  Δprog=+0.2550  Δclose=+0.0107  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_33                     A=-4.881443  R=57.994290  Δprog=-4.7356  Δclose=-0.2542  ev=running\n",
      "  random_30                     A=-5.078819  R=57.796915  Δprog=-4.9245  Δclose=-0.2684  ev=running\n",
      "  random_34                     A=-5.406784  R=57.468949  Δprog=-5.2544  Δclose=-0.2880  ev=running\n",
      "  random_54                     A=-5.605873  R=57.269860  Δprog=-5.4447  Δclose=-0.3024  ev=running\n",
      "  random_42                     A=-5.706475  R=57.169258  Δprog=-5.5358  Δclose=-0.3110  ev=running\n",
      "  anti_teacher                  A=-7.764379  R=55.111354  Δprog=-7.5357  Δclose=-0.4720  ev=running\n",
      "\n",
      "VERDICT: WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=random | seed_used=0 | snap_step=50 | K=20\n",
      "R_teacher = 62.786495 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=-0.045255  p50=-0.021885  p90=+0.110806  max=+0.252704  min=-0.536855  n=192\n",
      "random        A=R-R_teacher: mean=-3.898001  p50=-3.992649  p90=-2.434836  max=-1.788309  min=-6.149082  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-3.909122  p50=-3.909122  p90=-3.909122  max=-3.909122  min=-3.909122  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-8.551174  p50=-8.551174  p90=-8.551174  max=-8.551174  min=-8.551174  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  teacher_noise_eps0.30_49      A=+0.252704  R=63.039199  Δprog=+0.2421  Δclose=+0.0104  ev=running\n",
      "  teacher_noise_eps0.15_63      A=+0.219655  R=63.006151  Δprog=+0.2121  Δclose=+0.0091  ev=running\n",
      "  teacher_noise_eps0.30_63      A=+0.218829  R=63.005325  Δprog=+0.2155  Δclose=+0.0092  ev=running\n",
      "  teacher_noise_eps0.30_7       A=+0.195463  R=62.981959  Δprog=+0.1878  Δclose=+0.0076  ev=running\n",
      "  teacher_noise_eps0.30_17      A=+0.194484  R=62.980979  Δprog=+0.1942  Δclose=+0.0081  ev=running\n",
      "  teacher_noise_eps0.15_54      A=+0.186933  R=62.973428  Δprog=+0.1798  Δclose=+0.0077  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_33                     A=-5.534953  R=57.251543  Δprog=-5.3772  Δclose=-0.2969  ev=running\n",
      "  random_34                     A=-5.691407  R=57.095088  Δprog=-5.5386  Δclose=-0.3072  ev=running\n",
      "  random_30                     A=-5.822219  R=56.964276  Δprog=-5.6504  Δclose=-0.3185  ev=running\n",
      "  random_54                     A=-6.144373  R=56.642123  Δprog=-5.9731  Δclose=-0.3396  ev=running\n",
      "  random_42                     A=-6.149082  R=56.637414  Δprog=-5.9731  Δclose=-0.3418  ev=running\n",
      "  anti_teacher                  A=-8.551174  R=54.235322  Δprog=-8.3076  Δclose=-0.5392  ev=running\n",
      "\n",
      "VERDICT: WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=random | seed_used=0 | snap_step=80 | K=20\n",
      "R_teacher = 63.006848 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=-0.061167  p50=-0.025773  p90=+0.091957  max=+0.221000  min=-0.572622  n=192\n",
      "random        A=R-R_teacher: mean=-3.962743  p50=-4.057284  p90=-2.586918  max=-2.000072  min=-6.038383  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-4.050028  p50=-4.050028  p90=-4.050028  max=-4.050028  min=-4.050028  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-8.597479  p50=-8.597479  p90=-8.597479  max=-8.597479  min=-8.597479  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  teacher_noise_eps0.15_63      A=+0.221000  R=63.227847  Δprog=+0.2135  Δclose=+0.0090  ev=running\n",
      "  teacher_noise_eps0.15_54      A=+0.197861  R=63.204709  Δprog=+0.1905  Δclose=+0.0081  ev=running\n",
      "  teacher_noise_eps0.30_17      A=+0.191067  R=63.197914  Δprog=+0.1910  Δclose=+0.0080  ev=running\n",
      "  teacher_noise_eps0.30_7       A=+0.168542  R=63.175390  Δprog=+0.1624  Δclose=+0.0066  ev=running\n",
      "  teacher_noise_eps0.30_49      A=+0.164707  R=63.171554  Δprog=+0.1566  Δclose=+0.0066  ev=running\n",
      "  teacher_noise_eps0.15_14      A=+0.159761  R=63.166609  Δprog=+0.1574  Δclose=+0.0068  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_33                     A=-5.365246  R=57.641602  Δprog=-5.2084  Δclose=-0.2816  ev=running\n",
      "  random_34                     A=-5.483581  R=57.523267  Δprog=-5.3342  Δclose=-0.2891  ev=running\n",
      "  random_30                     A=-5.753949  R=57.252899  Δprog=-5.5802  Δclose=-0.3095  ev=running\n",
      "  random_42                     A=-6.032671  R=56.974177  Δprog=-5.8584  Δclose=-0.3286  ev=running\n",
      "  random_54                     A=-6.038383  R=56.968464  Δprog=-5.8670  Δclose=-0.3275  ev=running\n",
      "  anti_teacher                  A=-8.597479  R=54.409368  Δprog=-8.3498  Δclose=-0.5374  ev=running\n",
      "\n",
      "VERDICT: WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=random | seed_used=0 | snap_step=20 | K=50\n",
      "R_teacher = 166.301417 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=-0.035755  p50=+0.006063  p90=+0.477228  max=+1.532973  min=-2.094720  n=192\n",
      "random        A=R-R_teacher: mean=-19.460547  p50=-19.483458  p90=-14.825755  max=-11.872563  min=-28.784886  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-20.214328  p50=-20.214328  p90=-20.214328  max=-20.214328  min=-20.214328  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-50.412436  p50=-50.412436  p90=-50.412436  max=-50.412436  min=-50.412436  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  teacher_noise_eps0.30_16      A=+1.532973  R=167.834390  Δprog=+1.5128  Δclose=+0.0489  ev=running\n",
      "  teacher_noise_eps0.30_32      A=+1.208853  R=167.510270  Δprog=+1.1998  Δclose=+0.0365  ev=running\n",
      "  teacher_noise_eps0.30_7       A=+1.015320  R=167.316737  Δprog=+1.0025  Δclose=+0.0338  ev=running\n",
      "  teacher_noise_eps0.30_55      A=+0.929877  R=167.231294  Δprog=+0.9297  Δclose=+0.0312  ev=running\n",
      "  teacher_noise_eps0.15_54      A=+0.853593  R=167.155010  Δprog=+0.8463  Δclose=+0.0288  ev=running\n",
      "  teacher_noise_eps0.30_10      A=+0.853357  R=167.154775  Δprog=+0.8342  Δclose=+0.0225  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_50                     A=-25.740370  R=140.561047  Δprog=-25.4130  Δclose=-1.2772  ev=running\n",
      "  random_17                     A=-26.261517  R=140.039900  Δprog=-25.9167  Δclose=-1.3183  ev=running\n",
      "  random_54                     A=-26.925272  R=139.376146  Δprog=-26.5830  Δclose=-1.3472  ev=running\n",
      "  random_30                     A=-28.119048  R=138.182370  Δprog=-27.7573  Δclose=-1.4504  ev=running\n",
      "  random_34                     A=-28.784886  R=137.516531  Δprog=-28.4177  Δclose=-1.5052  ev=running\n",
      "  anti_teacher                  A=-50.412436  R=115.888981  Δprog=-49.6832  Δclose=-4.2751  ev=running\n",
      "\n",
      "VERDICT: WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=random | seed_used=0 | snap_step=50 | K=50\n",
      "R_teacher = 166.606000 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=-0.077939  p50=-0.001636  p90=+0.424890  max=+1.192153  min=-2.345997  n=192\n",
      "random        A=R-R_teacher: mean=-21.074601  p50=-20.678168  p90=-15.759732  max=-13.246432  min=-31.335691  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-21.445591  p50=-21.445591  p90=-21.445591  max=-21.445591  min=-21.445591  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-54.068624  p50=-54.068624  p90=-54.068624  max=-54.068624  min=-54.068624  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  teacher_noise_eps0.30_16      A=+1.192153  R=167.798152  Δprog=+1.1754  Δclose=+0.0364  ev=running\n",
      "  teacher_noise_eps0.15_18      A=+0.858245  R=167.464245  Δprog=+0.8475  Δclose=+0.0270  ev=running\n",
      "  teacher_noise_eps0.30_32      A=+0.811923  R=167.417923  Δprog=+0.8045  Δclose=+0.0219  ev=running\n",
      "  teacher_noise_eps0.15_54      A=+0.726260  R=167.332260  Δprog=+0.7205  Δclose=+0.0239  ev=running\n",
      "  teacher_noise_eps0.15_63      A=+0.716005  R=167.322005  Δprog=+0.7112  Δclose=+0.0236  ev=running\n",
      "  teacher_noise_eps0.30_7       A=+0.703105  R=167.309105  Δprog=+0.6919  Δclose=+0.0225  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_50                     A=-27.464774  R=139.141226  Δprog=-27.1145  Δclose=-1.3869  ev=running\n",
      "  random_17                     A=-28.279145  R=138.326855  Δprog=-27.9079  Δclose=-1.4537  ev=running\n",
      "  random_54                     A=-28.916436  R=137.689564  Δprog=-28.5538  Δclose=-1.4768  ev=running\n",
      "  random_34                     A=-30.145579  R=136.460420  Δprog=-29.7613  Δclose=-1.5951  ev=running\n",
      "  random_30                     A=-31.335691  R=135.270309  Δprog=-30.9344  Δclose=-1.6882  ev=running\n",
      "  anti_teacher                  A=-54.068624  R=112.537376  Δprog=-53.2781  Δclose=-4.8428  ev=running\n",
      "\n",
      "VERDICT: WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=random | seed_used=0 | snap_step=80 | K=50\n",
      "R_teacher = 167.484934 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=-0.120229  p50=-0.013888  p90=+0.341638  max=+1.084888  min=-2.428788  n=192\n",
      "random        A=R-R_teacher: mean=-21.813060  p50=-21.652832  p90=-16.792113  max=-14.042924  min=-31.849778  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-22.412249  p50=-22.412249  p90=-22.412249  max=-22.412249  min=-22.412249  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-55.418449  p50=-55.418449  p90=-55.418449  max=-55.418449  min=-55.418449  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  teacher_noise_eps0.30_16      A=+1.084888  R=168.569821  Δprog=+1.0692  Δclose=+0.0324  ev=running\n",
      "  teacher_noise_eps0.15_18      A=+0.766355  R=168.251289  Δprog=+0.7562  Δclose=+0.0230  ev=running\n",
      "  teacher_noise_eps0.15_54      A=+0.738030  R=168.222964  Δprog=+0.7325  Δclose=+0.0238  ev=running\n",
      "  teacher_noise_eps0.15_63      A=+0.717868  R=168.202802  Δprog=+0.7130  Δclose=+0.0232  ev=running\n",
      "  teacher_noise_eps0.30_32      A=+0.684663  R=168.169597  Δprog=+0.6780  Δclose=+0.0175  ev=running\n",
      "  teacher_noise_eps0.30_10      A=+0.641968  R=168.126902  Δprog=+0.6245  Δclose=+0.0140  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_10                     A=-28.518052  R=138.966882  Δprog=-28.1323  Δclose=-1.4385  ev=running\n",
      "  random_54                     A=-29.127929  R=138.357005  Δprog=-28.7611  Δclose=-1.4581  ev=running\n",
      "  random_17                     A=-29.495924  R=137.989010  Δprog=-29.1088  Δclose=-1.5122  ev=running\n",
      "  random_34                     A=-30.099607  R=137.385327  Δprog=-29.7126  Δclose=-1.5568  ev=running\n",
      "  random_30                     A=-31.849778  R=135.635156  Δprog=-31.4391  Δclose=-1.6934  ev=running\n",
      "  anti_teacher                  A=-55.418449  R=112.066485  Δprog=-54.6048  Δclose=-5.0095  ev=running\n",
      "\n",
      "VERDICT: WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=teacher | seed_used=0 | snap_step=20 | K=20\n",
      "R_teacher = 66.717103 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=-0.003419  p50=-0.005983  p90=+0.152273  max=+0.408399  min=-0.375709  n=192\n",
      "random        A=R-R_teacher: mean=-2.519966  p50=-2.569624  p90=-1.332599  max=-0.651289  min=-4.522093  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-2.675661  p50=-2.675661  p90=-2.675661  max=-2.675661  min=-2.675661  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-5.902229  p50=-5.902229  p90=-5.902229  max=-5.902229  min=-5.902229  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  teacher_noise_eps0.30_7       A=+0.408399  R=67.125503  Δprog=+0.3941  Δclose=+0.0132  ev=running\n",
      "  teacher_noise_eps0.30_63      A=+0.363494  R=67.080597  Δprog=+0.3590  Δclose=+0.0122  ev=running\n",
      "  teacher_noise_eps0.30_16      A=+0.356375  R=67.073478  Δprog=+0.3459  Δclose=+0.0119  ev=running\n",
      "  teacher_noise_eps0.30_49      A=+0.343602  R=67.060705  Δprog=+0.3323  Δclose=+0.0114  ev=running\n",
      "  teacher_noise_eps0.30_55      A=+0.325079  R=67.042183  Δprog=+0.3076  Δclose=+0.0107  ev=running\n",
      "  teacher_noise_eps0.30_17      A=+0.320261  R=67.037365  Δprog=+0.3183  Δclose=+0.0107  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_33                     A=-3.639510  R=63.077593  Δprog=-3.5148  Δclose=-0.1423  ev=running\n",
      "  random_30                     A=-3.747325  R=62.969778  Δprog=-3.6192  Δclose=-0.1480  ev=running\n",
      "  random_54                     A=-4.357224  R=62.359879  Δprog=-4.2165  Δclose=-0.1765  ev=running\n",
      "  random_34                     A=-4.358275  R=62.358828  Δprog=-4.2181  Δclose=-0.1758  ev=running\n",
      "  random_42                     A=-4.522093  R=62.195010  Δprog=-4.3664  Δclose=-0.1857  ev=running\n",
      "  anti_teacher                  A=-5.902229  R=60.814875  Δprog=-5.7007  Δclose=-0.2615  ev=running\n",
      "\n",
      "VERDICT: WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=teacher | seed_used=0 | snap_step=50 | K=20\n",
      "R_teacher = 71.500468 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=-0.004810  p50=-0.005150  p90=+0.115799  max=+0.328036  min=-0.305793  n=192\n",
      "random        A=R-R_teacher: mean=-1.263748  p50=-1.301320  p90=-0.283548  max=+0.320247  min=-2.949226  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-1.344021  p50=-1.344021  p90=-1.344021  max=-1.344021  min=-1.344021  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-3.148000  p50=-3.148000  p90=-3.148000  max=-3.148000  min=-3.148000  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  teacher_noise_eps0.30_7       A=+0.328036  R=71.828503  Δprog=+0.3135  Δclose=+0.0080  ev=running\n",
      "  random_38                     A=+0.320247  R=71.820715  Δprog=+0.3177  Δclose=+0.0078  ev=running\n",
      "  teacher_noise_eps0.30_63      A=+0.290101  R=71.790568  Δprog=+0.2863  Δclose=+0.0072  ev=running\n",
      "  teacher_noise_eps0.30_16      A=+0.283339  R=71.783807  Δprog=+0.2731  Δclose=+0.0070  ev=running\n",
      "  teacher_noise_eps0.30_49      A=+0.278220  R=71.778688  Δprog=+0.2670  Δclose=+0.0069  ev=running\n",
      "  teacher_noise_eps0.30_55      A=+0.261298  R=71.761765  Δprog=+0.2431  Δclose=+0.0064  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_33                     A=-2.284662  R=69.215805  Δprog=-2.1878  Δclose=-0.0625  ev=running\n",
      "  random_30                     A=-2.391812  R=69.108656  Δprog=-2.2908  Δclose=-0.0662  ev=running\n",
      "  random_54                     A=-2.837473  R=68.662995  Δprog=-2.7251  Δclose=-0.0799  ev=running\n",
      "  random_34                     A=-2.838021  R=68.662447  Δprog=-2.7248  Δclose=-0.0795  ev=running\n",
      "  random_42                     A=-2.949226  R=68.551241  Δprog=-2.8202  Δclose=-0.0839  ev=running\n",
      "  anti_teacher                  A=-3.148000  R=68.352468  Δprog=-3.0060  Δclose=-0.0908  ev=running\n",
      "\n",
      "VERDICT: WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=teacher | seed_used=0 | snap_step=80 | K=20\n",
      "R_teacher = 73.768495 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=-0.005959  p50=-0.004711  p90=+0.097942  max=+0.283189  min=-0.270974  n=192\n",
      "random        A=R-R_teacher: mean=-0.710642  p50=-0.719852  p90=+0.140235  max=+0.666064  min=-2.189414  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-0.745590  p50=-0.745590  p90=-0.745590  max=-0.745590  min=-0.745590  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-1.747963  p50=-1.747963  p90=-1.747963  max=-1.747963  min=-1.747963  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  random_38                     A=+0.666064  R=74.434559  Δprog=+0.6414  Δclose=+0.0139  ev=running\n",
      "  random_29                     A=+0.357266  R=74.125762  Δprog=+0.3641  Δclose=+0.0079  ev=running\n",
      "  teacher_noise_eps0.30_7       A=+0.283189  R=74.051684  Δprog=+0.2680  Δclose=+0.0059  ev=running\n",
      "  random_52                     A=+0.269377  R=74.037873  Δprog=+0.2524  Δclose=+0.0057  ev=running\n",
      "  teacher_noise_eps0.30_63      A=+0.252196  R=74.020691  Δprog=+0.2482  Δclose=+0.0055  ev=running\n",
      "  teacher_noise_eps0.30_16      A=+0.245272  R=74.013767  Δprog=+0.2348  Δclose=+0.0053  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_33                     A=-1.622477  R=72.146018  Δprog=-1.5419  Δclose=-0.0373  ev=running\n",
      "  random_30                     A=-1.738303  R=72.030193  Δprog=-1.6523  Δclose=-0.0404  ev=running\n",
      "  anti_teacher                  A=-1.747963  R=72.020532  Δprog=-1.6511  Δclose=-0.0406  ev=running\n",
      "  random_54                     A=-2.096967  R=71.671528  Δprog=-2.0004  Δclose=-0.0494  ev=running\n",
      "  random_34                     A=-2.097191  R=71.671304  Δprog=-1.9982  Δclose=-0.0490  ev=running\n",
      "  random_42                     A=-2.189414  R=71.579082  Δprog=-2.0750  Δclose=-0.0519  ev=running\n",
      "\n",
      "VERDICT: WARNING: random p90 advantage > 0 (many random beat teacher locally)\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=teacher | seed_used=0 | snap_step=20 | K=50\n",
      "R_teacher = 173.028802 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=+0.011123  p50=+0.023279  p90=+0.493396  max=+1.456363  min=-1.714778  n=192\n",
      "random        A=R-R_teacher: mean=-13.688982  p50=-14.020294  p90=-9.455149  max=-6.720913  min=-22.674293  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-14.305496  p50=-14.305496  p90=-14.305496  max=-14.305496  min=-14.305496  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-41.089009  p50=-41.089009  p90=-41.089009  max=-41.089009  min=-41.089009  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  teacher_noise_eps0.30_16      A=+1.456363  R=174.485166  Δprog=+1.4360  Δclose=+0.0401  ev=running\n",
      "  teacher_noise_eps0.30_32      A=+1.309804  R=174.338606  Δprog=+1.3013  Δclose=+0.0355  ev=running\n",
      "  teacher_noise_eps0.30_7       A=+1.117933  R=174.146735  Δprog=+1.1054  Δclose=+0.0319  ev=running\n",
      "  teacher_noise_eps0.30_55      A=+1.021255  R=174.050058  Δprog=+1.0196  Δclose=+0.0293  ev=running\n",
      "  teacher_noise_eps0.30_10      A=+0.836953  R=173.865755  Δprog=+0.8179  Δclose=+0.0208  ev=running\n",
      "  teacher_noise_eps0.15_54      A=+0.778526  R=173.807329  Δprog=+0.7713  Δclose=+0.0223  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_17                     A=-19.392984  R=153.635818  Δprog=-19.1322  Δclose=-0.7476  ev=running\n",
      "  random_42                     A=-19.710116  R=153.318686  Δprog=-19.4816  Δclose=-0.7531  ev=running\n",
      "  random_54                     A=-20.533888  R=152.494914  Δprog=-20.2666  Δclose=-0.7965  ev=running\n",
      "  random_30                     A=-20.765986  R=152.262816  Δprog=-20.4954  Δclose=-0.8154  ev=running\n",
      "  random_34                     A=-22.674293  R=150.354509  Δprog=-22.3851  Δclose=-0.9216  ev=running\n",
      "  anti_teacher                  A=-41.089009  R=131.939794  Δprog=-40.5196  Δclose=-2.5865  ev=running\n",
      "\n",
      "VERDICT: WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=teacher | seed_used=0 | snap_step=50 | K=50\n",
      "R_teacher = 181.691867 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=-0.000600  p50=+0.012982  p90=+0.398993  max=+1.213005  min=-1.494278  n=192\n",
      "random        A=R-R_teacher: mean=-6.874340  p50=-6.886303  p90=-3.302801  max=-1.062970  min=-14.494658  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-7.232228  p50=-7.232228  p90=-7.232228  max=-7.232228  min=-7.232228  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-25.284889  p50=-25.284889  p90=-25.284889  max=-25.284889  min=-25.284889  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  teacher_noise_eps0.30_16      A=+1.213005  R=182.904872  Δprog=+1.1925  Δclose=+0.0272  ev=running\n",
      "  teacher_noise_eps0.30_32      A=+1.098714  R=182.790580  Δprog=+1.0905  Δclose=+0.0246  ev=running\n",
      "  teacher_noise_eps0.30_7       A=+0.914956  R=182.606823  Δprog=+0.9030  Δclose=+0.0210  ev=running\n",
      "  teacher_noise_eps0.30_55      A=+0.835608  R=182.527475  Δprog=+0.8349  Δclose=+0.0193  ev=running\n",
      "  teacher_noise_eps0.30_10      A=+0.714159  R=182.406026  Δprog=+0.6942  Δclose=+0.0152  ev=running\n",
      "  teacher_noise_eps0.15_54      A=+0.646399  R=182.338266  Δprog=+0.6395  Δclose=+0.0149  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_17                     A=-11.526523  R=170.165343  Δprog=-11.3466  Δclose=-0.3208  ev=running\n",
      "  random_42                     A=-11.892883  R=169.798984  Δprog=-11.7490  Δclose=-0.3285  ev=running\n",
      "  random_54                     A=-12.672822  R=169.019045  Δprog=-12.4864  Δclose=-0.3546  ev=running\n",
      "  random_30                     A=-13.092630  R=168.599237  Δprog=-12.9016  Δclose=-0.3719  ev=running\n",
      "  random_34                     A=-14.494658  R=167.197208  Δprog=-14.2898  Δclose=-0.4223  ev=running\n",
      "  anti_teacher                  A=-25.284889  R=156.406978  Δprog=-24.8807  Δclose=-0.9884  ev=running\n",
      "\n",
      "VERDICT: WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "=== LOCAL ACTION-REWARD SANITY v3 (feedback + advantage dist) ===\n",
      "snapshot_source=teacher | seed_used=0 | snap_step=80 | K=50\n",
      "R_teacher = 186.028458 | event_last=running\n",
      "------------------------------------------------------------------------------\n",
      "teacher       A=R-R_teacher: mean=+0.000000  p50=+0.000000  p90=+0.000000  max=+0.000000  min=+0.000000  n=1\n",
      "teacher+noise  A=R-R_teacher: mean=-0.007314  p50=+0.009485  p90=+0.364038  max=+1.109651  min=-1.396408  n=192\n",
      "random        A=R-R_teacher: mean=-3.972777  p50=-3.948779  p90=-0.924601  max=+1.031225  min=-10.667293  n=64\n",
      "do_nothing    A=R-R_teacher: mean=-4.176088  p50=-4.176088  p90=-4.176088  max=-4.176088  min=-4.176088  n=1\n",
      "anti_teacher  A=R-R_teacher: mean=-14.980781  p50=-14.980781  p90=-14.980781  max=-14.980781  min=-14.980781  n=1\n",
      "\n",
      "Top candidates by advantage (beating teacher):\n",
      "  teacher_noise_eps0.30_16      A=+1.109651  R=187.138110  Δprog=+1.0871  Δclose=+0.0224  ev=running\n",
      "  random_38                     A=+1.031225  R=187.059683  Δprog=+1.0345  Δclose=+0.0215  ev=running\n",
      "  teacher_noise_eps0.30_32      A=+1.006318  R=187.034777  Δprog=+0.9964  Δclose=+0.0203  ev=running\n",
      "  teacher_noise_eps0.30_7       A=+0.832047  R=186.860505  Δprog=+0.8191  Δclose=+0.0171  ev=running\n",
      "  teacher_noise_eps0.30_55      A=+0.760690  R=186.789149  Δprog=+0.7595  Δclose=+0.0157  ev=running\n",
      "  teacher_noise_eps0.30_10      A=+0.647911  R=186.676369  Δprog=+0.6262  Δclose=+0.0127  ev=running\n",
      "\n",
      "Bottom candidates by advantage (worst):\n",
      "  random_17                     A=-7.972482  R=178.055976  Δprog=-7.8270  Δclose=-0.1890  ev=running\n",
      "  random_42                     A=-8.342828  R=177.685630  Δprog=-8.2344  Δclose=-0.1964  ev=running\n",
      "  random_54                     A=-9.048864  R=176.979594  Δprog=-8.8968  Δclose=-0.2153  ev=running\n",
      "  random_30                     A=-9.553210  R=176.475248  Δprog=-9.3952  Δclose=-0.2311  ev=running\n",
      "  random_34                     A=-10.667293  R=175.361165  Δprog=-10.4974  Δclose=-0.2633  ev=running\n",
      "  anti_teacher                  A=-14.980781  R=171.047677  Δprog=-14.6725  Δclose=-0.4267  ev=running\n",
      "\n",
      "VERDICT: WARNING: teacher+noise p90 advantage > 0 (basin very flat / teacher not locally optimal for this shaping)\n",
      "==============================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# LOCAL ACTION-REWARD COUPLING TEST (Step 2 v3)\n",
    "# ==========================================\n",
    "# This tests whether the reward provides good local gradients\n",
    "# around the teacher action, which is critical for PPO learning.\n",
    "# Uses feedback policies (recomputes action each step) and reports\n",
    "# advantage distributions vs teacher.\n",
    "\n",
    "env = missile_interception_3d()\n",
    "\n",
    "print(\"Testing local coupling (feedback policies) ...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for snapshot_source in [\"random\", \"teacher\"]:\n",
    "    for K in [20, 50]:\n",
    "        for snap_step in [20, 50, 80]:\n",
    "            out = local_action_reward_sanity_v3(\n",
    "                env,\n",
    "                base_seed=0,\n",
    "                snap_step=snap_step,\n",
    "                K=K,\n",
    "                snapshot_source=snapshot_source,\n",
    "                eps_list=(0.05, 0.15, 0.30),\n",
    "                n_noisy=64,\n",
    "                n_random=64,\n",
    "                print_top_bottom=6,\n",
    "            )\n",
    "            if not out.get(\"ok\", False):\n",
    "                print(f\"\\nWARNING: Test failed for snapshot_source={snapshot_source}, K={K}, snap_step={snap_step}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44738f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==========================================\n",
    "# # LOCAL COUPLING CURVE (advantage vs epsilon)\n",
    "# # ==========================================\n",
    "# # This is the \"PPO usability\" test: does expected return drop as you add noise?\n",
    "\n",
    "# env = missile_interception_3d()\n",
    "\n",
    "# for source in [\"random\", \"teacher\"]:\n",
    "#     for snap_step in [20, 50, 80]:\n",
    "#         _ = local_coupling_curve(\n",
    "#             env,\n",
    "#             snapshot_source=source,\n",
    "#             base_seed=0,\n",
    "#             snap_step=snap_step,\n",
    "#             K=50,\n",
    "#             eps_list=(0.05, 0.10, 0.15, 0.30),\n",
    "#             n_trials=256,\n",
    "#             max_tries=20,\n",
    "#         )\n",
    "#         print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee3f78cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ONE-STEP LOCAL COUPLING CURVE ===\n",
      "source=random snap_step=20 n_trials=256\n",
      "eps=0.05 | win_rate=0.469 | mean=+0.0000 | p50=+0.0000 | p90=+0.0002 | max=+0.0006 | min=-0.0005\n",
      "eps=0.10 | win_rate=0.512 | mean=-0.0001 | p50=-0.0000 | p90=+0.0003 | max=+0.0011 | min=-0.0016\n",
      "eps=0.15 | win_rate=0.496 | mean=+0.0000 | p50=+0.0000 | p90=+0.0006 | max=+0.0038 | min=-0.0028\n",
      "eps=0.30 | win_rate=0.465 | mean=+0.0001 | p50=+0.0000 | p90=+0.0014 | max=+0.0032 | min=-0.0049\n",
      "monotone_more_negative_with_eps: False\n",
      "\n",
      "\n",
      "=== ONE-STEP LOCAL COUPLING CURVE ===\n",
      "source=random snap_step=50 n_trials=256\n",
      "eps=0.05 | win_rate=0.488 | mean=+0.0000 | p50=+0.0000 | p90=+0.0002 | max=+0.0008 | min=-0.0009\n",
      "eps=0.10 | win_rate=0.516 | mean=-0.0000 | p50=-0.0000 | p90=+0.0003 | max=+0.0017 | min=-0.0012\n",
      "eps=0.15 | win_rate=0.492 | mean=+0.0000 | p50=+0.0000 | p90=+0.0005 | max=+0.0026 | min=-0.0021\n",
      "eps=0.30 | win_rate=0.449 | mean=+0.0000 | p50=+0.0000 | p90=+0.0010 | max=+0.0053 | min=-0.0044\n",
      "monotone_more_negative_with_eps: False\n",
      "\n",
      "\n",
      "=== ONE-STEP LOCAL COUPLING CURVE ===\n",
      "source=random snap_step=80 n_trials=256\n",
      "eps=0.05 | win_rate=0.438 | mean=-0.0000 | p50=+0.0000 | p90=+0.0002 | max=+0.0007 | min=-0.0011\n",
      "eps=0.10 | win_rate=0.512 | mean=-0.0000 | p50=-0.0000 | p90=+0.0003 | max=+0.0014 | min=-0.0017\n",
      "eps=0.15 | win_rate=0.488 | mean=+0.0000 | p50=+0.0000 | p90=+0.0006 | max=+0.0021 | min=-0.0026\n",
      "eps=0.30 | win_rate=0.527 | mean=+0.0000 | p50=-0.0000 | p90=+0.0011 | max=+0.0035 | min=-0.0035\n",
      "monotone_more_negative_with_eps: False\n",
      "\n",
      "\n",
      "=== ONE-STEP LOCAL COUPLING CURVE ===\n",
      "source=teacher snap_step=20 n_trials=256\n",
      "eps=0.05 | win_rate=0.473 | mean=+0.0000 | p50=+0.0000 | p90=+0.0008 | max=+0.0012 | min=-0.0013\n",
      "eps=0.10 | win_rate=0.602 | mean=-0.0001 | p50=-0.0001 | p90=+0.0011 | max=+0.0024 | min=-0.0023\n",
      "eps=0.15 | win_rate=0.504 | mean=+0.0001 | p50=-0.0000 | p90=+0.0018 | max=+0.0025 | min=-0.0027\n",
      "eps=0.30 | win_rate=0.539 | mean=-0.0001 | p50=-0.0002 | p90=+0.0023 | max=+0.0031 | min=-0.0034\n",
      "monotone_more_negative_with_eps: False\n",
      "\n",
      "\n",
      "=== ONE-STEP LOCAL COUPLING CURVE ===\n",
      "source=teacher snap_step=50 n_trials=256\n",
      "eps=0.05 | win_rate=0.469 | mean=+0.0000 | p50=+0.0000 | p90=+0.0008 | max=+0.0012 | min=-0.0012\n",
      "eps=0.10 | win_rate=0.578 | mean=-0.0001 | p50=-0.0001 | p90=+0.0011 | max=+0.0023 | min=-0.0022\n",
      "eps=0.15 | win_rate=0.492 | mean=+0.0001 | p50=+0.0000 | p90=+0.0018 | max=+0.0030 | min=-0.0026\n",
      "eps=0.30 | win_rate=0.484 | mean=+0.0001 | p50=+0.0000 | p90=+0.0026 | max=+0.0032 | min=-0.0031\n",
      "monotone_more_negative_with_eps: False\n",
      "\n",
      "\n",
      "=== ONE-STEP LOCAL COUPLING CURVE ===\n",
      "source=teacher snap_step=80 n_trials=256\n",
      "eps=0.05 | win_rate=0.477 | mean=+0.0000 | p50=+0.0000 | p90=+0.0007 | max=+0.0011 | min=-0.0011\n",
      "eps=0.10 | win_rate=0.570 | mean=-0.0001 | p50=-0.0001 | p90=+0.0010 | max=+0.0022 | min=-0.0022\n",
      "eps=0.15 | win_rate=0.496 | mean=+0.0001 | p50=+0.0001 | p90=+0.0016 | max=+0.0028 | min=-0.0025\n",
      "eps=0.30 | win_rate=0.496 | mean=+0.0001 | p50=+0.0000 | p90=+0.0025 | max=+0.0031 | min=-0.0030\n",
      "monotone_more_negative_with_eps: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = missile_interception_3d()\n",
    "\n",
    "for source in [\"random\", \"teacher\"]:\n",
    "    for snap_step in [20, 50, 80]:\n",
    "        _ = local_one_step_coupling_curve(\n",
    "            env,\n",
    "            snapshot_source=source,\n",
    "            base_seed=0,\n",
    "            snap_step=snap_step,\n",
    "            n_trials=256,\n",
    "            eps_list=(0.05, 0.10, 0.15, 0.30),\n",
    "        )\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
