{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e2154083",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "class missile_interception_3d(gym.Env):\n",
        "    def __init__(self):\n",
        "        # 1. Define Action Space (The Joystick: Left/Right, Up/Down)\n",
        "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2, ), dtype=np.float32)\n",
        "        \n",
        "        # 2. Define Observation Space (16D ego-frame, no actuator state)\n",
        "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(16,), dtype=np.float32)\n",
        "\n",
        "        self.np_random = np.random.RandomState()\n",
        "        \n",
        "        # 3. Time Settings\n",
        "        self.dt_act = 0.1             \n",
        "        self.n_substeps = 10          \n",
        "        self.dt_sim = self.dt_act / self.n_substeps \n",
        "        self.t_max = 650.0 # REVISAR IF THIS IS ENOF         \n",
        "\n",
        "        # 4. Physical Limits\n",
        "        self.a_max = 350.0   # Max G-force (m/s^2) ~35G\n",
        "        self.g = 9.81        \n",
        "        self.collision_radius = 150.0  \n",
        "        self.max_distance = 4_000_000.0 \n",
        "\n",
        "        # LET'S REMOVE THE \"HARD CASES\"\n",
        "\n",
        "        self.p_easy = 1.0                   \n",
        "        self.range_min = 70_000.0           \n",
        "        self.range_easy_max = 200_000.0     \n",
        "        self.range_hard_max = 1_000_000.0   \n",
        "\n",
        "        self.targetbox_x_min = -15000\n",
        "        self.targetbox_x_max = 15000\n",
        "        self.targetbox_y_min = -15000\n",
        "        self.targetbox_y_max = 15000\n",
        "\n",
        "        # --- Closest-approach shaping ---\n",
        "        self.gamma_shape = 0.9999      # set = PPO gamma when training\n",
        "        self.w_ca = 1.0\n",
        "        self.dstar_scale = 50_000.0   # meters; tune so r_ca ~ 0.05-0.15 under ProNav\n",
        "\n",
        "    def generate_enemy_missile(self):\n",
        "        if self.np_random.rand() < self.p_easy:\n",
        "            self.range_max_used = self.range_easy_max\n",
        "        else:\n",
        "            self.range_max_used = self.range_hard_max\n",
        "\n",
        "        range_min = self.range_min\n",
        "        self.attack_target_x = self.np_random.uniform(self.targetbox_x_min, self.targetbox_x_max)\n",
        "        self.attack_target_y = self.np_random.uniform(self.targetbox_y_min, self.targetbox_y_max)\n",
        "        self.enemy_launch_angle = self.np_random.uniform(0, 2 * np.pi)\n",
        "        self.enemy_theta = self.np_random.uniform(0.523599, 1.0472) \n",
        "\n",
        "        self.range_max_used = max(self.range_max_used, range_min + 1.0)\n",
        "        lower_limit = np.sqrt((range_min * self.g) / np.sin(2 * self.enemy_theta))\n",
        "        upper_limit = np.sqrt((self.range_max_used * self.g) / np.sin(2 * self.enemy_theta))\n",
        "        self.enemy_initial_velocity = self.np_random.uniform(lower_limit, upper_limit)\n",
        "\n",
        "        ground_range = (\n",
        "            self.enemy_initial_velocity * np.cos(self.enemy_theta)\n",
        "            * (2 * self.enemy_initial_velocity * np.sin(self.enemy_theta) / self.g)\n",
        "        )\n",
        "\n",
        "        self.enemy_launch_x = self.attack_target_x + ground_range * np.cos(self.enemy_launch_angle)\n",
        "        self.enemy_launch_y = self.attack_target_y + ground_range * np.sin(self.enemy_launch_angle)\n",
        "        self.enemy_z = 0\n",
        "        self.enemy_x = self.enemy_launch_x\n",
        "        self.enemy_y = self.enemy_launch_y\n",
        "        self.enemy_pos = np.array([self.enemy_x, self.enemy_y, self.enemy_z], dtype=np.float32)\n",
        "        self.enemy_azimuth = (self.enemy_launch_angle + np.pi) % (2 * np.pi)\n",
        "\n",
        "    def generate_defense_missile(self):\n",
        "        self.defense_launch_x = self.np_random.uniform(self.targetbox_x_min, self.targetbox_x_max)\n",
        "        self.defense_launch_y = self.np_random.uniform(self.targetbox_y_min, self.targetbox_y_max)\n",
        "\n",
        "        dx = self.enemy_launch_x - self.defense_launch_x\n",
        "        dy = self.enemy_launch_y - self.defense_launch_y\n",
        "        az_nominal = np.arctan2(dy, dx)\n",
        "\n",
        "        # --- Misalignment (domain randomization of initial heading) ---\n",
        "        # Mixture: most episodes small error, some episodes large az error\n",
        "        p_misaligned = 0.35  # 35% \"hard\" starts\n",
        "        if self.np_random.rand() < p_misaligned:\n",
        "            # Hard: big azimuth error → strong RIGHT required\n",
        "            az_noise = self.np_random.uniform(-np.deg2rad(60.0), np.deg2rad(60.0))\n",
        "        else:\n",
        "            # Easy: small azimuth error → gentle correction\n",
        "            az_noise = self.np_random.uniform(-np.deg2rad(10.0), np.deg2rad(10.0))\n",
        "\n",
        "        self.defense_azimuth = az_nominal + az_noise\n",
        "\n",
        "        # Elevation noise: avoid always same vertical plane\n",
        "        theta_nominal = 0.785398  # ~45 deg\n",
        "        theta_noise_deg = 10.0\n",
        "        theta_noise = self.np_random.uniform(\n",
        "            -np.deg2rad(theta_noise_deg),\n",
        "            +np.deg2rad(theta_noise_deg),\n",
        "        )\n",
        "        self.defense_theta = float(np.clip(theta_nominal + theta_noise,\n",
        "                                           np.deg2rad(10.0),\n",
        "                                           np.deg2rad(80.0)))\n",
        "        # ---------------------------------------------------------------\n",
        "\n",
        "        base_velocity = 3000.0\n",
        "        if hasattr(self, 'range_max_used'):\n",
        "            velocity_scale = min(self.range_max_used / self.range_easy_max, 1.5)\n",
        "            self.defense_initial_velocity = base_velocity * velocity_scale\n",
        "        else:\n",
        "            self.defense_initial_velocity = base_velocity\n",
        "\n",
        "        self.defense_x = self.defense_launch_x\n",
        "        self.defense_y = self.defense_launch_y\n",
        "        self.defense_z = 0.0\n",
        "        self.defense_pos = np.array([self.defense_x, self.defense_y, self.defense_z], dtype=np.float32)\n",
        "    \n",
        "    def _smoothstep(self, x: float) -> float:\n",
        "        \"\"\"Smooth ramp 0->1 with zero slope at ends, clamps outside [0,1]\"\"\"\n",
        "        x = float(np.clip(x, 0.0, 1.0))\n",
        "        return x * x * (3.0 - 2.0 * x)\n",
        "    \n",
        "    def calculate_pronav(self):\n",
        "        eps = 1e-9\n",
        "\n",
        "        # Relative geometry (use float64 for stability)\n",
        "        r = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
        "        v = self.defense_vel.astype(np.float64)\n",
        "        vrel = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
        "\n",
        "        R = float(np.linalg.norm(r)) + eps\n",
        "        V = float(np.linalg.norm(v)) + eps\n",
        "\n",
        "        rhat = r / R\n",
        "        vhat = v / V\n",
        "\n",
        "        # Heading error alpha = angle between velocity direction and LOS direction\n",
        "        cosang = float(np.clip(np.dot(vhat, rhat), -1.0, 1.0))\n",
        "        alpha = float(np.arccos(cosang))  # radians\n",
        "\n",
        "        # LOS angular rate omega (world frame)\n",
        "        omega = np.cross(r, vrel) / (float(np.dot(r, r)) + eps)\n",
        "        omega_mag = float(np.linalg.norm(omega))\n",
        "\n",
        "        # Closing speed (positive => closing)\n",
        "        vc = -float(np.dot(r, vrel)) / R\n",
        "\n",
        "        # --- PN term ---\n",
        "        N = 3.0\n",
        "        a_pn = N * vc * np.cross(omega, rhat)  # lateral accel in world frame\n",
        "\n",
        "        # --- Acquisition term (turn-to-LOS) ---\n",
        "        # Perpendicular component of LOS relative to forward direction\n",
        "        rhat_perp = rhat - float(np.dot(rhat, vhat)) * vhat\n",
        "        nperp = float(np.linalg.norm(rhat_perp))\n",
        "\n",
        "        if nperp < 1e-8:\n",
        "            a_acq = np.zeros(3, dtype=np.float64)\n",
        "        else:\n",
        "            rhat_perp /= nperp  # unit sideways \"turn toward LOS\" direction\n",
        "\n",
        "            # Curvature-based magnitude: ~k * V^2 / R, saturate later via a_max\n",
        "            k_acq = 5.0  # try 3.0–8.0\n",
        "            a_acq = k_acq * (V * V / R) * rhat_perp\n",
        "\n",
        "        # --- Blend weight w: 0 => pure PN, 1 => pure acquisition ---\n",
        "\n",
        "        # Alpha-based weight (dominant)\n",
        "        alpha_on   = np.deg2rad(20.0)   # start blending earlier\n",
        "        alpha_full = np.deg2rad(55.0)\n",
        "\n",
        "        x_alpha = (alpha - alpha_on) / (alpha_full - alpha_on + eps)\n",
        "        w_alpha = self._smoothstep(x_alpha)\n",
        "\n",
        "        # Omega-based modifier (only boosts acquisition when PN is sleepy)\n",
        "        omega_full = 0.00\n",
        "        omega_on   = 0.05   # <-- key: less brittle than 0.02\n",
        "\n",
        "        x_omega = (omega_on - omega_mag) / (omega_on - omega_full + eps)\n",
        "        w_omega = self._smoothstep(x_omega)\n",
        "\n",
        "        # Robust combine: alpha dominates; omega can't fully shut it off\n",
        "        w = w_alpha * (0.25 + 0.75 * w_omega)\n",
        "\n",
        "        # Optional: if not closing, force strong acquisition\n",
        "        if vc <= 0.0:\n",
        "            w = max(w, 0.9)\n",
        "\n",
        "        a_ideal = (1.0 - w) * a_pn + w * a_acq\n",
        "\n",
        "        # Project into your lateral control basis (right/up) and normalize by a_max\n",
        "        # Note: Environment now handles gravity compensation internally,\n",
        "        # so ProNav outputs desired NET lateral accel (same semantics as PPO)\n",
        "        forward, right, up = self._compute_lateral_basis(self.defense_vel)\n",
        "        a_right = float(np.dot(a_ideal, right))\n",
        "        a_up    = float(np.dot(a_ideal, up))\n",
        "\n",
        "        action = np.array([a_right / self.a_max, a_up / self.a_max], dtype=np.float32)\n",
        "        return np.clip(action, -1.0, 1.0)\n",
        "    \n",
        "    def _segment_sphere_intersect(self, r0, r1, r_hit):\n",
        "        dr = r1 - r0\n",
        "        dr_norm_sq = float(np.dot(dr, dr))\n",
        "        if dr_norm_sq < 1e-12:\n",
        "            return float(np.dot(r0, r0)) <= r_hit * r_hit\n",
        "        s_star = -float(np.dot(r0, dr)) / dr_norm_sq\n",
        "        s_star = max(0.0, min(1.0, s_star))\n",
        "        r_closest = r0 + s_star * dr\n",
        "        return float(np.dot(r_closest, r_closest)) <= r_hit * r_hit\n",
        "    \n",
        "    def _phi_closest_approach(self):\n",
        "        \"\"\"\n",
        "        Potential based on closest-approach miss distance under\n",
        "        constant relative velocity assumption.\n",
        "        Returns:\n",
        "            phi, d_star, t_star\n",
        "        \"\"\"\n",
        "        eps = 1e-9\n",
        "\n",
        "        r = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
        "        vrel = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
        "\n",
        "        vrel_norm_sq = float(np.dot(vrel, vrel)) + eps\n",
        "\n",
        "        # t* = argmin ||r + vrel t|| for t >= 0\n",
        "        t_star = -float(np.dot(r, vrel)) / vrel_norm_sq\n",
        "        t_star = max(0.0, t_star)\n",
        "\n",
        "        m_star = r + vrel * t_star\n",
        "        d_star = float(np.linalg.norm(m_star))\n",
        "\n",
        "        phi = -d_star / (self.dstar_scale + eps)\n",
        "        return float(phi), d_star, float(t_star)\n",
        "    \n",
        "    def _get_obs(self):\n",
        "        eps = 1e-9\n",
        "\n",
        "        # World-frame relative state\n",
        "        r_world = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
        "        vrel_world = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
        "\n",
        "        # Local basis from defense velocity (world frame unit vectors)\n",
        "        forward, right, up = self._compute_lateral_basis(self.defense_vel)\n",
        "\n",
        "        # ===============================\n",
        "        # 1) Ego-frame (body-frame) r and vrel\n",
        "        # ===============================\n",
        "        r_body = np.array([\n",
        "            float(np.dot(r_world, forward)),\n",
        "            float(np.dot(r_world, right)),\n",
        "            float(np.dot(r_world, up)),\n",
        "        ], dtype=np.float64)\n",
        "\n",
        "        vrel_body = np.array([\n",
        "            float(np.dot(vrel_world, forward)),\n",
        "            float(np.dot(vrel_world, right)),\n",
        "            float(np.dot(vrel_world, up)),\n",
        "        ], dtype=np.float64)\n",
        "\n",
        "        # Normalize r_body / vrel_body (easy range only)\n",
        "        pos_scale = float(self.range_easy_max)\n",
        "        vel_scale = 4000.0\n",
        "\n",
        "        r_body_n = (r_body / (pos_scale + eps)).astype(np.float32)\n",
        "        vrel_body_n = (vrel_body / (vel_scale + eps)).astype(np.float32)\n",
        "\n",
        "        # ===============================\n",
        "        # 2) Scalar helpers\n",
        "        # ===============================\n",
        "        dist = float(np.linalg.norm(r_world)) + 1e-6\n",
        "        v_close = -float(np.dot(r_world, vrel_world)) / dist  # positive when closing\n",
        "\n",
        "        dist_n = np.float32(np.clip(dist / 1_000_000.0, 0.0, 4.0))\n",
        "        vclose_n = np.float32(np.clip(v_close / 3000.0, -2.0, 2.0))\n",
        "        dist_vclose_feat = np.array([dist_n, vclose_n], dtype=np.float32)\n",
        "\n",
        "        # Defense own vertical state (keep for ground constraint)\n",
        "        def_z_n = np.float32(np.clip(self.defense_pos[2] / 100_000.0, -1.0, 2.0))\n",
        "        def_vz_n = np.float32(np.clip(self.defense_vel[2] / 3000.0, -2.0, 2.0))\n",
        "        def_state_feat = np.array([def_z_n, def_vz_n], dtype=np.float32)\n",
        "\n",
        "        # ===============================\n",
        "        # 4) Keep your geometry features (consistent with ego-frame)\n",
        "        # ===============================\n",
        "        dist_body = float(np.linalg.norm(r_body)) + 1e-6\n",
        "\n",
        "        # LOS lateral projections in body frame\n",
        "        los_right = float(r_body[1] / dist_body)\n",
        "        los_up    = float(r_body[2] / dist_body)\n",
        "\n",
        "        # LOS rate omega in body frame: omega = (r x vrel)/||r||^2\n",
        "        dist2_body = float(np.dot(r_body, r_body)) + eps\n",
        "        omega_body = np.cross(r_body, vrel_body) / dist2_body\n",
        "\n",
        "        omega_right = float(omega_body[1])\n",
        "        omega_up    = float(omega_body[2])\n",
        "\n",
        "        omega_scale = 2.0\n",
        "        omega_right_n = float(np.clip(omega_right / omega_scale, -2.0, 2.0))\n",
        "        omega_up_n    = float(np.clip(omega_up / omega_scale, -2.0, 2.0))\n",
        "\n",
        "        geom_feat = np.array([los_right, los_up, omega_right_n, omega_up_n], dtype=np.float32)\n",
        "\n",
        "        # ===============================\n",
        "        # 5) NEW: kinematics garnish\n",
        "        # ===============================\n",
        "        V_def = float(np.linalg.norm(self.defense_vel))\n",
        "        V_def_n = np.float32(np.clip(V_def / 3000.0, 0.0, 3.0))  # scale: 3000 m/s baseline\n",
        "        forward_z = np.float32(float(forward[2]))               # dot(forward, world_up) since world_up=[0,0,1]\n",
        "\n",
        "        kin_feat = np.array([V_def_n, forward_z], dtype=np.float32)\n",
        "\n",
        "        # Final obs (16D, no actuator state)\n",
        "        obs = np.concatenate(\n",
        "            [r_body_n, vrel_body_n, dist_vclose_feat, def_state_feat, geom_feat, kin_feat],\n",
        "            axis=0\n",
        "        ).astype(np.float32)\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def _compute_lateral_basis(self, velocity):\n",
        "        \"\"\"\n",
        "        Horizon-stable basis:\n",
        "          forward = along velocity\n",
        "          right   = world_up x forward  (horizontal right)\n",
        "          up      = forward x right     (completes orthonormal frame)\n",
        "        This keeps 'up' as close to world-up as possible and avoids weird twisting.\n",
        "        \"\"\"\n",
        "        speed = float(np.linalg.norm(velocity))\n",
        "        if speed < 1.0:\n",
        "            forward = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
        "        else:\n",
        "            forward = (velocity / speed).astype(np.float32)\n",
        "\n",
        "        world_up = np.array([0.0, 0.0, 1.0], dtype=np.float32)\n",
        "\n",
        "        # right = world_up x forward\n",
        "        right_raw = np.cross(world_up, forward)\n",
        "        rnorm = float(np.linalg.norm(right_raw))\n",
        "\n",
        "        # If forward is near world_up, right_raw ~ 0. Pick a consistent fallback.\n",
        "        if rnorm < 1e-6:\n",
        "            # Choose a fixed \"north\" axis in world XY and build right from that\n",
        "            # This prevents random spinning when vertical.\n",
        "            north = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
        "            right_raw = np.cross(north, forward)\n",
        "            rnorm = float(np.linalg.norm(right_raw))\n",
        "            if rnorm < 1e-6:\n",
        "                north = np.array([0.0, 1.0, 0.0], dtype=np.float32)\n",
        "                right_raw = np.cross(north, forward)\n",
        "                rnorm = float(np.linalg.norm(right_raw))\n",
        "\n",
        "        right = (right_raw / (rnorm + 1e-9)).astype(np.float32)\n",
        "\n",
        "        # up = forward x right (not right x forward)\n",
        "        up_raw = np.cross(forward, right)\n",
        "        up = (up_raw / (float(np.linalg.norm(up_raw)) + 1e-9)).astype(np.float32)\n",
        "\n",
        "        return forward, right, up\n",
        "\n",
        "    def step(self, action):\n",
        "        if getattr(self, \"done\", False):\n",
        "            return self._get_obs(), 0.0, True, False, {\"event\": \"called_step_after_done\", \"dist\": float(np.linalg.norm(self.enemy_pos - self.defense_pos))}\n",
        "        \n",
        "        action = np.clip(action, -1.0, 1.0).astype(np.float32)\n",
        "        mag = float(np.linalg.norm(action))\n",
        "        if mag > 1.0:\n",
        "            action = action / mag\n",
        "            mag = 1.0\n",
        "        \n",
        "        # Update episode trackers\n",
        "        self.ep_max_action_mag = max(self.ep_max_action_mag, float(mag))\n",
        "\n",
        "        dist_before = float(np.linalg.norm(self.enemy_pos - self.defense_pos))\n",
        "        \n",
        "        # --- Shaping: closest-approach potential BEFORE transition ---\n",
        "        phi_before, dstar_before, tstar_before = self._phi_closest_approach()\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        event = \"running\"\n",
        "        \n",
        "        for _ in range(self.n_substeps):\n",
        "            dt = self.dt_sim\n",
        "            enemy_pos_old = self.enemy_pos.copy()\n",
        "            defense_pos_old = self.defense_pos.copy()\n",
        "            \n",
        "            forward, right, up = self._compute_lateral_basis(self.defense_vel)\n",
        "            g_vec = np.array([0.0, 0.0, -self.g], dtype=np.float32)\n",
        "            # Direct commanded lateral accel (no lag/jerk)\n",
        "            a_lat = (action[0] * self.a_max) * right + (action[1] * self.a_max) * up\n",
        "            self.defense_vel += (a_lat + g_vec) * dt\n",
        "            self.defense_pos += self.defense_vel * dt\n",
        "            self.defense_x, self.defense_y, self.defense_z = self.defense_pos\n",
        "            \n",
        "            # Enemy missile: pure ballistic (gravity only)\n",
        "            self.enemy_vel += g_vec * dt\n",
        "            self.enemy_pos += self.enemy_vel * dt\n",
        "            self.enemy_x, self.enemy_y, self.enemy_z = self.enemy_pos\n",
        "            self.t += dt\n",
        "            \n",
        "            r0 = enemy_pos_old - defense_pos_old\n",
        "            r1 = self.enemy_pos - self.defense_pos\n",
        "            if self._segment_sphere_intersect(r0, r1, self.collision_radius):\n",
        "                self.success = True\n",
        "                terminated = True\n",
        "                self.done = True\n",
        "                event = \"hit\"\n",
        "                self.time_to_hit = float(self.t)\n",
        "                self.terminal_event = \"hit\"\n",
        "                break\n",
        "            \n",
        "            dist = float(np.linalg.norm(self.enemy_pos - self.defense_pos))\n",
        "            if dist > self.max_distance:\n",
        "                truncated = True\n",
        "                self.done = True\n",
        "                event = \"diverged\"\n",
        "                self.terminal_event = \"diverged\"\n",
        "                break\n",
        "            if self.defense_pos[2] < 0:\n",
        "                terminated = True\n",
        "                self.done = True\n",
        "                event = \"defense_ground\"\n",
        "                self.terminal_event = \"defense_ground\"\n",
        "                break\n",
        "            if self.enemy_pos[2] < 0:\n",
        "                terminated = True\n",
        "                self.done = True\n",
        "                event = \"enemy_ground\"\n",
        "                self.terminal_event = \"enemy_ground\"\n",
        "                break\n",
        "            if self.t >= self.t_max:\n",
        "                truncated = True\n",
        "                self.done = True\n",
        "                event = \"timeout\"\n",
        "                self.terminal_event = \"timeout\"\n",
        "                break\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        dist_after = float(np.linalg.norm(self.enemy_pos - self.defense_pos))\n",
        "        self.min_dist = min(getattr(self, \"min_dist\", float(\"inf\")), dist_after)\n",
        "        self.ep_min_dist = min(self.ep_min_dist, float(dist_after))\n",
        "        \n",
        "        # Reward: ZEM shaping + closing only\n",
        "        v_scale = 1500.0\n",
        "        r = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
        "        vrel = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
        "        d = float(np.linalg.norm(r)) + 1e-9\n",
        "        rhat = r / d\n",
        "        r_close = float(np.tanh(-float(np.dot(rhat, vrel)) / v_scale))\n",
        "        phi_after, _, _ = self._phi_closest_approach()  # always (no free jackpot on termination)\n",
        "        r_ca = self.w_ca * (self.gamma_shape * phi_after - phi_before)\n",
        "        reward = float(0.5 * r_ca + 0.5 * 0.1 * r_close)\n",
        "        final_event = event\n",
        "        if terminated or truncated:\n",
        "            self.terminal_event = event\n",
        "        info = {\"event\": final_event, \"t\": float(self.t), \"dist\": float(dist_after), \"r_ca\": float(r_ca), \"r_close\": float(r_close)}\n",
        "        if terminated or truncated:\n",
        "            info[\"min_dist\"] = float(self.ep_min_dist)\n",
        "            info[\"max_action_mag\"] = float(self.ep_max_action_mag)\n",
        "            info[\"time_to_hit\"] = float(self.time_to_hit) if self.time_to_hit is not None else np.nan\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None: self.np_random = np.random.RandomState(seed)\n",
        "        self.done = False\n",
        "        self.success = False\n",
        "        self.t = 0.0\n",
        "        self.generate_enemy_missile()\n",
        "        self.generate_defense_missile()\n",
        "        \n",
        "        self.defense_vel = np.array([\n",
        "            self.defense_initial_velocity * np.cos(self.defense_azimuth) * np.cos(self.defense_theta),\n",
        "            self.defense_initial_velocity * np.sin(self.defense_azimuth) * np.cos(self.defense_theta),\n",
        "            self.defense_initial_velocity * np.sin(self.defense_theta)\n",
        "        ], dtype=np.float32)\n",
        "        \n",
        "        self.enemy_vel = np.array([\n",
        "            self.enemy_initial_velocity * np.cos(self.enemy_azimuth) * np.cos(self.enemy_theta),\n",
        "            self.enemy_initial_velocity * np.sin(self.enemy_azimuth) * np.cos(self.enemy_theta),\n",
        "            self.enemy_initial_velocity * np.sin(self.enemy_theta)\n",
        "        ], dtype=np.float32)\n",
        "        \n",
        "        self.defense_pos = np.array([self.defense_x, self.defense_y, self.defense_z], dtype=np.float32)\n",
        "        self.enemy_pos = np.array([self.enemy_x, self.enemy_y, self.enemy_z], dtype=np.float32)\n",
        "        self.min_dist = float(np.linalg.norm(self.enemy_pos - self.defense_pos))\n",
        "        self.ep_min_dist = float(\"inf\")\n",
        "        self.ep_max_action_mag = 0.0\n",
        "        self.time_to_hit = None\n",
        "        self.terminal_event = \"running\"\n",
        "        \n",
        "        return self._get_obs(), {}\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# EVALUATION FUNCTION (separate from training reward)\n",
        "# ==========================================\n",
        "def evaluate_policy(env, policy_fn, n_episodes=100, seed0=0):\n",
        "    \"\"\"\n",
        "    Evaluate a policy and return episode-level metrics.\n",
        "    This is separate from training reward - these metrics are what you actually care about.\n",
        "    \n",
        "    Args:\n",
        "        env: missile_interception_3d environment instance\n",
        "        policy_fn: Function that takes obs and returns action\n",
        "        n_episodes: Number of episodes to evaluate\n",
        "        seed0: Starting seed (episodes use seed0, seed0+1, ..., seed0+n_episodes-1)\n",
        "    \n",
        "    Returns:\n",
        "        summary: Dict with aggregated metrics (hit_rate, min_dist stats, etc.)\n",
        "        metrics: Dict with raw episode data\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        \"hits\": 0,\n",
        "        \"ground_defense\": 0,\n",
        "        \"ground_enemy\": 0,\n",
        "        \"diverged\": 0,\n",
        "        \"timeout\": 0,\n",
        "        \"min_dist_list\": [],\n",
        "        \"time_to_hit_list\": [],\n",
        "        \"max_g_list\": [],\n",
        "    }\n",
        "\n",
        "    for i in range(n_episodes):\n",
        "        obs, _ = env.reset(seed=seed0 + i)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = policy_fn(obs)  # your PPO policy OR env.calculate_pronav() baseline\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "        event = info[\"event\"]\n",
        "        metrics[\"min_dist_list\"].append(env.ep_min_dist)\n",
        "        metrics[\"max_g_list\"].append(env.ep_max_action_mag)\n",
        "\n",
        "        if event == \"hit\":\n",
        "            metrics[\"hits\"] += 1\n",
        "            if env.time_to_hit is not None:\n",
        "                metrics[\"time_to_hit_list\"].append(env.time_to_hit)\n",
        "        elif event == \"defense_ground\":\n",
        "            metrics[\"ground_defense\"] += 1\n",
        "        elif event == \"enemy_ground\":\n",
        "            metrics[\"ground_enemy\"] += 1\n",
        "        elif event == \"diverged\":\n",
        "            metrics[\"diverged\"] += 1\n",
        "        elif event == \"timeout\":\n",
        "            metrics[\"timeout\"] += 1\n",
        "\n",
        "    hit_rate = metrics[\"hits\"] / n_episodes\n",
        "\n",
        "    summary = {\n",
        "        \"hit_rate\": hit_rate,\n",
        "        \"min_dist_mean\": float(np.mean(metrics[\"min_dist_list\"])),\n",
        "        \"min_dist_p50\": float(np.median(metrics[\"min_dist_list\"])),\n",
        "        \"min_dist_p10\": float(np.percentile(metrics[\"min_dist_list\"], 10)),\n",
        "        \"time_to_hit_mean\": float(np.mean(metrics[\"time_to_hit_list\"])) if metrics[\"time_to_hit_list\"] else None,\n",
        "        \"max_g_mean\": float(np.mean(metrics[\"max_g_list\"])),\n",
        "        \"violations\": {\n",
        "            \"defense_ground\": metrics[\"ground_defense\"],\n",
        "            \"enemy_ground\": metrics[\"ground_enemy\"],\n",
        "            \"diverged\": metrics[\"diverged\"],\n",
        "            \"timeout\": metrics[\"timeout\"],\n",
        "        }\n",
        "    }\n",
        "    return summary, metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def run_baseline():\n",
        "#     env = missile_interception_3d()\n",
        "#     outcomes = []\n",
        "#     min_distances = []\n",
        "#     action_loads = [] # Track if we are saturating (maxing out fins)\n",
        "\n",
        "#     N_EPISODES = 50\n",
        "#     print(f\"Running {N_EPISODES} episodes of Augmented ProNav...\")\n",
        "\n",
        "#     for i in range(N_EPISODES):\n",
        "#         obs, _ = env.reset(seed=i)\n",
        "#         done = False\n",
        "#         ep_actions = []\n",
        "\n",
        "#         while not done:\n",
        "#             # 1. Ask ProNav for the move\n",
        "#             action = env.calculate_pronav()\n",
        "            \n",
        "#             # 2. Track how hard it's pushing (0.0 to 1.0)\n",
        "#             mag = np.linalg.norm(action)\n",
        "#             ep_actions.append(mag)\n",
        "\n",
        "#             obs, reward, terminated, truncated, info = env.step(action)\n",
        "#             done = terminated or truncated\n",
        "        \n",
        "#         outcomes.append(info['event'])\n",
        "#         min_distances.append(env.ep_min_dist)\n",
        "#         avg_load = np.mean(ep_actions)\n",
        "#         action_loads.append(avg_load)\n",
        "\n",
        "#         print(f\"Ep {i+1:02d} | Res: {info['event']:<14} | Min Dist: {env.ep_min_dist:.1f} m | Avg G-Load: {avg_load*100:.1f}%\")\n",
        "\n",
        "#     # Final Stats\n",
        "#     hits = outcomes.count(\"hit\")\n",
        "#     non_hit = [d for d, e in zip(min_distances, outcomes) if e != \"hit\"]\n",
        "#     print(\"\\n--- SUMMARY ---\")\n",
        "#     print(f\"Hit Rate: {hits}/{N_EPISODES} ({hits/N_EPISODES*100:.1f}%)\")\n",
        "#     print(\"Average Miss Distance (Non-hits):\", f\"{np.mean(non_hit):.2f} m\" if non_hit else \"N/A (all hits)\")\n",
        "#     print(f\"Average G-Loading: {np.mean(action_loads)*100:.1f}% (If >90%, missile is physically too weak)\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     run_baseline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "97a043cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Policy: random | Episodes: 20\n",
            "Events: {'enemy_ground': 20}\n",
            "reward(step)       mean=-0.030056  p50=-0.049994  p10=-0.051672  p90= 0.041935  min=-0.052805  max= 0.063826\n",
            "r_ca_calc          mean=-0.003830  p50=-0.005062  p10=-0.006148  p90= 0.000236  min=-0.033897  max= 0.031497\n",
            "r_ca_info          mean=-0.003830  p50=-0.005062  p10=-0.006148  p90= 0.000236  min=-0.033897  max= 0.031497\n",
            "r_close_calc       mean=-0.562815  p50=-0.949687  p10=-0.973627  p90= 0.884774  min=-0.987381  max= 0.982641\n",
            "r_close_info       mean=-0.562815  p50=-0.949687  p10=-0.973627  p90= 0.884774  min=-0.987381  max= 0.982641\n",
            "dstar_before       mean= 186486.449533  p50= 159883.410032  p10= 53745.385761  p90= 364399.151761  min= 22805.965797  max= 567419.871967\n",
            "tstar_before       mean= 3.780076  p50= 0.000000  p10= 0.000000  p90= 16.917025  min= 0.000000  max= 54.858541\n",
            "phi_before         mean=-3.729729  p50=-3.197668  p10=-7.287983  p90=-1.074908  min=-11.348397  max=-0.456119\n",
            "dist_before        mean= 192287.221425  p50= 163132.234375  p10= 62547.641406  p90= 364399.156250  min= 23056.082031  max= 567419.875000\n",
            "action_mag         mean= 0.764842  p50= 0.799806  p10= 0.354550  p90= 1.113441  min= 0.003106  max= 1.410788\n",
            "|r_ca_info-calc|   mean= 0.000000  p50= 0.000000  p10= 0.000000  p90= 0.000000  min= 0.000000  max= 0.000000\n",
            "|r_close_info-calc| mean= 0.000000  p50= 0.000000  p10= 0.000000  p90= 0.000000  min= 0.000000  max= 0.000000\n",
            "\n",
            "--- Per-episode sums by terminal event (return components) ---\n",
            "  enemy_ground        n= 20  mean(sum_r_ca)=-6.0765  mean(sum_0.1*r_close)=-89.2821  mean(return)=-47.6793\n",
            "\n",
            "==============================\n",
            "Policy: pronav | Episodes: 20\n",
            "Events: {'hit': 20}\n",
            "reward(step)       mean= 0.050836  p50= 0.050099  p10= 0.049281  p90= 0.053917  min= 0.049047  max= 0.058585\n",
            "r_ca_calc          mean= 0.003508  p50= 0.001608  p10= 0.000068  p90= 0.010950  min=-0.000000  max= 0.022803\n",
            "r_ca_info          mean= 0.003508  p50= 0.001608  p10= 0.000068  p90= 0.010950  min=-0.000000  max= 0.022803\n",
            "r_close_calc       mean= 0.981629  p50= 0.983911  p10= 0.974035  p90= 0.989955  min= 0.847011  max= 0.992573\n",
            "r_close_info       mean= 0.981629  p50= 0.983911  p10= 0.974035  p90= 0.989955  min= 0.847011  max= 0.992573\n",
            "dstar_before       mean= 12168.986945  p50= 4817.789229  p10= 43.410206  p90= 35431.871145  min= 0.000230  max= 116102.276195\n",
            "tstar_before       mean= 18.514089  p50= 17.017938  p10= 3.407514  p90= 35.800728  min= 0.056764  max= 54.858541\n",
            "phi_before         mean=-0.243380  p50=-0.096356  p10=-0.708637  p90=-0.000868  min=-2.322046  max=-0.000000\n",
            "dist_before        mean= 68610.898328  p50= 62807.583984  p10= 12767.930176  p90= 131833.679688  min= 202.070374  max= 202408.109375\n",
            "action_mag         mean= 0.216813  p50= 0.130841  p10= 0.022859  p90= 0.565694  min= 0.000136  max= 1.096556\n",
            "|r_ca_info-calc|   mean= 0.000000  p50= 0.000000  p10= 0.000000  p90= 0.000000  min= 0.000000  max= 0.000000\n",
            "|r_close_info-calc| mean= 0.000000  p50= 0.000000  p10= 0.000000  p90= 0.000000  min= 0.000000  max= 0.000000\n",
            "\n",
            "--- Per-episode sums by terminal event (return components) ---\n",
            "  hit                 n= 20  mean(sum_r_ca)= 1.1833  mean(sum_0.1*r_close)= 33.1104  mean(return)= 17.1468\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def summarize(x, name):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    if x.size == 0:\n",
        "        print(f\"{name:18s} (empty)\")\n",
        "        return\n",
        "    print(\n",
        "        f\"{name:18s} mean={x.mean(): .6f}  p50={np.median(x): .6f}  \"\n",
        "        f\"p10={np.percentile(x,10): .6f}  p90={np.percentile(x,90): .6f}  \"\n",
        "        f\"min={x.min(): .6f}  max={x.max(): .6f}\"\n",
        "    )\n",
        "\n",
        "def probe_reward_scales(env, policy=\"random\", n_episodes=20, seed0=0, v_scale=1500.0, max_steps=20000):\n",
        "    \"\"\"\n",
        "    Logs per-STEP scales for:\n",
        "      - reward (returned)\n",
        "      - r_ca (info + recomputed), r_close (info + recomputed)\n",
        "      - phi_before / phi_after (closest-approach potential)\n",
        "      - dstar_before, tstar_before\n",
        "    \"\"\"\n",
        "    events = Counter()\n",
        "\n",
        "    S = {\n",
        "        \"reward\": [],\n",
        "        \"action_mag\": [],\n",
        "\n",
        "        \"r_ca_info\": [],\n",
        "        \"r_ca_calc\": [],\n",
        "        \"r_close_info\": [],\n",
        "        \"r_close_calc\": [],\n",
        "\n",
        "        \"phi_before\": [],\n",
        "        \"phi_after\": [],\n",
        "\n",
        "        \"dstar_before\": [],\n",
        "        \"tstar_before\": [],\n",
        "        \"dist_before\": [],\n",
        "        \"dist_after\": [],\n",
        "\n",
        "        \"abs_d_rca\": [],\n",
        "        \"abs_d_rclose\": [],\n",
        "    }\n",
        "\n",
        "    ep_sums = []  # (event, sum_r_ca, sum_01_r_close) per episode\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        obs, _ = env.reset(seed=seed0 + ep)\n",
        "        done = False\n",
        "        steps = 0\n",
        "        sum_r_ca_ep = 0.0\n",
        "        sum_01_r_close_ep = 0.0\n",
        "\n",
        "        while not done:\n",
        "            steps += 1\n",
        "            if steps > max_steps:\n",
        "                # safety breaker in case something goes infinite\n",
        "                events[\"max_steps_break\"] += 1\n",
        "                break\n",
        "\n",
        "            if policy == \"random\":\n",
        "                action = env.action_space.sample()\n",
        "            elif policy == \"pronav\":\n",
        "                action = env.calculate_pronav()\n",
        "            else:\n",
        "                raise ValueError(\"policy must be 'random' or 'pronav'\")\n",
        "\n",
        "            # ----- PRE-STEP GEOMETRY (closest-approach) -----\n",
        "            dist_b = float(np.linalg.norm(env.enemy_pos - env.defense_pos))\n",
        "            phi_b, dstar_b, tstar_b = env._phi_closest_approach()\n",
        "\n",
        "            # ----- STEP -----\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            done = bool(terminated or truncated)\n",
        "\n",
        "            # ----- POST-STEP GEOMETRY (always compute, same as env) -----\n",
        "            dist_a = float(np.linalg.norm(env.enemy_pos - env.defense_pos))\n",
        "            phi_a, _, _ = env._phi_closest_approach()\n",
        "\n",
        "            # recompute shaping from geometry\n",
        "            r_ca_calc = float(env.w_ca * (env.gamma_shape * phi_a - phi_b))\n",
        "\n",
        "            # recompute closing reward from post-step state (should match your env)\n",
        "            r = (env.enemy_pos - env.defense_pos).astype(np.float64)\n",
        "            vrel = (env.enemy_vel - env.defense_vel).astype(np.float64)\n",
        "            d = float(np.linalg.norm(r)) + 1e-9\n",
        "            rhat = r / d\n",
        "            r_close_calc = float(np.tanh(-float(np.dot(rhat, vrel)) / v_scale))\n",
        "\n",
        "            # pull info values if present\n",
        "            r_ca_info = float(info.get(\"r_ca\", np.nan))\n",
        "            r_close_info = float(info.get(\"r_close\", np.nan))\n",
        "\n",
        "            # log\n",
        "            S[\"reward\"].append(float(reward))\n",
        "            S[\"action_mag\"].append(float(np.linalg.norm(action)))\n",
        "\n",
        "            S[\"r_ca_info\"].append(r_ca_info)\n",
        "            S[\"r_ca_calc\"].append(r_ca_calc)\n",
        "            S[\"r_close_info\"].append(r_close_info)\n",
        "            S[\"r_close_calc\"].append(r_close_calc)\n",
        "\n",
        "            S[\"phi_before\"].append(float(phi_b))\n",
        "            S[\"phi_after\"].append(float(phi_a))\n",
        "\n",
        "            S[\"dstar_before\"].append(float(dstar_b))\n",
        "            S[\"tstar_before\"].append(float(tstar_b))\n",
        "            S[\"dist_before\"].append(dist_b)\n",
        "            S[\"dist_after\"].append(dist_a)\n",
        "\n",
        "            S[\"abs_d_rca\"].append(float(np.abs(r_ca_info - r_ca_calc)) if np.isfinite(r_ca_info) else np.nan)\n",
        "            S[\"abs_d_rclose\"].append(float(np.abs(r_close_info - r_close_calc)) if np.isfinite(r_close_info) else np.nan)\n",
        "\n",
        "            if np.isfinite(r_ca_info):\n",
        "                sum_r_ca_ep += r_ca_info\n",
        "            if np.isfinite(r_close_info):\n",
        "                sum_01_r_close_ep += 0.1 * r_close_info\n",
        "\n",
        "        ev = str(info.get(\"event\", \"unknown\"))\n",
        "        events[ev] += 1\n",
        "        ep_sums.append((ev, sum_r_ca_ep, sum_01_r_close_ep))\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\"Policy: {policy} | Episodes: {n_episodes}\")\n",
        "    print(\"Events:\", dict(events))\n",
        "\n",
        "    # Core scales\n",
        "    summarize(S[\"reward\"],        \"reward(step)\")\n",
        "    summarize(S[\"r_ca_calc\"],     \"r_ca_calc\")\n",
        "    summarize(S[\"r_ca_info\"],     \"r_ca_info\")\n",
        "    summarize(S[\"r_close_calc\"],  \"r_close_calc\")\n",
        "    summarize(S[\"r_close_info\"],  \"r_close_info\")\n",
        "\n",
        "    # Closest-approach geometry\n",
        "    summarize(S[\"dstar_before\"],   \"dstar_before\")\n",
        "    summarize(S[\"tstar_before\"],   \"tstar_before\")\n",
        "    summarize(S[\"phi_before\"],     \"phi_before\")\n",
        "    summarize(S[\"dist_before\"],   \"dist_before\")\n",
        "    summarize(S[\"action_mag\"],     \"action_mag\")\n",
        "\n",
        "    # Consistency checks (should be ~0 if info matches math)\n",
        "    summarize(S[\"abs_d_rca\"],    \"|r_ca_info-calc|\")\n",
        "    summarize(S[\"abs_d_rclose\"], \"|r_close_info-calc|\")\n",
        "\n",
        "    # Per-episode return components by terminal event (what PPO optimizes)\n",
        "    from collections import defaultdict\n",
        "    by_event = defaultdict(lambda: {\"sum_r_ca\": [], \"sum_01_r_close\": []})\n",
        "    for ev, s_ca, s_close in ep_sums:\n",
        "        by_event[ev][\"sum_r_ca\"].append(s_ca)\n",
        "        by_event[ev][\"sum_01_r_close\"].append(s_close)\n",
        "    print(\"\\n--- Per-episode sums by terminal event (return components) ---\")\n",
        "    for ev in sorted(by_event.keys()):\n",
        "        ca_list = by_event[ev][\"sum_r_ca\"]\n",
        "        close_list = by_event[ev][\"sum_01_r_close\"]\n",
        "        n = len(ca_list)\n",
        "        mean_ca = np.mean(ca_list) if ca_list else np.nan\n",
        "        mean_close = np.mean(close_list) if close_list else np.nan\n",
        "        mean_return = 0.5 * mean_ca + 0.5 * mean_close  # reward = 0.5*r_ca + 0.5*0.1*r_close\n",
        "        print(f\"  {ev:18s}  n={n:3d}  mean(sum_r_ca)={mean_ca: .4f}  mean(sum_0.1*r_close)={mean_close: .4f}  mean(return)={mean_return: .4f}\")\n",
        "\n",
        "# --- Run both ---\n",
        "env = missile_interception_3d()\n",
        "probe_reward_scales(env, policy=\"random\", n_episodes=20, seed0=0)\n",
        "\n",
        "env = missile_interception_3d()\n",
        "probe_reward_scales(env, policy=\"pronav\", n_episodes=20, seed0=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8cfeea92",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== STEP-0 ACTION SCAN ===\n",
            "seed=0 | n_actions=4096 | sigma=1.0\n",
            "frac(r_close>0)=1.000\n",
            "frac(r_ca>0)=0.494\n",
            "frac(r_ca>0 | r_close>0)=0.494\n",
            "reward:   p10=0.0369  p50=0.0473  p90=0.0582  max=0.0604\n",
            "r_close:  p10=0.9500  p50=0.9514  p90=0.9529  max=0.9532\n",
            "r_ca:     p10=-0.0212  p50=-0.0005  p90=0.0210  max=0.0255\n",
            "Δd*:      p10=-1043.98  p50=32.01  p90=1066.80  min=-1269.69\n",
            "\n",
            "Top actions by total reward:\n",
            "  a=[ 0.11187453 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.7\n",
            "  a=[ 0.1135906 -1.       ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.7\n",
            "  a=[ 0.10361419 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.7\n",
            "  a=[ 0.11148106 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.7\n",
            "  a=[ 0.10172243 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.6\n",
            "  a=[ 0.10507097 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.6\n",
            "  a=[ 0.10067383 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.6\n",
            "  a=[ 0.09553767 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.12368735 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.09458404 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.5\n",
            "\n",
            "Top actions by geometry shaping r_ca:\n",
            "  a=[ 0.11187453 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.7\n",
            "  a=[ 0.1135906 -1.       ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.7\n",
            "  a=[ 0.10361419 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.7\n",
            "  a=[ 0.11148106 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.7\n",
            "  a=[ 0.10172243 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.6\n",
            "  a=[ 0.10507097 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.6\n",
            "  a=[ 0.10067383 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.6\n",
            "  a=[ 0.12368735 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.12530889 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.09553767 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.5\n",
            "\n",
            "=== STEP-0 ACTION SCAN ===\n",
            "seed=0 | n_actions=2048 | sigma=1.0\n",
            "frac(r_close>0)=1.000\n",
            "frac(r_ca>0)=0.480\n",
            "frac(r_ca>0 | r_close>0)=0.480\n",
            "reward:   p10=0.0367  p50=0.0469  p90=0.0580  max=0.0604\n",
            "r_close:  p10=0.9500  p50=0.9514  p90=0.9528  max=0.9532\n",
            "r_ca:     p10=-0.0215  p50=-0.0013  p90=0.0206  max=0.0255\n",
            "Δd*:      p10=-1025.32  p50=72.92  p90=1082.49  min=-1269.66\n",
            "\n",
            "Top actions by total reward:\n",
            "  a=[ 0.10361419 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.7\n",
            "  a=[ 0.12368735 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.09458404 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.12530889 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.12626205 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.09269764 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.08380307 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1269.2\n",
            "  a=[ 0.07470915 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1268.8\n",
            "  a=[ 0.1472676 -1.       ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1268.8\n",
            "  a=[ 0.15254073 -1.        ]  reward=+0.0604  r_ca=+0.0255  r_close=+0.9532  Δd*=-1268.6\n",
            "\n",
            "Top actions by geometry shaping r_ca:\n",
            "  a=[ 0.10361419 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.7\n",
            "  a=[ 0.12368735 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.12530889 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.09458404 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.12626205 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.09269764 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.5\n",
            "  a=[ 0.08380307 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1269.2\n",
            "  a=[ 0.07470915 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1268.8\n",
            "  a=[ 0.1472676 -1.       ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1268.8\n",
            "  a=[ 0.15254073 -1.        ]  r_ca=+0.0255  reward=+0.0604  r_close=+0.9532  Δd*=-1268.6\n",
            "\n",
            "=== STEP-0 ACTION SCAN ===\n",
            "seed=1 | n_actions=2048 | sigma=1.0\n",
            "frac(r_close>0)=1.000\n",
            "frac(r_ca>0)=0.493\n",
            "frac(r_ca>0 | r_close>0)=0.493\n",
            "reward:   p10=0.0431  p50=0.0479  p90=0.0530  max=0.0540\n",
            "r_close:  p10=0.9588  p50=0.9599  p90=0.9609  max=0.9611\n",
            "r_ca:     p10=-0.0097  p50=-0.0002  p90=0.0099  max=0.0118\n",
            "Δd*:      p10=-492.04  p50=13.95  p90=486.42  min=-587.83\n",
            "\n",
            "Top actions by total reward:\n",
            "  a=[-0.10392936 -1.        ]  reward=+0.0540  r_ca=+0.0118  r_close=+0.9611  Δd*=-587.8\n",
            "  a=[-0.10227014 -1.        ]  reward=+0.0540  r_ca=+0.0118  r_close=+0.9611  Δd*=-587.8\n",
            "  a=[-0.09658013 -1.        ]  reward=+0.0540  r_ca=+0.0118  r_close=+0.9611  Δd*=-587.8\n",
            "  a=[-0.11379278 -1.        ]  reward=+0.0540  r_ca=+0.0118  r_close=+0.9611  Δd*=-587.8\n",
            "  a=[-0.11721668 -1.        ]  reward=+0.0540  r_ca=+0.0118  r_close=+0.9611  Δd*=-587.8\n",
            "  a=[-0.08599855 -1.        ]  reward=+0.0540  r_ca=+0.0118  r_close=+0.9611  Δd*=-587.7\n",
            "  a=[-0.12632293 -1.        ]  reward=+0.0540  r_ca=+0.0118  r_close=+0.9611  Δd*=-587.7\n",
            "  a=[-0.12705587 -1.        ]  reward=+0.0540  r_ca=+0.0118  r_close=+0.9611  Δd*=-587.7\n",
            "  a=[-0.1330821 -1.       ]  reward=+0.0540  r_ca=+0.0118  r_close=+0.9611  Δd*=-587.6\n",
            "  a=[-0.13862936 -1.        ]  reward=+0.0540  r_ca=+0.0118  r_close=+0.9611  Δd*=-587.5\n",
            "\n",
            "Top actions by geometry shaping r_ca:\n",
            "  a=[-0.10392936 -1.        ]  r_ca=+0.0118  reward=+0.0540  r_close=+0.9611  Δd*=-587.8\n",
            "  a=[-0.10227014 -1.        ]  r_ca=+0.0118  reward=+0.0540  r_close=+0.9611  Δd*=-587.8\n",
            "  a=[-0.09658013 -1.        ]  r_ca=+0.0118  reward=+0.0540  r_close=+0.9611  Δd*=-587.8\n",
            "  a=[-0.11379278 -1.        ]  r_ca=+0.0118  reward=+0.0540  r_close=+0.9611  Δd*=-587.8\n",
            "  a=[-0.11721668 -1.        ]  r_ca=+0.0118  reward=+0.0540  r_close=+0.9611  Δd*=-587.8\n",
            "  a=[-0.08599855 -1.        ]  r_ca=+0.0118  reward=+0.0540  r_close=+0.9611  Δd*=-587.7\n",
            "  a=[-0.12632293 -1.        ]  r_ca=+0.0118  reward=+0.0540  r_close=+0.9611  Δd*=-587.7\n",
            "  a=[-0.12705587 -1.        ]  r_ca=+0.0118  reward=+0.0540  r_close=+0.9611  Δd*=-587.7\n",
            "  a=[-0.1330821 -1.       ]  r_ca=+0.0118  reward=+0.0540  r_close=+0.9611  Δd*=-587.6\n",
            "  a=[-0.13862936 -1.        ]  r_ca=+0.0118  reward=+0.0540  r_close=+0.9611  Δd*=-587.5\n",
            "\n",
            "=== STEP-0 ACTION SCAN ===\n",
            "seed=2 | n_actions=2048 | sigma=1.0\n",
            "frac(r_close>0)=1.000\n",
            "frac(r_ca>0)=0.507\n",
            "frac(r_ca>0 | r_close>0)=0.507\n",
            "reward:   p10=0.0377  p50=0.0469  p90=0.0557  max=0.0565\n",
            "r_close:  p10=0.9322  p50=0.9345  p90=0.9367  max=0.9368\n",
            "r_ca:     p10=-0.0178  p50=0.0003  p90=0.0178  max=0.0193\n",
            "Δd*:      p10=-881.58  p50=-6.87  p90=895.77  min=-958.32\n",
            "\n",
            "Top actions by total reward:\n",
            "  a=[ 0.9479806 -1.       ]  reward=+0.0565  r_ca=+0.0193  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.74097234 -0.7766402 ]  reward=+0.0565  r_ca=+0.0193  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.9430208 -1.       ]  reward=+0.0565  r_ca=+0.0193  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.96388215 -1.        ]  reward=+0.0565  r_ca=+0.0193  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.94101894 -1.        ]  reward=+0.0565  r_ca=+0.0193  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.9371362 -1.       ]  reward=+0.0565  r_ca=+0.0193  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.98344326 -1.        ]  reward=+0.0565  r_ca=+0.0193  r_close=+0.9368  Δd*=-958.2\n",
            "  a=[ 0.87703896 -0.8909181 ]  reward=+0.0565  r_ca=+0.0193  r_close=+0.9368  Δd*=-958.2\n",
            "  a=[ 0.9193623 -1.       ]  reward=+0.0565  r_ca=+0.0193  r_close=+0.9368  Δd*=-958.1\n",
            "  a=[ 0.73773116 -0.7443791 ]  reward=+0.0565  r_ca=+0.0193  r_close=+0.9368  Δd*=-958.2\n",
            "\n",
            "Top actions by geometry shaping r_ca:\n",
            "  a=[ 0.74097234 -0.7766402 ]  r_ca=+0.0193  reward=+0.0565  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.9479806 -1.       ]  r_ca=+0.0193  reward=+0.0565  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.96388215 -1.        ]  r_ca=+0.0193  reward=+0.0565  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.9430208 -1.       ]  r_ca=+0.0193  reward=+0.0565  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.94101894 -1.        ]  r_ca=+0.0193  reward=+0.0565  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.9371362 -1.       ]  r_ca=+0.0193  reward=+0.0565  r_close=+0.9368  Δd*=-958.3\n",
            "  a=[ 0.98344326 -1.        ]  r_ca=+0.0193  reward=+0.0565  r_close=+0.9368  Δd*=-958.2\n",
            "  a=[ 0.87703896 -0.8909181 ]  r_ca=+0.0193  reward=+0.0565  r_close=+0.9368  Δd*=-958.2\n",
            "  a=[ 0.73773116 -0.7443791 ]  r_ca=+0.0193  reward=+0.0565  r_close=+0.9368  Δd*=-958.2\n",
            "  a=[ 0.99346685 -1.        ]  r_ca=+0.0193  reward=+0.0565  r_close=+0.9368  Δd*=-958.2\n",
            "\n",
            "=== STEP-0 ACTION SCAN ===\n",
            "seed=3 | n_actions=2048 | sigma=1.0\n",
            "frac(r_close>0)=1.000\n",
            "frac(r_ca>0)=0.509\n",
            "frac(r_ca>0 | r_close>0)=0.509\n",
            "reward:   p10=0.0338  p50=0.0491  p90=0.0635  max=0.0647\n",
            "r_close:  p10=0.9712  p50=0.9719  p90=0.9725  max=0.9726\n",
            "r_ca:     p10=-0.0295  p50=0.0009  p90=0.0297  max=0.0322\n",
            "Δd*:      p10=-1479.43  p50=-44.04  p90=1476.96  min=-1605.45\n",
            "\n",
            "Top actions by total reward:\n",
            "  a=[ 0.5889986 -1.       ]  reward=+0.0647  r_ca=+0.0322  r_close=+0.9726  Δd*=-1605.4\n",
            "  a=[ 0.5942743 -1.       ]  reward=+0.0647  r_ca=+0.0322  r_close=+0.9726  Δd*=-1605.4\n",
            "  a=[ 0.5785898 -0.9963979]  reward=+0.0647  r_ca=+0.0322  r_close=+0.9726  Δd*=-1605.4\n",
            "  a=[ 0.59402394 -1.        ]  reward=+0.0647  r_ca=+0.0322  r_close=+0.9726  Δd*=-1605.4\n",
            "  a=[ 0.5753973 -1.       ]  reward=+0.0647  r_ca=+0.0322  r_close=+0.9726  Δd*=-1605.4\n",
            "  a=[ 0.52584887 -0.923411  ]  reward=+0.0647  r_ca=+0.0322  r_close=+0.9726  Δd*=-1605.3\n",
            "  a=[ 0.5678519 -1.       ]  reward=+0.0647  r_ca=+0.0322  r_close=+0.9726  Δd*=-1605.2\n",
            "  a=[ 0.6106612 -1.       ]  reward=+0.0647  r_ca=+0.0322  r_close=+0.9726  Δd*=-1605.2\n",
            "  a=[ 0.5588329 -1.       ]  reward=+0.0647  r_ca=+0.0322  r_close=+0.9726  Δd*=-1605.0\n",
            "  a=[ 0.5561519 -1.       ]  reward=+0.0647  r_ca=+0.0322  r_close=+0.9726  Δd*=-1605.0\n",
            "\n",
            "Top actions by geometry shaping r_ca:\n",
            "  a=[ 0.5889986 -1.       ]  r_ca=+0.0322  reward=+0.0647  r_close=+0.9726  Δd*=-1605.4\n",
            "  a=[ 0.5942743 -1.       ]  r_ca=+0.0322  reward=+0.0647  r_close=+0.9726  Δd*=-1605.4\n",
            "  a=[ 0.59402394 -1.        ]  r_ca=+0.0322  reward=+0.0647  r_close=+0.9726  Δd*=-1605.4\n",
            "  a=[ 0.5785898 -0.9963979]  r_ca=+0.0322  reward=+0.0647  r_close=+0.9726  Δd*=-1605.4\n",
            "  a=[ 0.5753973 -1.       ]  r_ca=+0.0322  reward=+0.0647  r_close=+0.9726  Δd*=-1605.4\n",
            "  a=[ 0.52584887 -0.923411  ]  r_ca=+0.0322  reward=+0.0647  r_close=+0.9726  Δd*=-1605.3\n",
            "  a=[ 0.5678519 -1.       ]  r_ca=+0.0322  reward=+0.0647  r_close=+0.9726  Δd*=-1605.2\n",
            "  a=[ 0.6106612 -1.       ]  r_ca=+0.0322  reward=+0.0647  r_close=+0.9726  Δd*=-1605.2\n",
            "  a=[ 0.5588329 -1.       ]  r_ca=+0.0322  reward=+0.0647  r_close=+0.9726  Δd*=-1605.0\n",
            "  a=[ 0.5561519 -1.       ]  r_ca=+0.0322  reward=+0.0647  r_close=+0.9726  Δd*=-1605.0\n",
            "\n",
            "=== STEP-0 ACTION SCAN ===\n",
            "seed=4 | n_actions=2048 | sigma=1.0\n",
            "frac(r_close>0)=1.000\n",
            "frac(r_ca>0)=0.497\n",
            "frac(r_ca>0 | r_close>0)=0.497\n",
            "reward:   p10=0.0408  p50=0.0463  p90=0.0520  max=0.0532\n",
            "r_close:  p10=0.9260  p50=0.9281  p90=0.9302  max=0.9306\n",
            "r_ca:     p10=-0.0110  p50=-0.0001  p90=0.0110  max=0.0134\n",
            "Δd*:      p10=-544.79  p50=9.49  p90=552.42  min=-664.83\n",
            "\n",
            "Top actions by total reward:\n",
            "  a=[ 0.1314722 -1.       ]  reward=+0.0532  r_ca=+0.0134  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.13546331 -1.        ]  reward=+0.0532  r_ca=+0.0134  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.13790476 -1.        ]  reward=+0.0532  r_ca=+0.0134  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.11896231 -1.        ]  reward=+0.0532  r_ca=+0.0134  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.1429327 -1.       ]  reward=+0.0532  r_ca=+0.0134  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.11879931 -1.        ]  reward=+0.0532  r_ca=+0.0134  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.10070261 -1.        ]  reward=+0.0532  r_ca=+0.0134  r_close=+0.9306  Δd*=-664.6\n",
            "  a=[ 0.1629345 -1.       ]  reward=+0.0532  r_ca=+0.0134  r_close=+0.9306  Δd*=-664.5\n",
            "  a=[ 0.09777766 -1.        ]  reward=+0.0532  r_ca=+0.0134  r_close=+0.9306  Δd*=-664.5\n",
            "  a=[ 0.09336618 -1.        ]  reward=+0.0532  r_ca=+0.0134  r_close=+0.9306  Δd*=-664.4\n",
            "\n",
            "Top actions by geometry shaping r_ca:\n",
            "  a=[ 0.1314722 -1.       ]  r_ca=+0.0134  reward=+0.0532  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.13546331 -1.        ]  r_ca=+0.0134  reward=+0.0532  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.13790476 -1.        ]  r_ca=+0.0134  reward=+0.0532  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.11896231 -1.        ]  r_ca=+0.0134  reward=+0.0532  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.11879931 -1.        ]  r_ca=+0.0134  reward=+0.0532  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.1429327 -1.       ]  r_ca=+0.0134  reward=+0.0532  r_close=+0.9306  Δd*=-664.8\n",
            "  a=[ 0.10070261 -1.        ]  r_ca=+0.0134  reward=+0.0532  r_close=+0.9306  Δd*=-664.6\n",
            "  a=[ 0.09777766 -1.        ]  r_ca=+0.0134  reward=+0.0532  r_close=+0.9306  Δd*=-664.5\n",
            "  a=[ 0.1629345 -1.       ]  r_ca=+0.0134  reward=+0.0532  r_close=+0.9306  Δd*=-664.5\n",
            "  a=[ 0.09336618 -1.        ]  r_ca=+0.0134  reward=+0.0532  r_close=+0.9306  Δd*=-664.4\n",
            "\n",
            "=== STEP-0 ACTION SCAN ===\n",
            "seed=5 | n_actions=2048 | sigma=1.0\n",
            "frac(r_close>0)=1.000\n",
            "frac(r_ca>0)=0.493\n",
            "frac(r_ca>0 | r_close>0)=0.493\n",
            "reward:   p10=0.0355  p50=0.0473  p90=0.0591  max=0.0607\n",
            "r_close:  p10=0.9483  p50=0.9499  p90=0.9515  max=0.9517\n",
            "r_ca:     p10=-0.0239  p50=-0.0004  p90=0.0231  max=0.0263\n",
            "Δd*:      p10=-1148.05  p50=28.23  p90=1203.07  min=-1306.73\n",
            "\n",
            "Top actions by total reward:\n",
            "  a=[ 0.9813262  -0.43122137]  reward=+0.0607  r_ca=+0.0263  r_close=+0.9516  Δd*=-1306.7\n",
            "  a=[ 0.93367463 -0.39417717]  reward=+0.0607  r_ca=+0.0263  r_close=+0.9516  Δd*=-1306.7\n",
            "  a=[ 1.         -0.41506463]  reward=+0.0607  r_ca=+0.0263  r_close=+0.9516  Δd*=-1306.7\n",
            "  a=[ 1.        -0.4494908]  reward=+0.0607  r_ca=+0.0263  r_close=+0.9516  Δd*=-1306.6\n",
            "  a=[ 0.93966466 -0.3863618 ]  reward=+0.0607  r_ca=+0.0263  r_close=+0.9516  Δd*=-1306.6\n",
            "  a=[ 1.         -0.45260176]  reward=+0.0607  r_ca=+0.0263  r_close=+0.9516  Δd*=-1306.5\n",
            "  a=[ 1.         -0.46196422]  reward=+0.0607  r_ca=+0.0263  r_close=+0.9516  Δd*=-1306.3\n",
            "  a=[ 0.9541627 -0.4557254]  reward=+0.0607  r_ca=+0.0263  r_close=+0.9516  Δd*=-1305.7\n",
            "  a=[ 1.         -0.48212746]  reward=+0.0607  r_ca=+0.0263  r_close=+0.9516  Δd*=-1305.5\n",
            "  a=[ 0.9647784  -0.36767215]  reward=+0.0607  r_ca=+0.0263  r_close=+0.9516  Δd*=-1305.6\n",
            "\n",
            "Top actions by geometry shaping r_ca:\n",
            "  a=[ 0.93367463 -0.39417717]  r_ca=+0.0263  reward=+0.0607  r_close=+0.9516  Δd*=-1306.7\n",
            "  a=[ 0.9813262  -0.43122137]  r_ca=+0.0263  reward=+0.0607  r_close=+0.9516  Δd*=-1306.7\n",
            "  a=[ 1.         -0.41506463]  r_ca=+0.0263  reward=+0.0607  r_close=+0.9516  Δd*=-1306.7\n",
            "  a=[ 0.93966466 -0.3863618 ]  r_ca=+0.0263  reward=+0.0607  r_close=+0.9516  Δd*=-1306.6\n",
            "  a=[ 1.        -0.4494908]  r_ca=+0.0263  reward=+0.0607  r_close=+0.9516  Δd*=-1306.6\n",
            "  a=[ 1.         -0.45260176]  r_ca=+0.0263  reward=+0.0607  r_close=+0.9516  Δd*=-1306.5\n",
            "  a=[ 1.         -0.46196422]  r_ca=+0.0263  reward=+0.0607  r_close=+0.9516  Δd*=-1306.3\n",
            "  a=[ 0.9541627 -0.4557254]  r_ca=+0.0263  reward=+0.0607  r_close=+0.9516  Δd*=-1305.7\n",
            "  a=[ 0.9647784  -0.36767215]  r_ca=+0.0263  reward=+0.0607  r_close=+0.9516  Δd*=-1305.6\n",
            "  a=[ 1.         -0.38038814]  r_ca=+0.0263  reward=+0.0607  r_close=+0.9516  Δd*=-1305.6\n",
            "\n",
            "=== STEP-0 ACTION SCAN ===\n",
            "seed=6 | n_actions=2048 | sigma=1.0\n",
            "frac(r_close>0)=1.000\n",
            "frac(r_ca>0)=0.506\n",
            "frac(r_ca>0 | r_close>0)=0.506\n",
            "reward:   p10=0.0377  p50=0.0486  p90=0.0593  max=0.0604\n",
            "r_close:  p10=0.9679  p50=0.9689  p90=0.9698  max=0.9699\n",
            "r_ca:     p10=-0.0214  p50=0.0003  p90=0.0215  max=0.0238\n",
            "Δd*:      p10=-1071.22  p50=-10.28  p90=1077.13  min=-1182.19\n",
            "\n",
            "Top actions by total reward:\n",
            "  a=[-0.43656486 -1.        ]  reward=+0.0604  r_ca=+0.0238  r_close=+0.9699  Δd*=-1182.2\n",
            "  a=[-0.4203231 -0.9832849]  reward=+0.0604  r_ca=+0.0238  r_close=+0.9699  Δd*=-1182.2\n",
            "  a=[-0.444268 -1.      ]  reward=+0.0604  r_ca=+0.0238  r_close=+0.9699  Δd*=-1182.2\n",
            "  a=[-0.45084065 -1.        ]  reward=+0.0604  r_ca=+0.0238  r_close=+0.9699  Δd*=-1182.1\n",
            "  a=[-0.4229213 -0.9357118]  reward=+0.0604  r_ca=+0.0238  r_close=+0.9699  Δd*=-1182.1\n",
            "  a=[-0.41501054 -1.        ]  reward=+0.0604  r_ca=+0.0238  r_close=+0.9699  Δd*=-1182.0\n",
            "  a=[-0.46087578 -1.        ]  reward=+0.0604  r_ca=+0.0238  r_close=+0.9699  Δd*=-1181.9\n",
            "  a=[-0.40526313 -1.        ]  reward=+0.0604  r_ca=+0.0238  r_close=+0.9699  Δd*=-1181.8\n",
            "  a=[-0.40285486 -1.        ]  reward=+0.0604  r_ca=+0.0237  r_close=+0.9699  Δd*=-1181.7\n",
            "  a=[-0.4013312 -1.       ]  reward=+0.0604  r_ca=+0.0237  r_close=+0.9699  Δd*=-1181.7\n",
            "\n",
            "Top actions by geometry shaping r_ca:\n",
            "  a=[-0.43656486 -1.        ]  r_ca=+0.0238  reward=+0.0604  r_close=+0.9699  Δd*=-1182.2\n",
            "  a=[-0.4203231 -0.9832849]  r_ca=+0.0238  reward=+0.0604  r_close=+0.9699  Δd*=-1182.2\n",
            "  a=[-0.444268 -1.      ]  r_ca=+0.0238  reward=+0.0604  r_close=+0.9699  Δd*=-1182.2\n",
            "  a=[-0.45084065 -1.        ]  r_ca=+0.0238  reward=+0.0604  r_close=+0.9699  Δd*=-1182.1\n",
            "  a=[-0.4229213 -0.9357118]  r_ca=+0.0238  reward=+0.0604  r_close=+0.9699  Δd*=-1182.1\n",
            "  a=[-0.41501054 -1.        ]  r_ca=+0.0238  reward=+0.0604  r_close=+0.9699  Δd*=-1182.0\n",
            "  a=[-0.46087578 -1.        ]  r_ca=+0.0238  reward=+0.0604  r_close=+0.9699  Δd*=-1181.9\n",
            "  a=[-0.40526313 -1.        ]  r_ca=+0.0238  reward=+0.0604  r_close=+0.9699  Δd*=-1181.8\n",
            "  a=[-0.40285486 -1.        ]  r_ca=+0.0237  reward=+0.0604  r_close=+0.9699  Δd*=-1181.7\n",
            "  a=[-0.4013312 -1.       ]  r_ca=+0.0237  reward=+0.0604  r_close=+0.9699  Δd*=-1181.7\n",
            "\n",
            "=== STEP-0 ACTION SCAN ===\n",
            "seed=7 | n_actions=2048 | sigma=1.0\n",
            "frac(r_close>0)=1.000\n",
            "frac(r_ca>0)=0.520\n",
            "frac(r_ca>0 | r_close>0)=0.520\n",
            "reward:   p10=0.0370  p50=0.0478  p90=0.0571  max=0.0592\n",
            "r_close:  p10=0.9379  p50=0.9397  p90=0.9412  max=0.9416\n",
            "r_ca:     p10=-0.0197  p50=0.0015  p90=0.0200  max=0.0243\n",
            "Δd*:      p10=-996.93  p50=-72.00  p90=989.85  min=-1211.49\n",
            "\n",
            "Top actions by total reward:\n",
            "  a=[ 0.02069277 -1.        ]  reward=+0.0592  r_ca=+0.0243  r_close=+0.9416  Δd*=-1211.5\n",
            "  a=[ 0.03722796 -1.        ]  reward=+0.0592  r_ca=+0.0243  r_close=+0.9416  Δd*=-1211.5\n",
            "  a=[ 0.04018198 -1.        ]  reward=+0.0592  r_ca=+0.0243  r_close=+0.9416  Δd*=-1211.4\n",
            "  a=[ 0.04466036 -1.        ]  reward=+0.0592  r_ca=+0.0243  r_close=+0.9416  Δd*=-1211.3\n",
            "  a=[ 0.04662176 -1.        ]  reward=+0.0592  r_ca=+0.0243  r_close=+0.9416  Δd*=-1211.3\n",
            "  a=[ 0.04693719 -1.        ]  reward=+0.0592  r_ca=+0.0243  r_close=+0.9416  Δd*=-1211.3\n",
            "  a=[ 0.04797043 -1.        ]  reward=+0.0592  r_ca=+0.0243  r_close=+0.9416  Δd*=-1211.3\n",
            "  a=[ 2.5359003e-04 -1.0000000e+00]  reward=+0.0592  r_ca=+0.0243  r_close=+0.9416  Δd*=-1211.1\n",
            "  a=[-0.00379803 -1.        ]  reward=+0.0592  r_ca=+0.0243  r_close=+0.9416  Δd*=-1210.9\n",
            "  a=[-0.00635984 -1.        ]  reward=+0.0592  r_ca=+0.0243  r_close=+0.9416  Δd*=-1210.8\n",
            "\n",
            "Top actions by geometry shaping r_ca:\n",
            "  a=[ 0.02069277 -1.        ]  r_ca=+0.0243  reward=+0.0592  r_close=+0.9416  Δd*=-1211.5\n",
            "  a=[ 0.03722796 -1.        ]  r_ca=+0.0243  reward=+0.0592  r_close=+0.9416  Δd*=-1211.5\n",
            "  a=[ 0.04018198 -1.        ]  r_ca=+0.0243  reward=+0.0592  r_close=+0.9416  Δd*=-1211.4\n",
            "  a=[ 0.04466036 -1.        ]  r_ca=+0.0243  reward=+0.0592  r_close=+0.9416  Δd*=-1211.3\n",
            "  a=[ 0.04662176 -1.        ]  r_ca=+0.0243  reward=+0.0592  r_close=+0.9416  Δd*=-1211.3\n",
            "  a=[ 0.04693719 -1.        ]  r_ca=+0.0243  reward=+0.0592  r_close=+0.9416  Δd*=-1211.3\n",
            "  a=[ 0.04797043 -1.        ]  r_ca=+0.0243  reward=+0.0592  r_close=+0.9416  Δd*=-1211.3\n",
            "  a=[ 2.5359003e-04 -1.0000000e+00]  r_ca=+0.0243  reward=+0.0592  r_close=+0.9416  Δd*=-1211.1\n",
            "  a=[-0.00379803 -1.        ]  r_ca=+0.0243  reward=+0.0592  r_close=+0.9416  Δd*=-1210.9\n",
            "  a=[-0.00635984 -1.        ]  r_ca=+0.0243  reward=+0.0592  r_close=+0.9416  Δd*=-1210.8\n",
            "\n",
            "=== STEP-0 ACTION SCAN ===\n",
            "seed=8 | n_actions=2048 | sigma=1.0\n",
            "frac(r_close>0)=1.000\n",
            "frac(r_ca>0)=0.479\n",
            "frac(r_ca>0 | r_close>0)=0.479\n",
            "reward:   p10=0.0434  p50=0.0472  p90=0.0516  max=0.0524\n",
            "r_close:  p10=0.9482  p50=0.9495  p90=0.9510  max=0.9512\n",
            "r_ca:     p10=-0.0081  p50=-0.0006  p90=0.0080  max=0.0097\n",
            "Δd*:      p10=-399.08  p50=34.61  p90=408.33  min=-483.96\n",
            "\n",
            "Top actions by total reward:\n",
            "  a=[-0.05880946 -1.        ]  reward=+0.0524  r_ca=+0.0097  r_close=+0.9512  Δd*=-484.0\n",
            "  a=[-0.06312952 -1.        ]  reward=+0.0524  r_ca=+0.0097  r_close=+0.9512  Δd*=-484.0\n",
            "  a=[-0.0839112 -1.       ]  reward=+0.0524  r_ca=+0.0097  r_close=+0.9512  Δd*=-483.8\n",
            "  a=[-0.08674433 -1.        ]  reward=+0.0524  r_ca=+0.0097  r_close=+0.9512  Δd*=-483.8\n",
            "  a=[-0.09029067 -1.        ]  reward=+0.0524  r_ca=+0.0097  r_close=+0.9512  Δd*=-483.7\n",
            "  a=[-0.02725931 -1.        ]  reward=+0.0524  r_ca=+0.0097  r_close=+0.9512  Δd*=-483.7\n",
            "  a=[-0.02699479 -1.        ]  reward=+0.0524  r_ca=+0.0097  r_close=+0.9512  Δd*=-483.7\n",
            "  a=[-0.0235559 -1.       ]  reward=+0.0524  r_ca=+0.0097  r_close=+0.9512  Δd*=-483.6\n",
            "  a=[-0.09502873 -1.        ]  reward=+0.0524  r_ca=+0.0097  r_close=+0.9512  Δd*=-483.7\n",
            "  a=[-0.01970255 -1.        ]  reward=+0.0524  r_ca=+0.0097  r_close=+0.9512  Δd*=-483.5\n",
            "\n",
            "Top actions by geometry shaping r_ca:\n",
            "  a=[-0.05880946 -1.        ]  r_ca=+0.0097  reward=+0.0524  r_close=+0.9512  Δd*=-484.0\n",
            "  a=[-0.06312952 -1.        ]  r_ca=+0.0097  reward=+0.0524  r_close=+0.9512  Δd*=-484.0\n",
            "  a=[-0.0839112 -1.       ]  r_ca=+0.0097  reward=+0.0524  r_close=+0.9512  Δd*=-483.8\n",
            "  a=[-0.08674433 -1.        ]  r_ca=+0.0097  reward=+0.0524  r_close=+0.9512  Δd*=-483.8\n",
            "  a=[-0.09029067 -1.        ]  r_ca=+0.0097  reward=+0.0524  r_close=+0.9512  Δd*=-483.7\n",
            "  a=[-0.02725931 -1.        ]  r_ca=+0.0097  reward=+0.0524  r_close=+0.9512  Δd*=-483.7\n",
            "  a=[-0.02699479 -1.        ]  r_ca=+0.0097  reward=+0.0524  r_close=+0.9512  Δd*=-483.7\n",
            "  a=[-0.09502873 -1.        ]  r_ca=+0.0097  reward=+0.0524  r_close=+0.9512  Δd*=-483.7\n",
            "  a=[-0.0235559 -1.       ]  r_ca=+0.0097  reward=+0.0524  r_close=+0.9512  Δd*=-483.6\n",
            "  a=[-0.09891625 -1.        ]  r_ca=+0.0097  reward=+0.0524  r_close=+0.9512  Δd*=-483.6\n",
            "\n",
            "=== STEP-0 ACTION SCAN ===\n",
            "seed=9 | n_actions=2048 | sigma=1.0\n",
            "frac(r_close>0)=1.000\n",
            "frac(r_ca>0)=0.483\n",
            "frac(r_ca>0 | r_close>0)=0.483\n",
            "reward:   p10=0.0402  p50=0.0480  p90=0.0565  max=0.0573\n",
            "r_close:  p10=0.9651  p50=0.9661  p90=0.9670  max=0.9671\n",
            "r_ca:     p10=-0.0161  p50=-0.0005  p90=0.0162  max=0.0178\n",
            "Δd*:      p10=-807.76  p50=31.16  p90=809.00  min=-888.08\n",
            "\n",
            "Top actions by total reward:\n",
            "  a=[ 0.7233565  -0.98208445]  reward=+0.0573  r_ca=+0.0178  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.73199487 -1.        ]  reward=+0.0573  r_ca=+0.0178  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.73421454 -1.        ]  reward=+0.0573  r_ca=+0.0178  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.7312211 -1.       ]  reward=+0.0573  r_ca=+0.0178  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.6129352  -0.84115505]  reward=+0.0573  r_ca=+0.0178  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.607479   -0.83764535]  reward=+0.0573  r_ca=+0.0178  r_close=+0.9671  Δd*=-888.0\n",
            "  a=[ 0.74881744 -1.        ]  reward=+0.0573  r_ca=+0.0178  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.7225296  -0.96173066]  reward=+0.0573  r_ca=+0.0178  r_close=+0.9671  Δd*=-888.0\n",
            "  a=[ 0.7629245 -1.       ]  reward=+0.0573  r_ca=+0.0178  r_close=+0.9671  Δd*=-888.0\n",
            "  a=[ 0.7109196 -1.       ]  reward=+0.0573  r_ca=+0.0178  r_close=+0.9671  Δd*=-887.9\n",
            "\n",
            "Top actions by geometry shaping r_ca:\n",
            "  a=[ 0.7233565  -0.98208445]  r_ca=+0.0178  reward=+0.0573  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.73199487 -1.        ]  r_ca=+0.0178  reward=+0.0573  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.73421454 -1.        ]  r_ca=+0.0178  reward=+0.0573  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.7312211 -1.       ]  r_ca=+0.0178  reward=+0.0573  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.6129352  -0.84115505]  r_ca=+0.0178  reward=+0.0573  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.74881744 -1.        ]  r_ca=+0.0178  reward=+0.0573  r_close=+0.9671  Δd*=-888.1\n",
            "  a=[ 0.607479   -0.83764535]  r_ca=+0.0178  reward=+0.0573  r_close=+0.9671  Δd*=-888.0\n",
            "  a=[ 0.7225296  -0.96173066]  r_ca=+0.0178  reward=+0.0573  r_close=+0.9671  Δd*=-888.0\n",
            "  a=[ 0.7629245 -1.       ]  r_ca=+0.0178  reward=+0.0573  r_close=+0.9671  Δd*=-888.0\n",
            "  a=[ 0.7109196 -1.       ]  r_ca=+0.0178  reward=+0.0573  r_close=+0.9671  Δd*=-887.9\n",
            "\n",
            "=== SUMMARY ACROSS INITIAL STATES ===\n",
            "seed=  0 | frac_close>0=1.00 | frac_ca>0=0.48 | reward(p90-p50)=0.0111 | r_ca(p90-p50)=0.0220\n",
            "seed=  1 | frac_close>0=1.00 | frac_ca>0=0.49 | reward(p90-p50)=0.0051 | r_ca(p90-p50)=0.0101\n",
            "seed=  2 | frac_close>0=1.00 | frac_ca>0=0.51 | reward(p90-p50)=0.0088 | r_ca(p90-p50)=0.0175\n",
            "seed=  3 | frac_close>0=1.00 | frac_ca>0=0.51 | reward(p90-p50)=0.0144 | r_ca(p90-p50)=0.0287\n",
            "seed=  4 | frac_close>0=1.00 | frac_ca>0=0.50 | reward(p90-p50)=0.0057 | r_ca(p90-p50)=0.0111\n",
            "seed=  5 | frac_close>0=1.00 | frac_ca>0=0.49 | reward(p90-p50)=0.0118 | r_ca(p90-p50)=0.0235\n",
            "seed=  6 | frac_close>0=1.00 | frac_ca>0=0.51 | reward(p90-p50)=0.0107 | r_ca(p90-p50)=0.0212\n",
            "seed=  7 | frac_close>0=1.00 | frac_ca>0=0.52 | reward(p90-p50)=0.0093 | r_ca(p90-p50)=0.0185\n",
            "seed=  8 | frac_close>0=1.00 | frac_ca>0=0.48 | reward(p90-p50)=0.0044 | r_ca(p90-p50)=0.0087\n",
            "seed=  9 | frac_close>0=1.00 | frac_ca>0=0.48 | reward(p90-p50)=0.0084 | r_ca(p90-p50)=0.0168\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'seed': 0,\n",
              "  'frac_close_pos': 1.0,\n",
              "  'frac_ca_pos': 0.47998046875,\n",
              "  'reward_p90': 0.057961663303557134,\n",
              "  'reward_p50': 0.04690812801554631,\n",
              "  'reward_gap_p90_p50': 0.011053535288010827,\n",
              "  'ca_gap_p90_p50': 0.0219626521167243},\n",
              " {'seed': 1,\n",
              "  'frac_close_pos': 1.0,\n",
              "  'frac_ca_pos': 0.4931640625,\n",
              "  'reward_p90': 0.05299899088856686,\n",
              "  'reward_p50': 0.04788358455548042,\n",
              "  'reward_gap_p90_p50': 0.005115406333086443,\n",
              "  'ca_gap_p90_p50': 0.010118706780408469},\n",
              " {'seed': 2,\n",
              "  'frac_close_pos': 1.0,\n",
              "  'frac_ca_pos': 0.5068359375,\n",
              "  'reward_p90': 0.05571018787214409,\n",
              "  'reward_p50': 0.04686736244438779,\n",
              "  'reward_gap_p90_p50': 0.008842825427756294,\n",
              "  'ca_gap_p90_p50': 0.017492544198548596},\n",
              " {'seed': 3,\n",
              "  'frac_close_pos': 1.0,\n",
              "  'frac_ca_pos': 0.50927734375,\n",
              "  'reward_p90': 0.06345264024546382,\n",
              "  'reward_p50': 0.049075707932783263,\n",
              "  'reward_gap_p90_p50': 0.014376932312680557,\n",
              "  'ca_gap_p90_p50': 0.028704914869653174},\n",
              " {'seed': 4,\n",
              "  'frac_close_pos': 1.0,\n",
              "  'frac_ca_pos': 0.49658203125,\n",
              "  'reward_p90': 0.05199661322367286,\n",
              "  'reward_p50': 0.04634327191126223,\n",
              "  'reward_gap_p90_p50': 0.005653341312410631,\n",
              "  'ca_gap_p90_p50': 0.011084349543763593},\n",
              " {'seed': 5,\n",
              "  'frac_close_pos': 1.0,\n",
              "  'frac_ca_pos': 0.49267578125,\n",
              "  'reward_p90': 0.05913377196974251,\n",
              "  'reward_p50': 0.04729237296202746,\n",
              "  'reward_gap_p90_p50': 0.011841399007715053,\n",
              "  'ca_gap_p90_p50': 0.023523316978168986},\n",
              " {'seed': 6,\n",
              "  'frac_close_pos': 1.0,\n",
              "  'frac_ca_pos': 0.505859375,\n",
              "  'reward_p90': 0.0592587694595389,\n",
              "  'reward_p50': 0.048606261494739046,\n",
              "  'reward_gap_p90_p50': 0.010652507964799857,\n",
              "  'ca_gap_p90_p50': 0.02121666568014115},\n",
              " {'seed': 7,\n",
              "  'frac_close_pos': 1.0,\n",
              "  'frac_ca_pos': 0.52001953125,\n",
              "  'reward_p90': 0.05707869484788134,\n",
              "  'reward_p50': 0.047754138450235406,\n",
              "  'reward_gap_p90_p50': 0.009324556397645935,\n",
              "  'ca_gap_p90_p50': 0.01849685478948425},\n",
              " {'seed': 8,\n",
              "  'frac_close_pos': 1.0,\n",
              "  'frac_ca_pos': 0.478515625,\n",
              "  'reward_p90': 0.05156286897136654,\n",
              "  'reward_p50': 0.047159534532734804,\n",
              "  'reward_gap_p90_p50': 0.004403334438631737,\n",
              "  'ca_gap_p90_p50': 0.00867305606604467},\n",
              " {'seed': 9,\n",
              "  'frac_close_pos': 1.0,\n",
              "  'frac_ca_pos': 0.4833984375,\n",
              "  'reward_p90': 0.056470845645936914,\n",
              "  'reward_p50': 0.0480318847674122,\n",
              "  'reward_gap_p90_p50': 0.008438960878524714,\n",
              "  'ca_gap_p90_p50': 0.01677663872238212}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ========== STEP-0 ACTION SENSITIVITY TEST ==========\n",
        "# Snapshot/restore env so each action starts from the same state.\n",
        "\n",
        "def get_env_state(env):\n",
        "    return {\n",
        "        \"enemy_pos\": env.enemy_pos.copy(),\n",
        "        \"defense_pos\": env.defense_pos.copy(),\n",
        "        \"enemy_vel\": env.enemy_vel.copy(),\n",
        "        \"defense_vel\": env.defense_vel.copy(),\n",
        "        \"t\": float(env.t),\n",
        "        \"done\": bool(getattr(env, \"done\", False)),\n",
        "        \"success\": bool(getattr(env, \"success\", False)),\n",
        "        \"min_dist\": float(getattr(env, \"min_dist\", np.inf)),\n",
        "        \"ep_min_dist\": float(getattr(env, \"ep_min_dist\", np.inf)),\n",
        "        \"ep_max_action_mag\": float(getattr(env, \"ep_max_action_mag\", 0.0)),\n",
        "        \"time_to_hit\": None if getattr(env, \"time_to_hit\", None) is None else float(env.time_to_hit),\n",
        "        \"terminal_event\": str(getattr(env, \"terminal_event\", \"running\")),\n",
        "        \"enemy_x\": float(getattr(env, \"enemy_x\", env.enemy_pos[0])),\n",
        "        \"enemy_y\": float(getattr(env, \"enemy_y\", env.enemy_pos[1])),\n",
        "        \"enemy_z\": float(getattr(env, \"enemy_z\", env.enemy_pos[2])),\n",
        "        \"defense_x\": float(getattr(env, \"defense_x\", env.defense_pos[0])),\n",
        "        \"defense_y\": float(getattr(env, \"defense_y\", env.defense_pos[1])),\n",
        "        \"defense_z\": float(getattr(env, \"defense_z\", env.defense_pos[2])),\n",
        "    }\n",
        "\n",
        "def set_env_state(env, S):\n",
        "    env.enemy_pos = S[\"enemy_pos\"].copy()\n",
        "    env.defense_pos = S[\"defense_pos\"].copy()\n",
        "    env.enemy_vel = S[\"enemy_vel\"].copy()\n",
        "    env.defense_vel = S[\"defense_vel\"].copy()\n",
        "    env.t = float(S[\"t\"])\n",
        "    env.done = bool(S[\"done\"])\n",
        "    env.success = bool(S[\"success\"])\n",
        "    env.min_dist = float(S[\"min_dist\"])\n",
        "    env.ep_min_dist = float(S[\"ep_min_dist\"])\n",
        "    env.ep_max_action_mag = float(S[\"ep_max_action_mag\"])\n",
        "    env.time_to_hit = S[\"time_to_hit\"]\n",
        "    env.terminal_event = S[\"terminal_event\"]\n",
        "    env.enemy_x, env.enemy_y, env.enemy_z = S[\"enemy_x\"], S[\"enemy_y\"], S[\"enemy_z\"]\n",
        "    env.defense_x, env.defense_y, env.defense_z = S[\"defense_x\"], S[\"defense_y\"], S[\"defense_z\"]\n",
        "\n",
        "\n",
        "def eval_one_step(env, action, v_scale=1500.0):\n",
        "    phi_b, dstar_b, tstar_b = env._phi_closest_approach()\n",
        "    obs2, reward, terminated, truncated, info = env.step(action)\n",
        "    done = bool(terminated or truncated)\n",
        "    phi_a, dstar_a, tstar_a = env._phi_closest_approach()\n",
        "    r_ca = float(info.get(\"r_ca\", np.nan))\n",
        "    r_close = float(info.get(\"r_close\", np.nan))\n",
        "    return {\n",
        "        \"reward\": float(reward),\n",
        "        \"r_ca\": r_ca,\n",
        "        \"r_close\": r_close,\n",
        "        \"done\": done,\n",
        "        \"event\": str(info.get(\"event\", \"unknown\")),\n",
        "        \"dstar_before\": float(dstar_b),\n",
        "        \"dstar_after\": float(dstar_a),\n",
        "        \"ddstar\": float(dstar_a - dstar_b),\n",
        "        \"tstar_before\": float(tstar_b),\n",
        "        \"tstar_after\": float(tstar_a),\n",
        "        \"phi_before\": float(phi_b),\n",
        "        \"phi_after\": float(phi_a),\n",
        "    }\n",
        "\n",
        "\n",
        "def sample_actions_like_initial_ppo(n, sigma=1.0, rng=None):\n",
        "    if rng is None:\n",
        "        rng = np.random.RandomState(0)\n",
        "    a = rng.randn(n, 2).astype(np.float32) * float(sigma)\n",
        "    a = np.clip(a, -1.0, 1.0)\n",
        "    return a\n",
        "\n",
        "\n",
        "def scan_initial_state(env, n_actions=4096, sigma=1.0, seed=0, v_scale=1500.0, topk=10):\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "    S0 = get_env_state(env)\n",
        "    rng = np.random.RandomState(seed + 12345)\n",
        "    actions = sample_actions_like_initial_ppo(n_actions, sigma=sigma, rng=rng)\n",
        "    rewards = np.zeros(n_actions, dtype=np.float64)\n",
        "    r_ca = np.zeros(n_actions, dtype=np.float64)\n",
        "    r_close = np.zeros(n_actions, dtype=np.float64)\n",
        "    ddstar = np.zeros(n_actions, dtype=np.float64)\n",
        "    done = np.zeros(n_actions, dtype=bool)\n",
        "    for i in range(n_actions):\n",
        "        set_env_state(env, S0)\n",
        "        out = eval_one_step(env, actions[i], v_scale=v_scale)\n",
        "        rewards[i] = out[\"reward\"]\n",
        "        r_ca[i] = out[\"r_ca\"]\n",
        "        r_close[i] = out[\"r_close\"]\n",
        "        ddstar[i] = out[\"ddstar\"]\n",
        "        done[i] = out[\"done\"]\n",
        "    def q(x, p): return float(np.percentile(x, p))\n",
        "    frac_close_pos = float(np.mean(r_close > 0))\n",
        "    frac_ca_pos = float(np.mean(r_ca > 0))\n",
        "    frac_ca_pos_given_close = float(np.mean((r_ca > 0) & (r_close > 0)) / max(np.mean(r_close > 0), 1e-9))\n",
        "    print(\"\\n=== STEP-0 ACTION SCAN ===\")\n",
        "    print(f\"seed={seed} | n_actions={n_actions} | sigma={sigma}\")\n",
        "    print(f\"frac(r_close>0)={frac_close_pos:.3f}\")\n",
        "    print(f\"frac(r_ca>0)={frac_ca_pos:.3f}\")\n",
        "    print(f\"frac(r_ca>0 | r_close>0)={frac_ca_pos_given_close:.3f}\")\n",
        "    print(\"reward:   p10={:.4f}  p50={:.4f}  p90={:.4f}  max={:.4f}\".format(q(rewards,10), q(rewards,50), q(rewards,90), float(rewards.max())))\n",
        "    print(\"r_close:  p10={:.4f}  p50={:.4f}  p90={:.4f}  max={:.4f}\".format(q(r_close,10), q(r_close,50), q(r_close,90), float(r_close.max())))\n",
        "    print(\"r_ca:     p10={:.4f}  p50={:.4f}  p90={:.4f}  max={:.4f}\".format(q(r_ca,10), q(r_ca,50), q(r_ca,90), float(r_ca.max())))\n",
        "    print(\"Δd*:      p10={:.2f}  p50={:.2f}  p90={:.2f}  min={:.2f}\".format(q(ddstar,10), q(ddstar,50), q(ddstar,90), float(ddstar.min())))\n",
        "    idx_best_reward = np.argsort(-rewards)[:topk]\n",
        "    idx_best_ca = np.argsort(-r_ca)[:topk]\n",
        "    print(\"\\nTop actions by total reward:\")\n",
        "    for j in idx_best_reward:\n",
        "        print(f\"  a={actions[j]}  reward={rewards[j]:+.4f}  r_ca={r_ca[j]:+.4f}  r_close={r_close[j]:+.4f}  Δd*={ddstar[j]:+.1f}\")\n",
        "    print(\"\\nTop actions by geometry shaping r_ca:\")\n",
        "    for j in idx_best_ca:\n",
        "        print(f\"  a={actions[j]}  r_ca={r_ca[j]:+.4f}  reward={rewards[j]:+.4f}  r_close={r_close[j]:+.4f}  Δd*={ddstar[j]:+.1f}\")\n",
        "    set_env_state(env, S0)\n",
        "    return {\"actions\": actions, \"reward\": rewards, \"r_ca\": r_ca, \"r_close\": r_close, \"ddstar\": ddstar, \"done\": done}\n",
        "\n",
        "\n",
        "def scan_many_initial_states(env, seeds, n_actions=2048, sigma=1.0):\n",
        "    stats = []\n",
        "    for s in seeds:\n",
        "        out = scan_initial_state(env, n_actions=n_actions, sigma=sigma, seed=s)\n",
        "        stats.append({\n",
        "            \"seed\": s,\n",
        "            \"frac_close_pos\": float(np.mean(out[\"r_close\"] > 0)),\n",
        "            \"frac_ca_pos\": float(np.mean(out[\"r_ca\"] > 0)),\n",
        "            \"reward_p90\": float(np.percentile(out[\"reward\"], 90)),\n",
        "            \"reward_p50\": float(np.percentile(out[\"reward\"], 50)),\n",
        "            \"reward_gap_p90_p50\": float(np.percentile(out[\"reward\"], 90) - np.percentile(out[\"reward\"], 50)),\n",
        "            \"ca_gap_p90_p50\": float(np.percentile(out[\"r_ca\"], 90) - np.percentile(out[\"r_ca\"], 50)),\n",
        "        })\n",
        "    print(\"\\n=== SUMMARY ACROSS INITIAL STATES ===\")\n",
        "    for row in stats:\n",
        "        print(f\"seed={row['seed']:>3d} | frac_close>0={row['frac_close_pos']:.2f} | frac_ca>0={row['frac_ca_pos']:.2f} | reward(p90-p50)={row['reward_gap_p90_p50']:.4f} | r_ca(p90-p50)={row['ca_gap_p90_p50']:.4f}\")\n",
        "    return stats\n",
        "\n",
        "\n",
        "# ---------- K-step scan: 1 step with candidate action, then K-1 steps ProNav ----------\n",
        "def eval_one_step_then_pronav_k_steps(env, action, K, v_scale=1500.0):\n",
        "    \"\"\"Take action for 1 step, then ProNav for K-1 steps. Caller must restore S0 before each call.\"\"\"\n",
        "    total_reward = 0.0\n",
        "    obs, reward, term, trunc, info = env.step(action)\n",
        "    total_reward += float(reward)\n",
        "    r_ca_first = float(info.get(\"r_ca\", np.nan))\n",
        "    done = bool(term or trunc)\n",
        "    steps_taken = 1\n",
        "    while not done and steps_taken < K:\n",
        "        a_pronav = env.calculate_pronav()\n",
        "        obs, reward, term, trunc, info = env.step(a_pronav)\n",
        "        total_reward += float(reward)\n",
        "        done = bool(term or trunc)\n",
        "        steps_taken += 1\n",
        "    min_dist = float(getattr(env, \"ep_min_dist\", np.inf))\n",
        "    _, dstar_final, _ = env._phi_closest_approach()\n",
        "    return {\n",
        "        \"total_reward\": total_reward,\n",
        "        \"min_dist\": min_dist,\n",
        "        \"dstar_final\": float(dstar_final),\n",
        "        \"done\": done,\n",
        "        \"event\": str(info.get(\"event\", \"unknown\")),\n",
        "        \"steps_taken\": steps_taken,\n",
        "        \"r_ca_first\": r_ca_first,\n",
        "    }\n",
        "\n",
        "\n",
        "def scan_initial_state_k_steps(env, n_actions=2048, K=20, sigma=1.0, seed=0, topk=10, v_scale=1500.0):\n",
        "    \"\"\"Same as step-0 scan but evaluate each action by: 1 step action + (K-1) steps ProNav. Report min_dist, d*, return.\"\"\"\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "    S0 = get_env_state(env)\n",
        "    rng = np.random.RandomState(seed + 12345)\n",
        "    actions = sample_actions_like_initial_ppo(n_actions, sigma=sigma, rng=rng)\n",
        "    total_reward = np.zeros(n_actions, dtype=np.float64)\n",
        "    min_dist = np.zeros(n_actions, dtype=np.float64)\n",
        "    dstar_final = np.zeros(n_actions, dtype=np.float64)\n",
        "    r_ca_first = np.zeros(n_actions, dtype=np.float64)\n",
        "    r_ca_first[:] = np.nan\n",
        "    done_arr = np.zeros(n_actions, dtype=bool)\n",
        "    event_counts = {}\n",
        "    for i in range(n_actions):\n",
        "        set_env_state(env, S0)\n",
        "        out = eval_one_step_then_pronav_k_steps(env, actions[i], K=K, v_scale=v_scale)\n",
        "        total_reward[i] = out[\"total_reward\"]\n",
        "        min_dist[i] = out[\"min_dist\"]\n",
        "        dstar_final[i] = out[\"dstar_final\"]\n",
        "        done_arr[i] = out[\"done\"]\n",
        "        r_ca_first[i] = out[\"r_ca_first\"]\n",
        "        e = out[\"event\"]\n",
        "        event_counts[e] = event_counts.get(e, 0) + 1\n",
        "    def q(x, p): return float(np.percentile(x, p))\n",
        "    set_env_state(env, S0)\n",
        "    print(\"\\n=== K-STEP ACTION SCAN (1 step candidate + {} steps ProNav) ===\".format(K - 1))\n",
        "    print(\"seed={} | n_actions={} | K={} | sigma={}\".format(seed, n_actions, K, sigma))\n",
        "    print(\"Terminal events:\", event_counts)\n",
        "    print(\"total_reward: p10={:.4f}  p50={:.4f}  p90={:.4f}  max={:.4f}\".format(q(total_reward, 10), q(total_reward, 50), q(total_reward, 90), float(total_reward.max())))\n",
        "    print(\"min_dist:     p10={:.1f}  p50={:.1f}  p90={:.1f}  min={:.1f}\".format(q(min_dist, 10), q(min_dist, 50), q(min_dist, 90), float(min_dist.min())))\n",
        "    print(\"dstar_final:  p10={:.1f}  p50={:.1f}  p90={:.1f}  min={:.1f}\".format(q(dstar_final, 10), q(dstar_final, 50), q(dstar_final, 90), float(dstar_final.min())))\n",
        "    idx_best_return = np.argsort(-total_reward)[:topk]\n",
        "    idx_best_min_dist = np.argsort(min_dist)[:topk]\n",
        "    idx_best_dstar = np.argsort(dstar_final)[:topk]\n",
        "    print(\"\\nTop actions by total reward (1+K steps):\")\n",
        "    for j in idx_best_return:\n",
        "        print(\"  a={}  total_reward={:+.4f}  min_dist={:.1f}  d*={:.1f}\".format(actions[j], total_reward[j], min_dist[j], dstar_final[j]))\n",
        "    print(\"\\nTop actions by min_dist (lower is better):\")\n",
        "    for j in idx_best_min_dist:\n",
        "        print(\"  a={}  min_dist={:.1f}  total_reward={:+.4f}  d*={:.1f}\".format(actions[j], min_dist[j], total_reward[j], dstar_final[j]))\n",
        "    valid_ca = np.isfinite(r_ca_first)\n",
        "    if np.any(valid_ca):\n",
        "        corr_ca_min = np.corrcoef(r_ca_first[valid_ca], min_dist[valid_ca])[0, 1] if np.sum(valid_ca) > 1 else np.nan\n",
        "        print(\"\\nCorrelation(r_ca at step 0, min_dist after K steps): {:.4f} (negative = good r_ca -> lower miss)\".format(corr_ca_min if np.isfinite(corr_ca_min) else np.nan))\n",
        "    set_env_state(env, S0)\n",
        "    return {\"actions\": actions, \"total_reward\": total_reward, \"min_dist\": min_dist, \"dstar_final\": dstar_final, \"r_ca_first\": r_ca_first, \"done\": done_arr}\n",
        "\n",
        "\n",
        "# Run: single initial state\n",
        "env = missile_interception_3d()\n",
        "scan_initial_state(env, n_actions=4096, sigma=1.0, seed=0)\n",
        "\n",
        "# Run: many initial states (full output per seed; summary at end)\n",
        "scan_many_initial_states(env, seeds=list(range(10)), n_actions=2048, sigma=1.0)\n",
        "\n",
        "# K-step scan: do best step-0 actions stay good after 2s? (1 step candidate + 19 steps ProNav)\n",
        "scan_initial_state_k_steps(env, n_actions=2048, K=20, sigma=1.0, seed=0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
