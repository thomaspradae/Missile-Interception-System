{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906a9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "class missile_interception_3d(gym.Env):\n",
    "    def __init__(self):\n",
    "        # 1. Define Action Space (The Joystick: Left/Right, Up/Down)\n",
    "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2, ), dtype=np.float32)\n",
    "        \n",
    "        # 2. Define Observation Space (20D ego-frame version)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(20,), dtype=np.float32)\n",
    "\n",
    "        self.np_random = np.random.RandomState()\n",
    "        \n",
    "        # 3. Time Settings\n",
    "        self.dt_act = 0.1             \n",
    "        self.n_substeps = 10          \n",
    "        self.dt_sim = self.dt_act / self.n_substeps \n",
    "        self.t_max = 650.0            \n",
    "\n",
    "        # 4. Physical Limits\n",
    "        self.a_max = 350.0   # Max G-force (m/s^2) ~35G\n",
    "        self.da_max = 2500.0 # Jerk Limit (m/s^3)\n",
    "        self.tau = 0.05      # Airframe Lag\n",
    "        self.g = 9.81        \n",
    "        self.collision_radius = 150.0  \n",
    "        self.max_distance = 4_000_000.0 \n",
    "\n",
    "        self.p_easy = 1.0                   \n",
    "        self.range_min = 70_000.0           \n",
    "        self.range_easy_max = 200_000.0     \n",
    "        self.range_hard_max = 1_000_000.0   \n",
    "\n",
    "        self.targetbox_x_min = -15000\n",
    "        self.targetbox_x_max = 15000\n",
    "        self.targetbox_y_min = -15000\n",
    "        self.targetbox_y_max = 15000\n",
    "\n",
    "        # ----------------------------\n",
    "        # Potential-based shaping (ZEM_perp)\n",
    "        # IMPORTANT: gamma_shape MUST match PPO's gamma\n",
    "        # ----------------------------\n",
    "        self.gamma_shape = 0.9999      # set to your PPO gamma\n",
    "        self.w_zem = 1.0             # shaping weight\n",
    "        self.zem_scale = 50_000.0    # meters; tunes magnitude of phi\n",
    "        self.tgo_max = 15.0          # seconds; clamp lookahead when closing\n",
    "        self.tgo_fixed = 3.0         # seconds; lookahead when NOT closing\n",
    "        self.vc_min = 1.0            # m/s; treat <= this as \"not closing\"\n",
    "\n",
    "    def generate_enemy_missile(self):\n",
    "        if self.np_random.rand() < self.p_easy:\n",
    "            self.range_max_used = self.range_easy_max\n",
    "        else:\n",
    "            self.range_max_used = self.range_hard_max\n",
    "\n",
    "        range_min = self.range_min\n",
    "        self.attack_target_x = self.np_random.uniform(self.targetbox_x_min, self.targetbox_x_max)\n",
    "        self.attack_target_y = self.np_random.uniform(self.targetbox_y_min, self.targetbox_y_max)\n",
    "        self.enemy_launch_angle = self.np_random.uniform(0, 2 * np.pi)\n",
    "        self.enemy_theta = self.np_random.uniform(0.523599, 1.0472) \n",
    "\n",
    "        self.range_max_used = max(self.range_max_used, range_min + 1.0)\n",
    "        lower_limit = np.sqrt((range_min * self.g) / np.sin(2 * self.enemy_theta))\n",
    "        upper_limit = np.sqrt((self.range_max_used * self.g) / np.sin(2 * self.enemy_theta))\n",
    "        self.enemy_initial_velocity = self.np_random.uniform(lower_limit, upper_limit)\n",
    "\n",
    "        ground_range = (\n",
    "            self.enemy_initial_velocity * np.cos(self.enemy_theta)\n",
    "            * (2 * self.enemy_initial_velocity * np.sin(self.enemy_theta) / self.g)\n",
    "        )\n",
    "\n",
    "        self.enemy_launch_x = self.attack_target_x + ground_range * np.cos(self.enemy_launch_angle)\n",
    "        self.enemy_launch_y = self.attack_target_y + ground_range * np.sin(self.enemy_launch_angle)\n",
    "        self.enemy_z = 0\n",
    "        self.enemy_x = self.enemy_launch_x\n",
    "        self.enemy_y = self.enemy_launch_y\n",
    "        self.enemy_pos = np.array([self.enemy_x, self.enemy_y, self.enemy_z], dtype=np.float32)\n",
    "        self.enemy_azimuth = (self.enemy_launch_angle + np.pi) % (2 * np.pi)\n",
    "\n",
    "    def generate_defense_missile(self):\n",
    "        self.defense_launch_x = self.np_random.uniform(self.targetbox_x_min, self.targetbox_x_max)\n",
    "        self.defense_launch_y = self.np_random.uniform(self.targetbox_y_min, self.targetbox_y_max)\n",
    "\n",
    "        dx = self.enemy_launch_x - self.defense_launch_x\n",
    "        dy = self.enemy_launch_y - self.defense_launch_y\n",
    "        az_nominal = np.arctan2(dy, dx)\n",
    "\n",
    "        # --- Misalignment (domain randomization of initial heading) ---\n",
    "        # Mixture: most episodes small error, some episodes large az error\n",
    "        p_misaligned = 0.35  # 35% \"hard\" starts\n",
    "        if self.np_random.rand() < p_misaligned:\n",
    "            # Hard: big azimuth error → strong RIGHT required\n",
    "            az_noise = self.np_random.uniform(-np.deg2rad(60.0), np.deg2rad(60.0))\n",
    "        else:\n",
    "            # Easy: small azimuth error → gentle correction\n",
    "            az_noise = self.np_random.uniform(-np.deg2rad(10.0), np.deg2rad(10.0))\n",
    "\n",
    "        self.defense_azimuth = az_nominal + az_noise\n",
    "\n",
    "        # Elevation noise: avoid always same vertical plane\n",
    "        theta_nominal = 0.785398  # ~45 deg\n",
    "        theta_noise_deg = 10.0\n",
    "        theta_noise = self.np_random.uniform(\n",
    "            -np.deg2rad(theta_noise_deg),\n",
    "            +np.deg2rad(theta_noise_deg),\n",
    "        )\n",
    "        self.defense_theta = float(np.clip(theta_nominal + theta_noise,\n",
    "                                           np.deg2rad(10.0),\n",
    "                                           np.deg2rad(80.0)))\n",
    "        # ---------------------------------------------------------------\n",
    "\n",
    "        base_velocity = 3000.0\n",
    "        if hasattr(self, 'range_max_used'):\n",
    "            velocity_scale = min(self.range_max_used / self.range_easy_max, 1.5)\n",
    "            self.defense_initial_velocity = base_velocity * velocity_scale\n",
    "        else:\n",
    "            self.defense_initial_velocity = base_velocity\n",
    "\n",
    "        self.defense_x = self.defense_launch_x\n",
    "        self.defense_y = self.defense_launch_y\n",
    "        self.defense_z = 0.0\n",
    "        self.defense_pos = np.array([self.defense_x, self.defense_y, self.defense_z], dtype=np.float32)\n",
    "\n",
    "        self.defense_ax = 0.0\n",
    "        self.defense_ay = 0.0\n",
    "        self.defense_az = 0.0\n",
    "    \n",
    "    def _smoothstep(self, x: float) -> float:\n",
    "        \"\"\"Smooth ramp 0->1 with zero slope at ends, clamps outside [0,1]\"\"\"\n",
    "        x = float(np.clip(x, 0.0, 1.0))\n",
    "        return x * x * (3.0 - 2.0 * x)\n",
    "    \n",
    "    def calculate_pronav(self):\n",
    "        eps = 1e-9\n",
    "\n",
    "        # Relative geometry (use float64 for stability)\n",
    "        r = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
    "        v = self.defense_vel.astype(np.float64)\n",
    "        vrel = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
    "\n",
    "        R = float(np.linalg.norm(r)) + eps\n",
    "        V = float(np.linalg.norm(v)) + eps\n",
    "\n",
    "        rhat = r / R\n",
    "        vhat = v / V\n",
    "\n",
    "        # Heading error alpha = angle between velocity direction and LOS direction\n",
    "        cosang = float(np.clip(np.dot(vhat, rhat), -1.0, 1.0))\n",
    "        alpha = float(np.arccos(cosang))  # radians\n",
    "\n",
    "        # LOS angular rate omega (world frame)\n",
    "        omega = np.cross(r, vrel) / (float(np.dot(r, r)) + eps)\n",
    "        omega_mag = float(np.linalg.norm(omega))\n",
    "\n",
    "        # Closing speed (positive => closing)\n",
    "        vc = -float(np.dot(r, vrel)) / R\n",
    "\n",
    "        # --- PN term ---\n",
    "        N = 3.0\n",
    "        a_pn = N * vc * np.cross(omega, rhat)  # lateral accel in world frame\n",
    "\n",
    "        # --- Acquisition term (turn-to-LOS) ---\n",
    "        # Perpendicular component of LOS relative to forward direction\n",
    "        rhat_perp = rhat - float(np.dot(rhat, vhat)) * vhat\n",
    "        nperp = float(np.linalg.norm(rhat_perp))\n",
    "\n",
    "        if nperp < 1e-8:\n",
    "            a_acq = np.zeros(3, dtype=np.float64)\n",
    "        else:\n",
    "            rhat_perp /= nperp  # unit sideways \"turn toward LOS\" direction\n",
    "\n",
    "            # Curvature-based magnitude: ~k * V^2 / R, saturate later via a_max\n",
    "            k_acq = 5.0  # try 3.0–8.0\n",
    "            a_acq = k_acq * (V * V / R) * rhat_perp\n",
    "\n",
    "        # --- Blend weight w: 0 => pure PN, 1 => pure acquisition ---\n",
    "\n",
    "        # Alpha-based weight (dominant)\n",
    "        alpha_on   = np.deg2rad(20.0)   # start blending earlier\n",
    "        alpha_full = np.deg2rad(55.0)\n",
    "\n",
    "        x_alpha = (alpha - alpha_on) / (alpha_full - alpha_on + eps)\n",
    "        w_alpha = self._smoothstep(x_alpha)\n",
    "\n",
    "        # Omega-based modifier (only boosts acquisition when PN is sleepy)\n",
    "        omega_full = 0.00\n",
    "        omega_on   = 0.05   # <-- key: less brittle than 0.02\n",
    "\n",
    "        x_omega = (omega_on - omega_mag) / (omega_on - omega_full + eps)\n",
    "        w_omega = self._smoothstep(x_omega)\n",
    "\n",
    "        # Robust combine: alpha dominates; omega can't fully shut it off\n",
    "        w = w_alpha * (0.25 + 0.75 * w_omega)\n",
    "\n",
    "        # Optional: if not closing, force strong acquisition\n",
    "        if vc <= 0.0:\n",
    "            w = max(w, 0.9)\n",
    "\n",
    "        a_ideal = (1.0 - w) * a_pn + w * a_acq\n",
    "\n",
    "        # Project into your lateral control basis (right/up) and normalize by a_max\n",
    "        # Note: Environment now handles gravity compensation internally,\n",
    "        # so ProNav outputs desired NET lateral accel (same semantics as PPO)\n",
    "        forward, right, up = self._compute_lateral_basis(self.defense_vel)\n",
    "        a_right = float(np.dot(a_ideal, right))\n",
    "        a_up    = float(np.dot(a_ideal, up))\n",
    "\n",
    "        action = np.array([a_right / self.a_max, a_up / self.a_max], dtype=np.float32)\n",
    "        return np.clip(action, -1.0, 1.0)\n",
    "    \n",
    "    def _rate_limit_norm(self, a_cmd, a_prev, da_max, dt):\n",
    "        delta = a_cmd - a_prev\n",
    "        max_delta = da_max * dt\n",
    "        dnorm = float(np.linalg.norm(delta))\n",
    "        if dnorm <= max_delta or dnorm < 1e-9:\n",
    "            return a_cmd\n",
    "        return a_prev + delta * (max_delta / dnorm)\n",
    "    \n",
    "    def _segment_sphere_intersect(self, r0, r1, r_hit):\n",
    "        dr = r1 - r0\n",
    "        dr_norm_sq = float(np.dot(dr, dr))\n",
    "        if dr_norm_sq < 1e-12:\n",
    "            return float(np.dot(r0, r0)) <= r_hit * r_hit\n",
    "        s_star = -float(np.dot(r0, dr)) / dr_norm_sq\n",
    "        s_star = max(0.0, min(1.0, s_star))\n",
    "        r_closest = r0 + s_star * dr\n",
    "        return float(np.dot(r_closest, r_closest)) <= r_hit * r_hit\n",
    "    \n",
    "    def _phi_zem_perp(self):\n",
    "        \"\"\"\n",
    "        Potential for potential-based shaping:\n",
    "          Phi(s) = - ||ZEM_perp|| / zem_scale\n",
    "\n",
    "        Returns:\n",
    "          phi, zem_perp_norm, Vc, tgo\n",
    "        \"\"\"\n",
    "        eps = 1e-9\n",
    "\n",
    "        r = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
    "        vrel = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
    "\n",
    "        R = float(np.linalg.norm(r)) + eps\n",
    "        rhat = r / R\n",
    "\n",
    "        # Closing speed (positive = closing)\n",
    "        Vc = -float(np.dot(rhat, vrel))\n",
    "\n",
    "        if Vc > self.vc_min:\n",
    "            tgo = R / max(Vc, eps)\n",
    "            tgo = float(np.clip(tgo, 0.0, self.tgo_max))\n",
    "        else:\n",
    "            tgo = float(self.tgo_fixed)\n",
    "\n",
    "        zem = r + vrel * tgo\n",
    "        zem_perp = zem - float(np.dot(zem, rhat)) * rhat\n",
    "        zem_perp_norm = float(np.linalg.norm(zem_perp))\n",
    "\n",
    "        phi = -zem_perp_norm / (float(self.zem_scale) + eps)\n",
    "        return float(phi), zem_perp_norm, float(Vc), tgo\n",
    "    \n",
    "    \n",
    "    def _get_obs(self):\n",
    "        eps = 1e-9\n",
    "\n",
    "        # World-frame relative state\n",
    "        r_world = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
    "        vrel_world = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
    "\n",
    "        # Local basis from defense velocity (world frame unit vectors)\n",
    "        forward, right, up = self._compute_lateral_basis(self.defense_vel)\n",
    "\n",
    "        # ===============================\n",
    "        # 1) Ego-frame (body-frame) r and vrel\n",
    "        # ===============================\n",
    "        r_body = np.array([\n",
    "            float(np.dot(r_world, forward)),\n",
    "            float(np.dot(r_world, right)),\n",
    "            float(np.dot(r_world, up)),\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        vrel_body = np.array([\n",
    "            float(np.dot(vrel_world, forward)),\n",
    "            float(np.dot(vrel_world, right)),\n",
    "            float(np.dot(vrel_world, up)),\n",
    "        ], dtype=np.float64)\n",
    "\n",
    "        # Normalize r_body / vrel_body (keep your original scaling)\n",
    "        pos_scale = float(self.range_hard_max)   # 1_000_000\n",
    "        vel_scale = 4000.0\n",
    "\n",
    "        r_body_n = (r_body / (pos_scale + eps)).astype(np.float32)\n",
    "        vrel_body_n = (vrel_body / (vel_scale + eps)).astype(np.float32)\n",
    "\n",
    "        # ===============================\n",
    "        # 2) Actuator state in the same action frame\n",
    "        # ===============================\n",
    "        a_lat = np.array([\n",
    "            float(np.dot(self.a_actual, right)) / (self.a_max + eps),\n",
    "            float(np.dot(self.a_actual, up)) / (self.a_max + eps),\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        # NEW: hidden actuator state that affects transitions\n",
    "        a_cmd_prev_lat = np.array([\n",
    "            float(np.dot(self.a_cmd_prev, right)) / (self.a_max + eps),\n",
    "            float(np.dot(self.a_cmd_prev, up)) / (self.a_max + eps),\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        # ===============================\n",
    "        # 3) Scalar helpers (kept)\n",
    "        # ===============================\n",
    "        dist = float(np.linalg.norm(r_world)) + 1e-6\n",
    "        v_close = -float(np.dot(r_world, vrel_world)) / dist  # positive when closing\n",
    "\n",
    "        dist_n = np.float32(np.clip(dist / 1_000_000.0, 0.0, 4.0))\n",
    "        vclose_n = np.float32(np.clip(v_close / 3000.0, -2.0, 2.0))\n",
    "        dist_vclose_feat = np.array([dist_n, vclose_n], dtype=np.float32)\n",
    "\n",
    "        # Defense own vertical state (keep for ground constraint)\n",
    "        def_z_n = np.float32(np.clip(self.defense_pos[2] / 100_000.0, -1.0, 2.0))\n",
    "        def_vz_n = np.float32(np.clip(self.defense_vel[2] / 3000.0, -2.0, 2.0))\n",
    "        def_state_feat = np.array([def_z_n, def_vz_n], dtype=np.float32)\n",
    "\n",
    "        # ===============================\n",
    "        # 4) Keep your geometry features (consistent with ego-frame)\n",
    "        # ===============================\n",
    "        dist_body = float(np.linalg.norm(r_body)) + 1e-6\n",
    "\n",
    "        # LOS lateral projections in body frame\n",
    "        los_right = float(r_body[1] / dist_body)\n",
    "        los_up    = float(r_body[2] / dist_body)\n",
    "\n",
    "        # LOS rate omega in body frame: omega = (r x vrel)/||r||^2\n",
    "        dist2_body = float(np.dot(r_body, r_body)) + eps\n",
    "        omega_body = np.cross(r_body, vrel_body) / dist2_body\n",
    "\n",
    "        omega_right = float(omega_body[1])\n",
    "        omega_up    = float(omega_body[2])\n",
    "\n",
    "        omega_scale = 2.0\n",
    "        omega_right_n = float(np.clip(omega_right / omega_scale, -2.0, 2.0))\n",
    "        omega_up_n    = float(np.clip(omega_up / omega_scale, -2.0, 2.0))\n",
    "\n",
    "        geom_feat = np.array([los_right, los_up, omega_right_n, omega_up_n], dtype=np.float32)\n",
    "\n",
    "        # ===============================\n",
    "        # 5) NEW: kinematics garnish\n",
    "        # ===============================\n",
    "        V_def = float(np.linalg.norm(self.defense_vel))\n",
    "        V_def_n = np.float32(np.clip(V_def / 3000.0, 0.0, 3.0))  # scale: 3000 m/s baseline\n",
    "        forward_z = np.float32(float(forward[2]))               # dot(forward, world_up) since world_up=[0,0,1]\n",
    "\n",
    "        kin_feat = np.array([V_def_n, forward_z], dtype=np.float32)\n",
    "\n",
    "        # Final obs (20D)\n",
    "        obs = np.concatenate(\n",
    "            [r_body_n, vrel_body_n, a_lat, a_cmd_prev_lat, dist_vclose_feat, def_state_feat, geom_feat, kin_feat],\n",
    "            axis=0\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # Optional sanity check while iterating\n",
    "        # assert obs.shape == (20,), obs.shape\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _compute_lateral_basis(self, velocity):\n",
    "        \"\"\"\n",
    "        Horizon-stable basis:\n",
    "          forward = along velocity\n",
    "          right   = world_up x forward  (horizontal right)\n",
    "          up      = forward x right     (completes orthonormal frame)\n",
    "        This keeps 'up' as close to world-up as possible and avoids weird twisting.\n",
    "        \"\"\"\n",
    "        speed = float(np.linalg.norm(velocity))\n",
    "        if speed < 1.0:\n",
    "            forward = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
    "        else:\n",
    "            forward = (velocity / speed).astype(np.float32)\n",
    "\n",
    "        world_up = np.array([0.0, 0.0, 1.0], dtype=np.float32)\n",
    "\n",
    "        # right = world_up x forward\n",
    "        right_raw = np.cross(world_up, forward)\n",
    "        rnorm = float(np.linalg.norm(right_raw))\n",
    "\n",
    "        # If forward is near world_up, right_raw ~ 0. Pick a consistent fallback.\n",
    "        if rnorm < 1e-6:\n",
    "            # Choose a fixed \"north\" axis in world XY and build right from that\n",
    "            # This prevents random spinning when vertical.\n",
    "            north = np.array([1.0, 0.0, 0.0], dtype=np.float32)\n",
    "            right_raw = np.cross(north, forward)\n",
    "            rnorm = float(np.linalg.norm(right_raw))\n",
    "            if rnorm < 1e-6:\n",
    "                north = np.array([0.0, 1.0, 0.0], dtype=np.float32)\n",
    "                right_raw = np.cross(north, forward)\n",
    "                rnorm = float(np.linalg.norm(right_raw))\n",
    "\n",
    "        right = (right_raw / (rnorm + 1e-9)).astype(np.float32)\n",
    "\n",
    "        # up = forward x right (not right x forward)\n",
    "        up_raw = np.cross(forward, right)\n",
    "        up = (up_raw / (float(np.linalg.norm(up_raw)) + 1e-9)).astype(np.float32)\n",
    "\n",
    "        return forward, right, up\n",
    "\n",
    "    def step(self, action):\n",
    "        if getattr(self, \"done\", False):\n",
    "            return self._get_obs(), 0.0, True, False, {\"event\": \"called_step_after_done\", \"dist\": self.relative_distances[-1]}\n",
    "        \n",
    "        action = np.clip(action, -1.0, 1.0).astype(np.float32)\n",
    "        mag = float(np.linalg.norm(action))\n",
    "        if mag > 1.0:\n",
    "            action = action / mag\n",
    "            mag = 1.0\n",
    "        \n",
    "        # Update episode trackers\n",
    "        self.ep_max_action_mag = max(self.ep_max_action_mag, float(mag))\n",
    "        self.ep_max_accel = max(self.ep_max_accel, float(np.linalg.norm(self.a_actual)))\n",
    "\n",
    "        dist_before = float(np.linalg.norm(self.enemy_pos - self.defense_pos))\n",
    "        \n",
    "        # --- Shaping: compute phi BEFORE transition ---\n",
    "        phi_before, zem_perp_before, Vc_before, tgo_before = self._phi_zem_perp()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        event = \"running\"\n",
    "        \n",
    "        for _ in range(self.n_substeps):\n",
    "            dt = self.dt_sim\n",
    "            enemy_pos_old = self.enemy_pos.copy()\n",
    "            defense_pos_old = self.defense_pos.copy()\n",
    "            \n",
    "            forward, right, up = self._compute_lateral_basis(self.defense_vel)\n",
    "            \n",
    "            # Agent command = desired NET lateral accel (world frame)\n",
    "            a_net_lat_cmd = (action[0] * self.a_max * right) + (action[1] * self.a_max * up)\n",
    "            \n",
    "            # Gravity (world frame)\n",
    "            g_vec = np.array([0.0, 0.0, -self.g], dtype=np.float32)\n",
    "            \n",
    "            # Lateral component of gravity in the right/up plane\n",
    "            g_lat = (np.dot(g_vec, right) * right) + (np.dot(g_vec, up) * up)\n",
    "            \n",
    "            # Fins must cancel lateral gravity to achieve commanded NET lateral accel\n",
    "            a_fins_cmd = a_net_lat_cmd - g_lat\n",
    "            \n",
    "            # Apply rate limit + lag to fins acceleration\n",
    "            self.a_cmd_prev = self._rate_limit_norm(a_fins_cmd, self.a_cmd_prev, self.da_max, dt)\n",
    "            self.a_actual += (self.a_cmd_prev - self.a_actual) * (dt / self.tau)\n",
    "            \n",
    "            # Integrate translational dynamics\n",
    "            self.defense_vel += (self.a_actual + g_vec) * dt\n",
    "            self.defense_pos += self.defense_vel * dt\n",
    "            self.defense_x, self.defense_y, self.defense_z = self.defense_pos\n",
    "            \n",
    "            # Enemy missile: pure ballistic (gravity only)\n",
    "            self.enemy_vel += g_vec * dt\n",
    "            self.enemy_pos += self.enemy_vel * dt\n",
    "            self.enemy_x, self.enemy_y, self.enemy_z = self.enemy_pos\n",
    "            self.t += dt\n",
    "            \n",
    "            r0 = enemy_pos_old - defense_pos_old\n",
    "            r1 = self.enemy_pos - self.defense_pos\n",
    "            if self._segment_sphere_intersect(r0, r1, self.collision_radius):\n",
    "                self.success = True\n",
    "                terminated = True\n",
    "                self.done = True\n",
    "                event = \"hit\"\n",
    "                self.time_to_hit = float(self.t)\n",
    "                self.terminal_event = \"hit\"\n",
    "                break\n",
    "            \n",
    "            dist = float(np.linalg.norm(self.enemy_pos - self.defense_pos))\n",
    "            if dist > self.max_distance:\n",
    "                truncated = True\n",
    "                self.done = True\n",
    "                event = \"diverged\"\n",
    "                self.terminal_event = \"diverged\"\n",
    "                break\n",
    "            if self.defense_pos[2] < 0:\n",
    "                terminated = True\n",
    "                self.done = True\n",
    "                event = \"defense_ground\"\n",
    "                self.terminal_event = \"defense_ground\"\n",
    "                break\n",
    "            if self.enemy_pos[2] < 0:\n",
    "                terminated = True\n",
    "                self.done = True\n",
    "                event = \"enemy_ground\"\n",
    "                self.terminal_event = \"enemy_ground\"\n",
    "                break\n",
    "            if self.t >= self.t_max:\n",
    "                truncated = True\n",
    "                self.done = True\n",
    "                event = \"timeout\"\n",
    "                self.terminal_event = \"timeout\"\n",
    "                break\n",
    "\n",
    "        self.enemy_path.append(self.enemy_pos.copy())\n",
    "        self.defense_path.append(self.defense_pos.copy())\n",
    "        self.relative_distances.append(float(np.linalg.norm(self.enemy_pos - self.defense_pos)))\n",
    "        self.times.append(self.t)\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "        dist_after = float(np.linalg.norm(self.enemy_pos - self.defense_pos))\n",
    "        self.min_dist = min(getattr(self, \"min_dist\", float(\"inf\")), dist_after)\n",
    "        self.ep_min_dist = min(self.ep_min_dist, float(dist_after))\n",
    "        \n",
    "        # Reward calculation (same as before)\n",
    "        r_progress = (dist_before - dist_after) / 100.0\n",
    "        v_scale = 1500.0\n",
    "        r = (self.enemy_pos - self.defense_pos).astype(np.float64)\n",
    "        vrel = (self.enemy_vel - self.defense_vel).astype(np.float64)\n",
    "        d = float(np.linalg.norm(r)) + 1e-9\n",
    "        rhat = r / d\n",
    "        d_dot = float(np.dot(rhat, vrel))\n",
    "        r_close = np.tanh((-d_dot) / v_scale)\n",
    "        \n",
    "        # --- Shaping: compute phi AFTER transition ---\n",
    "        if terminated or truncated:\n",
    "            # standard trick for episodic shaping: set terminal potential to 0\n",
    "            phi_after = 0.0\n",
    "            zem_perp_after = None\n",
    "            Vc_after = None\n",
    "            tgo_after = None\n",
    "        else:\n",
    "            phi_after, zem_perp_after, Vc_after, tgo_after = self._phi_zem_perp()\n",
    "\n",
    "        r_zem = self.w_zem * (self.gamma_shape * phi_after - phi_before)\n",
    "        \n",
    "        # Accumulate shaping rewards for debugging\n",
    "        self.sum_r_progress += float(r_progress)\n",
    "        self.sum_r_close += float(r_close)\n",
    "        self.sum_r_zem += float(r_zem)\n",
    "        \n",
    "        # Reward breakdown (named components)\n",
    "        step_penalty = -0.001\n",
    "        terminal_bonus = 0.0\n",
    "        terminal_penalty = 0.0\n",
    "        \n",
    "        if self.success:\n",
    "            terminal_bonus = 10000.0\n",
    "        elif terminated or truncated:\n",
    "            if event == \"defense_ground\":\n",
    "                terminal_penalty += 5000.0\n",
    "            terminal_penalty += min(2000.0, self.ep_min_dist / 50.0)\n",
    "        \n",
    "        reward = (1.0 * r_progress) + (0.1 * r_close) + step_penalty + terminal_bonus - terminal_penalty + r_zem\n",
    "        \n",
    "        info = {\n",
    "            \"event\": event,\n",
    "            \"t\": float(self.t),\n",
    "            \n",
    "            # reward pieces (training-related)\n",
    "            \"reward_terms\": {\n",
    "                \"r_progress\": float(r_progress),\n",
    "                \"r_close\": float(r_close),\n",
    "                \"step_penalty\": float(step_penalty),\n",
    "                \"terminal_bonus\": float(terminal_bonus),\n",
    "                \"terminal_penalty\": float(-terminal_penalty),  # negative contribution\n",
    "                \"r_zem\": float(r_zem),\n",
    "            },\n",
    "            \"reward\": float(reward),\n",
    "            \n",
    "            # ZEM shaping debug info\n",
    "            \"zem_debug\": {\n",
    "                \"phi_before\": float(phi_before),\n",
    "                \"phi_after\": float(phi_after),\n",
    "                \"zem_perp_before\": float(zem_perp_before),\n",
    "                \"zem_perp_after\": None if zem_perp_after is None else float(zem_perp_after),\n",
    "                \"Vc_before\": float(Vc_before),\n",
    "                \"Vc_after\": None if Vc_after is None else float(Vc_after),\n",
    "                \"tgo_before\": float(tgo_before),\n",
    "                \"tgo_after\": None if tgo_after is None else float(tgo_after),\n",
    "            },\n",
    "            \n",
    "            # eval snapshots (NOT the full episode metrics yet)\n",
    "            \"eval_step\": {\n",
    "                \"dist\": float(dist_after),\n",
    "                \"action_mag\": float(mag),\n",
    "                \"accel_norm\": float(np.linalg.norm(self.a_actual)),\n",
    "            },\n",
    "        }\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    def _snapshot(self):\n",
    "        \"\"\"Copy everything that affects future transitions + trackers used by reward.\"\"\"\n",
    "        return {\n",
    "            \"t\": float(self.t),\n",
    "            \"enemy_pos\": self.enemy_pos.copy(),\n",
    "            \"enemy_vel\": self.enemy_vel.copy(),\n",
    "            \"defense_pos\": self.defense_pos.copy(),\n",
    "            \"defense_vel\": self.defense_vel.copy(),\n",
    "            \"a_actual\": self.a_actual.copy(),\n",
    "            \"a_cmd_prev\": self.a_cmd_prev.copy(),\n",
    "            \"done\": bool(getattr(self, \"done\", False)),\n",
    "            \"success\": bool(getattr(self, \"success\", False)),\n",
    "\n",
    "            # reward-critical trackers\n",
    "            \"min_dist\": float(getattr(self, \"min_dist\", float(\"inf\"))),\n",
    "            \"ep_min_dist\": float(getattr(self, \"ep_min_dist\", float(\"inf\"))),\n",
    "\n",
    "            # episode trackers (not strictly needed for the local test but cheap)\n",
    "            \"ep_max_action_mag\": float(getattr(self, \"ep_max_action_mag\", 0.0)),\n",
    "            \"ep_max_accel\": float(getattr(self, \"ep_max_accel\", 0.0)),\n",
    "            \"time_to_hit\": getattr(self, \"time_to_hit\", None),\n",
    "            \"terminal_event\": getattr(self, \"terminal_event\", \"running\"),\n",
    "\n",
    "            # log lengths (so restore can truncate)\n",
    "            \"enemy_path_len\": len(getattr(self, \"enemy_path\", [])),\n",
    "            \"defense_path_len\": len(getattr(self, \"defense_path\", [])),\n",
    "            \"relative_distances_len\": len(getattr(self, \"relative_distances\", [])),\n",
    "            \"times_len\": len(getattr(self, \"times\", [])),\n",
    "        }\n",
    "\n",
    "    def _restore(self, snap):\n",
    "        \"\"\"Restore environment state from snapshot.\"\"\"\n",
    "        self.t = float(snap[\"t\"])\n",
    "        self.enemy_pos = snap[\"enemy_pos\"].copy()\n",
    "        self.enemy_vel = snap[\"enemy_vel\"].copy()\n",
    "        self.defense_pos = snap[\"defense_pos\"].copy()\n",
    "        self.defense_vel = snap[\"defense_vel\"].copy()\n",
    "        self.a_actual = snap[\"a_actual\"].copy()\n",
    "        self.a_cmd_prev = snap[\"a_cmd_prev\"].copy()\n",
    "        self.done = bool(snap[\"done\"])\n",
    "        self.success = bool(snap[\"success\"])\n",
    "\n",
    "        self.min_dist = float(snap[\"min_dist\"])\n",
    "        self.ep_min_dist = float(snap[\"ep_min_dist\"])\n",
    "\n",
    "        self.ep_max_action_mag = float(snap[\"ep_max_action_mag\"])\n",
    "        self.ep_max_accel = float(snap[\"ep_max_accel\"])\n",
    "        self.time_to_hit = snap[\"time_to_hit\"]\n",
    "        self.terminal_event = snap[\"terminal_event\"]\n",
    "\n",
    "        # truncate logs (avoid memory blow + keep things consistent)\n",
    "        if hasattr(self, \"enemy_path\"):\n",
    "            self.enemy_path = self.enemy_path[: snap[\"enemy_path_len\"]]\n",
    "        if hasattr(self, \"defense_path\"):\n",
    "            self.defense_path = self.defense_path[: snap[\"defense_path_len\"]]\n",
    "        if hasattr(self, \"relative_distances\"):\n",
    "            self.relative_distances = self.relative_distances[: snap[\"relative_distances_len\"]]\n",
    "        if hasattr(self, \"times\"):\n",
    "            self.times = self.times[: snap[\"times_len\"]]\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None: self.np_random = np.random.RandomState(seed)\n",
    "        self.done = False\n",
    "        self.success = False\n",
    "        self.t = 0.0\n",
    "        self.generate_enemy_missile()\n",
    "        self.generate_defense_missile()\n",
    "        \n",
    "        self.defense_vel = np.array([\n",
    "            self.defense_initial_velocity * np.cos(self.defense_azimuth) * np.cos(self.defense_theta),\n",
    "            self.defense_initial_velocity * np.sin(self.defense_azimuth) * np.cos(self.defense_theta),\n",
    "            self.defense_initial_velocity * np.sin(self.defense_theta)\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        self.enemy_vel = np.array([\n",
    "            self.enemy_initial_velocity * np.cos(self.enemy_azimuth) * np.cos(self.enemy_theta),\n",
    "            self.enemy_initial_velocity * np.sin(self.enemy_azimuth) * np.cos(self.enemy_theta),\n",
    "            self.enemy_initial_velocity * np.sin(self.enemy_theta)\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        self.a_actual = np.zeros(3, dtype=np.float32)\n",
    "        self.a_cmd_prev = np.zeros(3, dtype=np.float32)\n",
    "        self.defense_pos = np.array([self.defense_x, self.defense_y, self.defense_z], dtype=np.float32)\n",
    "        self.enemy_pos = np.array([self.enemy_x, self.enemy_y, self.enemy_z], dtype=np.float32)\n",
    "        self.enemy_path = [self.enemy_pos.copy()]\n",
    "        self.defense_path = [self.defense_pos.copy()]\n",
    "        self.relative_distances = [float(np.linalg.norm(self.enemy_pos - self.defense_pos))]\n",
    "        self.times = [self.t]\n",
    "        self.min_dist = float(self.relative_distances[-1])\n",
    "        self.sum_r_progress = 0.0\n",
    "        self.sum_r_close = 0.0\n",
    "        self.sum_r_zem = 0.0\n",
    "        \n",
    "        # --- episode eval trackers (NOT used in reward) ---\n",
    "        self.ep_min_dist = float(\"inf\")\n",
    "        self.ep_max_action_mag = 0.0\n",
    "        self.ep_max_accel = 0.0          # optional: actual accel norm\n",
    "        self.time_to_hit = None\n",
    "        self.terminal_event = \"running\"\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# EVALUATION FUNCTION (separate from training reward)\n",
    "# ==========================================\n",
    "def evaluate_policy(env, policy_fn, n_episodes=100, seed0=0):\n",
    "    \"\"\"\n",
    "    Evaluate a policy and return episode-level metrics.\n",
    "    This is separate from training reward - these metrics are what you actually care about.\n",
    "    \n",
    "    Args:\n",
    "        env: missile_interception_3d environment instance\n",
    "        policy_fn: Function that takes obs and returns action\n",
    "        n_episodes: Number of episodes to evaluate\n",
    "        seed0: Starting seed (episodes use seed0, seed0+1, ..., seed0+n_episodes-1)\n",
    "    \n",
    "    Returns:\n",
    "        summary: Dict with aggregated metrics (hit_rate, min_dist stats, etc.)\n",
    "        metrics: Dict with raw episode data\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"hits\": 0,\n",
    "        \"ground_defense\": 0,\n",
    "        \"ground_enemy\": 0,\n",
    "        \"diverged\": 0,\n",
    "        \"timeout\": 0,\n",
    "        \"min_dist_list\": [],\n",
    "        \"time_to_hit_list\": [],\n",
    "        \"max_g_list\": [],\n",
    "    }\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        obs, _ = env.reset(seed=seed0 + i)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy_fn(obs)  # your PPO policy OR env.calculate_pronav() baseline\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "        event = info[\"event\"]\n",
    "        metrics[\"min_dist_list\"].append(env.ep_min_dist)\n",
    "        metrics[\"max_g_list\"].append(env.ep_max_action_mag)\n",
    "\n",
    "        if event == \"hit\":\n",
    "            metrics[\"hits\"] += 1\n",
    "            if env.time_to_hit is not None:\n",
    "                metrics[\"time_to_hit_list\"].append(env.time_to_hit)\n",
    "        elif event == \"defense_ground\":\n",
    "            metrics[\"ground_defense\"] += 1\n",
    "        elif event == \"enemy_ground\":\n",
    "            metrics[\"ground_enemy\"] += 1\n",
    "        elif event == \"diverged\":\n",
    "            metrics[\"diverged\"] += 1\n",
    "        elif event == \"timeout\":\n",
    "            metrics[\"timeout\"] += 1\n",
    "\n",
    "    hit_rate = metrics[\"hits\"] / n_episodes\n",
    "\n",
    "    summary = {\n",
    "        \"hit_rate\": hit_rate,\n",
    "        \"min_dist_mean\": float(np.mean(metrics[\"min_dist_list\"])),\n",
    "        \"min_dist_p50\": float(np.median(metrics[\"min_dist_list\"])),\n",
    "        \"min_dist_p10\": float(np.percentile(metrics[\"min_dist_list\"], 10)),\n",
    "        \"time_to_hit_mean\": float(np.mean(metrics[\"time_to_hit_list\"])) if metrics[\"time_to_hit_list\"] else None,\n",
    "        \"max_g_mean\": float(np.mean(metrics[\"max_g_list\"])),\n",
    "        \"violations\": {\n",
    "            \"defense_ground\": metrics[\"ground_defense\"],\n",
    "            \"enemy_ground\": metrics[\"ground_enemy\"],\n",
    "            \"diverged\": metrics[\"diverged\"],\n",
    "            \"timeout\": metrics[\"timeout\"],\n",
    "        }\n",
    "    }\n",
    "    return summary, metrics\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# TEST PRONAV BASELINE (No Animation)\n",
    "# ==========================================\n",
    "\n",
    "# def run_baseline():\n",
    "#     env = missile_interception_3d()\n",
    "#     outcomes = []\n",
    "#     min_distances = []\n",
    "#     action_loads = [] # Track if we are saturating (maxing out fins)\n",
    "\n",
    "#     N_EPISODES = 50\n",
    "#     print(f\"Running {N_EPISODES} episodes of Augmented ProNav...\")\n",
    "\n",
    "#     for i in range(N_EPISODES):\n",
    "#         obs, _ = env.reset(seed=i)\n",
    "#         done = False\n",
    "#         ep_actions = []\n",
    "\n",
    "#         while not done:\n",
    "#             # 1. Ask ProNav for the move\n",
    "#             action = env.calculate_pronav()\n",
    "            \n",
    "#             # 2. Track how hard it's pushing (0.0 to 1.0)\n",
    "#             mag = np.linalg.norm(action)\n",
    "#             ep_actions.append(mag)\n",
    "\n",
    "#             obs, reward, terminated, truncated, info = env.step(action)\n",
    "#             done = terminated or truncated\n",
    "        \n",
    "#         outcomes.append(info['event'])\n",
    "#         min_distances.append(info['min_dist'])\n",
    "#         avg_load = np.mean(ep_actions)\n",
    "#         action_loads.append(avg_load)\n",
    "\n",
    "#         print(f\"Ep {i+1:02d} | Res: {info['event']:<14} | Min Dist: {info['min_dist']:.1f} m | Avg G-Load: {avg_load*100:.1f}%\")\n",
    "\n",
    "#     # Final Stats\n",
    "#     hits = outcomes.count(\"hit\")\n",
    "#     print(\"\\n--- SUMMARY ---\")\n",
    "#     print(f\"Hit Rate: {hits}/{N_EPISODES} ({hits/N_EPISODES*100:.1f}%)\")\n",
    "#     print(f\"Average Miss Distance (Non-hits): {np.mean([d for d, e in zip(min_distances, outcomes) if e != 'hit']):.2f} m\")\n",
    "#     print(f\"Average G-Loading: {np.mean(action_loads)*100:.1f}% (If >90%, missile is physically too weak)\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b14fb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProNav return: (11300.580837627409, 401.0159554241552)\n",
      "Random return: (-4018.706675229951, 676.1909682148818)\n"
     ]
    }
   ],
   "source": [
    "def rollout_return(env, policy_fn, episodes=50, seed0=0):\n",
    "    import numpy as np\n",
    "    Rs = []\n",
    "    for i in range(episodes):\n",
    "        obs, _ = env.reset(seed=seed0 + i)\n",
    "        done = False\n",
    "        G = 0.0\n",
    "        while not done:\n",
    "            a = policy_fn(obs)\n",
    "            obs, r, term, trunc, info = env.step(a)\n",
    "            G += r\n",
    "            done = term or trunc\n",
    "        Rs.append(G)\n",
    "    return float(np.mean(Rs)), float(np.std(Rs))\n",
    "\n",
    "env = missile_interception_3d()\n",
    "def pronav_pi(obs): return env.calculate_pronav()\n",
    "def random_pi(obs): return env.action_space.sample().astype(np.float32)\n",
    "\n",
    "print(\"ProNav return:\", rollout_return(env, pronav_pi))\n",
    "print(\"Random return:\", rollout_return(env, random_pi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
