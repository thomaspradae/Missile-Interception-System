{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.4.2.6)\n",
    "Testing out v.2.4.2.5 with multiple defense missiles in order to achieve 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class initial_conditions_missile_interception(Env):\n",
    "    def __init__(self):\n",
    "        self.radius = 0.02\n",
    "        self.create_target()\n",
    "        self.create_attack(self.target)\n",
    "    \n",
    "    def create_target(self):\n",
    "        x = random.uniform(-0.3, 0.3)\n",
    "        y = random.uniform(-0.3, 0.3)\n",
    "        self.target = np.array([x, y])\n",
    "\n",
    "    def create_attack(self, target):\n",
    "        x_side_left = random.uniform(-0.95, max(((target[0] - self.radius) - 0.5), -0.94))\n",
    "        x_side_right = random.uniform(min(((target[0] + self.radius) + 0.5), 0.94), 0.95)\n",
    "        y_below = random.uniform(max(((target[1] - self.radius) - 0.5), -0.94), -0.95)\n",
    "        y_above = random.uniform(max(((target[1] + self.radius) + 0.5), 0.94), 0.95)\n",
    "        x_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_below_x_inclusive = np.array([x_inclusive, y_below])\n",
    "        y_above_x_inclusive = np.array([x_inclusive, y_above])\n",
    "        x_left_y_inclusive = np.array([x_side_left, y_inclusive])\n",
    "        x_right_y_inclusive = np.array([x_side_right, y_inclusive])\n",
    "\n",
    "        self.attack = random.choice([y_below_x_inclusive, y_above_x_inclusive, x_left_y_inclusive, x_right_y_inclusive])\n",
    "\n",
    "    def reset(self):\n",
    "        return self.target, self.attack\n",
    "\n",
    "init = initial_conditions_missile_interception()\n",
    "init_coords = init.reset()\n",
    "print(init_coords[0])\n",
    "print(init_coords[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "from gymnasium import Env\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import gymnasium as gym\n",
    "import numpy as npm\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import tensorflow as tf\n",
    "\n",
    "# # Set seed for reproducibility\n",
    "# seed = 42\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "class missile_interception(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.discrete.Discrete(5)\n",
    "        \n",
    "        low = np.array([-1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -2, -2])\n",
    "        high = np.array([1, 1, 1, 1, 1, 1, 2*math.pi, 2*math.pi, 2*math.pi, math.pi, 2.9, 2.9, 1, 1, 1, 1, 2, 2])\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "        self.radius = 0.02\n",
    "\n",
    "        self.episode_count = 0\n",
    "        self.distance_t_minus_one = 0\n",
    "        self.distance_change = 0\n",
    "\n",
    "        self.out_of_bounds = 0\n",
    "        self.interceptions = 0\n",
    "        self.reached_max_steps = 0\n",
    "        self.enemy_impacts = 0\n",
    "\n",
    "        self.defense_positions = []\n",
    "        self.attack_positions = []\n",
    "        self.attack_starting_position = 0\n",
    "\n",
    "        self.max_steps_per_episode = 150\n",
    "        self.activate_value = 0\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        global init_coords\n",
    "        self.dict_state = {}\n",
    "        self.activate_enemy_impact = False\n",
    "        self.defense_positions = []\n",
    "        self.attack_positions = []\n",
    "        self.reward = 0\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.target = init_coords[0]\n",
    "        self.create_defense(self.target)\n",
    "        self.attack = init_coords[1]\n",
    "\n",
    "        self.calc_defense_attack_theta(self.defense, self.attack)\n",
    "        self.calc_attack_target_theta(self.attack, self.target)\n",
    "        self.initial_defense_angle()\n",
    "        self.calc_defense_attack_distance()\n",
    "        self.calc_attack_target_distance()\n",
    "        self.attack_starting_position = self.attack.copy()\n",
    "\n",
    "        self.get_state()\n",
    "        \n",
    "        return self.state, {}\n",
    "\n",
    "    def create_target(self):\n",
    "        x = random.uniform(-0.3, 0.3)\n",
    "        y = random.uniform(-0.3, 0.3)\n",
    "        self.target = np.array([x, y])\n",
    "\n",
    "    def create_defense(self, target):\n",
    "        x = random.uniform((target[0] - self.radius) - 0.15, (target[0] + self.radius) + 0.15)\n",
    "        y = random.uniform((target[1] - self.radius) - 0.15, (target[1] + self.radius) + 0.15)\n",
    "        self.defense = np.array([x, y])\n",
    "\n",
    "    def create_attack(self, target):\n",
    "        x_side_left = random.uniform(-0.95, max(((target[0] - self.radius) - 0.5), -0.94))\n",
    "        x_side_right = random.uniform(min(((target[0] + self.radius) + 0.5), 0.94), 0.95)\n",
    "        y_below = random.uniform(max(((target[1] - self.radius) - 0.5), -0.94), -0.95)\n",
    "        y_above = random.uniform(max(((target[1] + self.radius) + 0.5), 0.94), 0.95)\n",
    "        x_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_inclusive = random.uniform(-0.95, 0.95)\n",
    "        y_below_x_inclusive = np.array([x_inclusive, y_below])\n",
    "        y_above_x_inclusive = np.array([x_inclusive, y_above])\n",
    "        x_left_y_inclusive = np.array([x_side_left, y_inclusive])\n",
    "        x_right_y_inclusive = np.array([x_side_right, y_inclusive])\n",
    "\n",
    "        self.attack = random.choice([y_below_x_inclusive, y_above_x_inclusive, x_left_y_inclusive, x_right_y_inclusive])\n",
    "\n",
    "    def calc_defense_attack_theta(self, defense, attack):\n",
    "\n",
    "        # create an adjacent point of the form (attack_x, defense_y)\n",
    "        adjacent_point = np.array([attack[0], defense[1]])\n",
    "\n",
    "        # calculate the distance between the adjacent point and the defense, attack points\n",
    "        adj_point_defense_len = abs(defense[0] - adjacent_point[0]) \n",
    "        adj_point_attack_len = abs(attack[1] - adjacent_point[1])\n",
    "\n",
    "        # calculate the angle, using soh cah toa, where adj_point_defense_len is the adjacent side and adj_point_attack_len is the opposite side\n",
    "        self.defense_attack_theta = np.arctan(adj_point_attack_len / adj_point_defense_len)\n",
    "        \n",
    "        if attack[0] > defense[0]:\n",
    "            if attack[1] > defense[1]:\n",
    "                self.defense_attack_theta = self.defense_attack_theta # 1st quadrant\n",
    "            else: \n",
    "                self.defense_attack_theta = (2*math.pi) - self.defense_attack_theta # 360 - theta\n",
    "        else:\n",
    "            if attack[1] > defense[1]:\n",
    "                self.defense_attack_theta = math.pi - self.defense_attack_theta # 180 - theta\n",
    "            else:\n",
    "                self.defense_attack_theta = math.pi + self.defense_attack_theta # 180 + theta\n",
    "\n",
    "    def calc_attack_target_theta(self, attack, target):\n",
    "        # create an adjacent point of the form (target_x, attack_y)\n",
    "        adjacent_point = np.array([target[0], attack[1]])\n",
    "\n",
    "        # calculate the distance between the adjacent point and the attack, target points\n",
    "        adj_point_attack_len = abs(attack[0] - adjacent_point[0])\n",
    "        adj_point_target_len = abs(target[1] - adjacent_point[1])\n",
    "        \n",
    "        # calculate the angle, using soh cah toa, where adj_point_attack_len is the adjacent side and adj_point_target_len is the opposite side\n",
    "        self.attack_target_theta = np.arctan(adj_point_target_len / adj_point_attack_len)\n",
    "\n",
    "        if target[0] > attack[0]:\n",
    "            if target[1] > attack[1]:\n",
    "                self.attack_target_theta = self.attack_target_theta\n",
    "            else:\n",
    "                self.attack_target_theta = (2*math.pi) - self.attack_target_theta\n",
    "        else:\n",
    "            if target[1] > attack[1]:\n",
    "                self.attack_target_theta = math.pi - self.attack_target_theta\n",
    "            else:\n",
    "                self.attack_target_theta = math.pi + self.attack_target_theta        \n",
    "\n",
    "    def initial_defense_angle(self):\n",
    "        self.defense_angle = np.random.uniform((self.defense_attack_theta - 2.35619), (self.defense_attack_theta + 2.35619))\n",
    "        if self.defense_angle > 2*math.pi:\n",
    "            self.defense_angle = self.defense_angle - 2*math.pi\n",
    "        elif self.defense_angle < 0:\n",
    "            self.defense_angle = 2*math.pi + self.defense_angle\n",
    "\n",
    "    def calculate_distance(self, point1, point2):\n",
    "        return math.hypot(point1[0] - point2[0], point1[1] - point2[1])\n",
    "    \n",
    "    def calc_defense_attack_distance(self):\n",
    "        self.defense_attack_distance = (self.calculate_distance(self.defense, self.attack) - (2 * self.radius))\n",
    "\n",
    "    def calc_attack_target_distance(self):\n",
    "        self.attack_target_distance = (self.calculate_distance(self.attack, self.target) - (2 * self.radius))\n",
    "\n",
    "    def calc_defense_angle(self, action):\n",
    "        if action == 0:\n",
    "            self.defense_angle = self.defense_angle \n",
    "        elif action == 1:\n",
    "            self.defense_angle += 0.174532925\n",
    "        elif action == 2:\n",
    "            self.defense_angle += 0.523599\n",
    "        elif action == 3:\n",
    "            self.defense_angle -= 0.174532925\n",
    "        elif action == 4:\n",
    "            self.defense_angle -= 0.523599\n",
    "        \n",
    "        if self.defense_angle > 2*math.pi:\n",
    "            self.defense_angle = self.defense_angle - 2*math.pi\n",
    "        elif self.defense_angle < 0:\n",
    "            self.defense_angle = 2*math.pi + self.defense_angle\n",
    "\n",
    "    def update_coords(self):\n",
    "        self.defense[0] += (0.02 * math.cos(self.defense_angle)) # gotta test this\n",
    "        self.defense[1] += (0.02 * math.sin(self.defense_angle))\n",
    "        self.attack[0] += (0.02 * math.cos(self.attack_target_theta))\n",
    "        self.attack[1] += (0.02 * math.sin(self.attack_target_theta))\n",
    "        self.defense_positions.append(self.defense.copy())\n",
    "        self.attack_positions.append(self.attack.copy())\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        self.calc_defense_attack_distance()\n",
    "        self.calc_attack_target_distance()\n",
    "\n",
    "        if self.attack_target_distance < 0:\n",
    "            print(\"ENEMY HIT!\")\n",
    "            self.activate_enemy_impact = True\n",
    "            self.activate_value = 0\n",
    "            self.dict_state = self.get_state_dict()\n",
    "            self.reward = -10000\n",
    "            self.done = True\n",
    "            self.enemy_impacts += 1\n",
    "\n",
    "        elif self.defense_attack_distance < 0:\n",
    "            print(\"HIT!\")\n",
    "            self.activate_interception = True\n",
    "            self.reward = 10000\n",
    "            self.done = True\n",
    "            self.interceptions += 1\n",
    "        else:\n",
    "            self.angle_diff = abs(self.defense_attack_theta - self.defense_angle)\n",
    "            self.angle_diff = min(self.angle_diff, 2*math.pi - self.angle_diff)\n",
    "            self.reward = 1/self.angle_diff\n",
    "                \n",
    "        if self.defense[0] < -1 or self.defense[0] > 1 or self.defense[1] < -1 or self.defense[1] > 1:\n",
    "            print(\"OUT OF BOUNDS\")\n",
    "            self.reward = -1000\n",
    "            self.done = True\n",
    "            self.out_of_bounds += 1\n",
    "\n",
    "    def angle_conversion(self):\n",
    "        self.sin_defense_attack_theta, self.sin_defense_angle = np.sin(self.defense_attack_theta), np.sin(self.defense_angle)\n",
    "        self.cos_defense_attack_theta, self.cos_defense_angle = np.cos(self.defense_attack_theta), np.cos(self.defense_angle)\n",
    "\n",
    "        self.delta_sin = self.sin_defense_attack_theta - self.sin_defense_angle\n",
    "        self.delta_cos = self.cos_defense_attack_theta - self.cos_defense_angle\n",
    "\n",
    "    def get_state(self):\n",
    "        self.angle_conversion()\n",
    "\n",
    "        self.state = np.array([\n",
    "            self.attack[0], self.attack[1], \n",
    "            self.defense[0], self.defense[1], \n",
    "            self.target[0], self.target[1],\n",
    "            self.defense_attack_theta, self.attack_target_theta,\n",
    "            self.defense_angle,\n",
    "            min(abs(self.defense_attack_theta - self.defense_angle), 2*math.pi - abs(self.defense_attack_theta - self.defense_angle)),\n",
    "            self.defense_attack_distance,\n",
    "            self.attack_target_distance,\n",
    "            self.sin_defense_attack_theta, self.cos_defense_attack_theta, \n",
    "            self.sin_defense_angle, self.cos_defense_angle,\n",
    "            self.delta_sin, self.delta_cos\n",
    "        ])\n",
    "\n",
    "    def get_state_dict(self):\n",
    "        return {\n",
    "            \"self.activate\": self.activate_enemy_impact,\n",
    "            \"attack_x\": self.attack[0],\n",
    "            \"attack_y\": self.attack[1],\n",
    "            \"defense_x\": self.defense[0],\n",
    "            \"defense_y\": self.defense[1],\n",
    "            \"target_x\": self.target[0],\n",
    "            \"target_y\": self.target[1],\n",
    "            \"defense_attack_theta\": self.defense_attack_theta,\n",
    "            \"attack_target_theta\": self.attack_target_theta,\n",
    "            \"defense_angle\": self.defense_angle,\n",
    "            \"angle_diff\": min(abs(self.defense_attack_theta - self.defense_angle), 2*math.pi - abs(self.defense_attack_theta - self.defense_angle)),\n",
    "            \"distance_attack_missile\": self.defense_attack_distance,\n",
    "            \"distance_attack_target\": self.attack_target_distance,\n",
    "            \"sin_defense_attack_theta\": self.sin_defense_attack_theta,\n",
    "            \"cos_defense_attack_theta\": self.cos_defense_attack_theta,\n",
    "            \"sin_defense_angle\": self.sin_defense_angle,\n",
    "            \"cos_defense_angle\": self.cos_defense_angle,\n",
    "            \"delta_sin\": self.delta_sin,\n",
    "            \"delta_cos\": self.delta_cos\n",
    "        }\n",
    "                 \n",
    "    def step(self, action):\n",
    "        self.distance_t_minus_one = self.defense_attack_distance\n",
    "        self.calc_defense_angle(action)\n",
    "        self.update_coords()\n",
    "        self.calc_defense_attack_theta(self.defense, self.attack)\n",
    "        self.calculate_reward()\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= self.max_steps_per_episode:\n",
    "            print(\"MAX STEPS REACHED\")\n",
    "            self.done = True\n",
    "            self.reward = -1000\n",
    "            self.reached_max_steps += 1\n",
    "\n",
    "        self.get_state()\n",
    "        # print(\"/////////////////////////////////////////\")\n",
    "        # print(\"dict state: \", dict_state)\n",
    "        # print(\"state: \", state)\n",
    "        return self.state, self.reward, self.done, False, {'activated': self.activate_enemy_impact}\n",
    "\n",
    "    def graph(self, defense, attack, target):\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.xlim(-1, 1)\n",
    "        plt.ylim(-1, 1)\n",
    "\n",
    "        plt.axhline(0, color='black', linewidth=0.5)\n",
    "        plt.axvline(0, color='black', linewidth=0.5)\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot trails\n",
    "        if self.defense_positions:\n",
    "            defense_xs, defense_ys = zip(*self.defense_positions)\n",
    "            ax.plot(defense_xs, defense_ys, color='#858585', label='Defense Trail')  # Blue line for defense\n",
    "\n",
    "        if self.attack_positions:\n",
    "            attack_xs, attack_ys = zip(*self.attack_positions)\n",
    "            ax.plot(attack_xs, attack_ys, color='#FFA281', label='Attack Trail')  # Red line for attack\n",
    "\n",
    "        # Plot current positions\n",
    "        plt.scatter(defense[0], defense[1], color='#1C1C1C')\n",
    "        plt.scatter(attack[0], attack[1], color='#FF5A1F')\n",
    "        plt.scatter(self.target[0], self.target[1], color='#85A3FF')\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = missile_interception()\n",
    "env.reset()\n",
    "\n",
    "print(env.target)\n",
    "print(env.defense)\n",
    "print(env.attack)\n",
    "env.graph(env.defense, env.attack, env.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Custom environment import placeholder\n",
    "# from your_custom_environment import missile_interception, initial_conditions_missile_interception\n",
    "\n",
    "model_path = \"dqn_missile_guidance_v(2.4.2.5)_PROD_11\"\n",
    "model = DQN.load(model_path)\n",
    "\n",
    "# Initialize environments\n",
    "env1 = missile_interception()\n",
    "env2 = missile_interception()\n",
    "envs = [env1, env2]\n",
    "\n",
    "# Initialize episode data\n",
    "episode_data = [{\n",
    "    'past_defense_positions': [],\n",
    "    'past_attack_positions': [],\n",
    "    'attack_positions': [],\n",
    "    'defense_positions': [],\n",
    "    'target_position': [],\n",
    "    'actions': [],\n",
    "    'rewards': [],\n",
    "    'defense_angle': [],\n",
    "    'defense_attack_theta': [],\n",
    "} for _ in envs]\n",
    "\n",
    "# Initialize conditions and reset environments\n",
    "init = initial_conditions_missile_interception()\n",
    "init_coords = init.reset()\n",
    "obs_list = [env.reset()[0] for env in envs]  \n",
    "\n",
    "# Set initial target positions\n",
    "for i, obs in enumerate(obs_list):\n",
    "    episode_data[i]['target_position'] = obs[4:6]\n",
    "\n",
    "done_list = [False] * len(envs)\n",
    "total_rewards = [0] * len(envs)\n",
    "steps = [0] * len(envs)\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env1.step(action)\n",
    "    print(\"Done\", done)\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env2.step(action)\n",
    "\n",
    "\n",
    "\n",
    "# while not any(done_list):\n",
    "#     for i, env in enumerate(envs):\n",
    "#         print(\"Env: \", i)\n",
    "#         print(\"Attack Coords\", env.attack)\n",
    "#         print(\"Defense Coords\", env.defense)\n",
    "#         obs = np.array([obs_list[i]]) \n",
    "#         action, _ = model.predict(obs, deterministic=True)\n",
    "#         obs, reward, done, truncated, info = env.step(action)\n",
    "#         obs_list[i] = obs\n",
    "#         total_rewards[i] += reward\n",
    "#         steps[i] += 1\n",
    "#         done_list[i] = done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Path to the saved model\n",
    "model_path = \"dqn_missile_guidance_v(2.4.2.5)_PROD_11\"\n",
    "model = DQN.load(model_path)\n",
    "\n",
    "# Create environments\n",
    "env1 = make_vec_env(lambda: missile_interception(), n_envs=1)\n",
    "env2 = make_vec_env(lambda: missile_interception(), n_envs=1)\n",
    "envs = [env1, env2]\n",
    "\n",
    "# Initialize episode data\n",
    "episode_data = [{\n",
    "    'past_defense_positions': [],\n",
    "    'past_attack_positions': [],\n",
    "    'attack_positions': [],\n",
    "    'defense_positions': [],\n",
    "    'target_position': [],\n",
    "    'actions': [],\n",
    "    'rewards': [],\n",
    "    'defense_angle': [],\n",
    "    'defense_attack_theta': [],\n",
    "} for _ in envs]\n",
    "\n",
    "# Reset environments\n",
    "init = initial_conditions_missile_interception()\n",
    "init_coords = init.reset()\n",
    "print(init_coords[0])\n",
    "print(init_coords[1])\n",
    "obs_list = [env.reset() for env in envs]\n",
    "\n",
    "for i, obs in enumerate(obs_list):\n",
    "    episode_data[i]['target_position'] = obs[0][4:6]\n",
    "\n",
    "# Run environments step-by-step\n",
    "done_list = [False] * len(envs)\n",
    "total_rewards = [0] * len(envs)\n",
    "steps = [0] * len(envs)\n",
    "\n",
    "while not any(done_list):\n",
    "    for i, env in enumerate(envs):  \n",
    "        # print(obs_list)\n",
    "        # print(\"i\", i)\n",
    "        # print(obs_list[i])\n",
    "        # print(f\"Environment {i} ob: {obs_list[i]}\")\n",
    "        # print(f\"Environment {i} done: {done_list[i]}\")\n",
    "        if not done_list[i]:\n",
    "            print(f\"Environment {i} done: {done_list[i]}\")\n",
    "            print(\"----------------------------------------------------------\")\n",
    "            print(\"Before\")\n",
    "            print(f\"Environment {i}\")\n",
    "            print(f\"Environment {0} steaaaaaaaaaaaaaaaap: {obs_list[0][0][0:2]}\")\n",
    "            print(f\"Environment {1} steaaaaaaaaaaaaaaaap: {obs_list[1][0][0:2]}\")\n",
    "            print(\"----------------------------------------------------------\")\n",
    "            action, _ = model.predict(obs_list[i], deterministic=True)\n",
    "            episode_data[i]['actions'].append(deepcopy(action))\n",
    "            # print(\"\\n Defense position\")\n",
    "            # print(np.array([obs_list[i][0][2], obs_list[i][0][3]]))\n",
    "            # print(\"\\n\")\n",
    "            episode_data[i]['past_defense_positions'].append(deepcopy(np.array([obs_list[i][0][2], obs_list[i][0][3]])))\n",
    "            episode_data[i]['past_attack_positions'].append(deepcopy(np.array([obs_list[i][0][0], obs_list[i][0][1]])))\n",
    "            # print(\"\\n Attack position\")\n",
    "            # print(np.array([obs_list[i][0][0], obs_list[i][0][1]]))\n",
    "            # print(\"\\n\")\n",
    "            print(f\"Environment {i} action: {obs_list[i][0][0:2]}\")\n",
    "            print(envs)\n",
    "            print(env)\n",
    "            carlos, reward, done, info = env.step(action)\n",
    "            print(\"Carlos: \", carlos)   \n",
    "            print(f\"Environment {i} action: {obs_list[i][0][0:2]}\")\n",
    "            print(\"----------------------------------------------------------\")\n",
    "            print(\"After\")\n",
    "            print(f\"Environment {i}\")\n",
    "            print(f\"Environment {0} steaaaaaaaaaaaaaaaap: {obs_list[0][0][0:2]}\")\n",
    "            print(f\"Environment {1} steaaaaaaaaaaaaaaaap: {obs_list[1][0][0:2]}\")\n",
    "            print(\"----------------------------------------------------------\")\n",
    "\n",
    "            episode_data[i]['rewards'].append(deepcopy(reward))\n",
    "            episode_data[i]['defense_angle'].append(deepcopy(obs[0][8]))\n",
    "            episode_data[i]['defense_attack_theta'].append(deepcopy(obs[0][6]))\n",
    "            episode_data[i]['attack_positions'].append(deepcopy(np.array([obs[0][2], obs[0][3]])))\n",
    "            episode_data[i]['defense_positions'].append(deepcopy(np.array([obs[0][0], obs[0][1]])))\n",
    "            print(\"----------------------------------------------------------------------\")\n",
    "            print(f\"Environment {i} step: {steps[i]}\")\n",
    "            # print(f\"Environment {i} action: {action}\")\n",
    "            # print(f\"Environment {i} reward: {reward}\")\n",
    "            print(f\"Environment {i} done: {done}\")\n",
    "            # print(f\"Environment {i} defense position: {obs[0][2], obs[0][3]}\")\n",
    "            print(f\"Environment {i} attack position: {obs_list[i][0][0], obs_list[i][0][1]}\")\n",
    "            # print(f\"Environment {i} target position: {obs[0][4], obs[0][5]}\")\n",
    "            # print(f\"Environment {i} defense angle: {obs[0][8]}\")\n",
    "            # print(f\"Environment {i} defense attack theta: {obs[0][6]}\")\n",
    "            print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "            total_rewards[i] += reward\n",
    "            steps[i] += 1\n",
    "            # obs_list[i] = ob\n",
    "            done_list[i] = done\n",
    "\n",
    "# Print results\n",
    "for i, total_reward in enumerate(total_rewards):\n",
    "    print(f\"Environment {i} finished with total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# so we're gonna need two environments \n",
    "\n",
    "# Function to run an episode\n",
    "def run_episode(env, model):\n",
    "    single_env = env.envs[0]\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    episode_data = {\n",
    "        'past_defense_positions': [],\n",
    "        'past_attack_positions': [],\n",
    "        'attack_positions': [],\n",
    "        'defense_positions': [],\n",
    "        'target_position': [],\n",
    "        'actions': [],\n",
    "        'rewards': [],\n",
    "        'defense_angle': [],\n",
    "        'defense_attack_theta': [],\n",
    "    }\n",
    "    \n",
    "    episode_data['target_position'] = obs[0][4:6]\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        episode_data['actions'].append(deepcopy(action))\n",
    "        episode_data['past_defense_positions'].append(deepcopy(np.array([obs[0][2], obs[0][3]])))\n",
    "        episode_data['past_attack_positions'].append(deepcopy(np.array([obs[0][0], obs[0][1]])))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_data['rewards'].append(deepcopy(reward))\n",
    "        episode_data['defense_angle'].append(deepcopy(obs[0][8]))\n",
    "        episode_data['defense_attack_theta'].append(deepcopy(obs[0][6]))\n",
    "        episode_data['attack_positions'].append(deepcopy(np.array([obs[0][2], obs[0][3]])))\n",
    "        episode_data['defense_positions'].append(deepcopy(np.array([obs[0][0], obs[0][1]])))\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "\n",
    "    print(f\"Episode finished in {step} steps with reward {total_reward}. Out of bounds: {single_env.out_of_bounds} ({((single_env.out_of_bounds / (single_env.out_of_bounds + single_env.interceptions + single_env.reached_max_steps + single_env.enemy_impacts)) * 100)}), Interceptions: {single_env.interceptions} ({((single_env.interceptions / (single_env.out_of_bounds + single_env.interceptions + single_env.reached_max_steps + single_env.enemy_impacts)) * 100)}), Reached max steps: {single_env.reached_max_steps} ({((single_env.reached_max_steps / (single_env.out_of_bounds + single_env.interceptions + single_env.reached_max_steps + single_env.enemy_impacts)) * 100)}), Enemy impacts: {single_env.enemy_impacts} ({((single_env.enemy_impacts / (single_env.out_of_bounds + single_env.interceptions + single_env.reached_max_steps + single_env.enemy_impacts)) * 100)})\")\n",
    "\n",
    "    return step, total_reward, single_env.out_of_bounds, single_env.interceptions, single_env.reached_max_steps, single_env.enemy_impacts, info, episode_data\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model_path, n_episodes=100):\n",
    "    # Create the environment\n",
    "    env = make_vec_env(lambda: missile_interception(), n_envs=1)\n",
    "    env2 = make_vec_env(lambda: missile_interception(), n_envs=1)\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = DQN.load(model_path)\n",
    "    \n",
    "    target_hit_episodes = []\n",
    "    \n",
    "    # Run multiple evaluation episodes\n",
    "    for i in range(n_episodes):\n",
    "        print(f\"Running episode {i + 1}\")\n",
    "        episode_length, episode_reward, out_of_bounds, interceptions, reached_max_steps, enemy_impacts, info, episode_data = run_episode(env, model)\n",
    "\n",
    "        print(info[0]['activated'])\n",
    "        print(\"........................................................................................\")\n",
    "\n",
    "        if info[0]['activated'] == True:\n",
    "            target_hit_episodes.append(episode_data)\n",
    "    \n",
    "    return target_hit_episodes\n",
    "\n",
    "# Function to graph the episode data\n",
    "def graph_episode(defense_positions, attack_positions, target_position):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "\n",
    "    plt.axhline(0, color='black', linewidth=0.5)\n",
    "    plt.axvline(0, color='black', linewidth=0.5)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot trails\n",
    "    if defense_positions:\n",
    "        defense_xs, defense_ys = zip(*defense_positions)\n",
    "        ax.plot(defense_xs, defense_ys, color='#858585', label='Defense Trail')  # Blue line for defense\n",
    "\n",
    "    if attack_positions:\n",
    "        attack_xs, attack_ys = zip(*attack_positions)\n",
    "        ax.plot(attack_xs, attack_ys, color='#FFA281', label='Attack Trail')  # Red line for attack\n",
    "\n",
    "    # Plot current positions\n",
    "    plt.scatter(defense_positions[-1][0], defense_positions[-1][1], color='#1C1C1C', label='Defense Position')\n",
    "    plt.scatter(attack_positions[-1][0], attack_positions[-1][1], color='#FF5A1F', label='Attack Position')\n",
    "    plt.scatter(target_position[0], target_position[1], color='#85A3FF', label='Target Position')\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Path to the saved model\n",
    "model_path = \"dqn_missile_guidance_v(2.4.2.5)_PROD_11\"\n",
    "\n",
    "# Evaluate the model and get episodes with target hits\n",
    "target_hit_episodes = evaluate_model(model_path, n_episodes=50000)\n",
    "\n",
    "# Graph the episodes with target hits\n",
    "i = 0\n",
    "for episode_data in target_hit_episodes:\n",
    "    episode_name = f'Episode {i} collision.gif '\n",
    "    animate_episode(episode_data, episode_name)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
